# SOME DESCRIPTIVE TITLE
# Copyright (C) YEAR The FreeBSD Project
# This file is distributed under the same license as the FreeBSD Documentation package.
# "Kyung-tak, Yoo" <stonegaze@me.com>, 2023.
# Junho Choi <junho.choi@gmail.com>, 2024.
msgid ""
msgstr ""
"Project-Id-Version: FreeBSD Documentation VERSION\n"
"POT-Creation-Date: 2025-05-01 19:56-0300\n"
"PO-Revision-Date: 2024-07-03 15:24+0000\n"
"Last-Translator: Junho Choi <junho.choi@gmail.com>\n"
"Language-Team: Korean <https://translate-dev.freebsd.org/projects/"
"documentation/bookshandbookzfs_index/ko/>\n"
"Language: ko\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Generator: Weblate 4.17\n"

#. type: YAML Front Matter: description
#: documentation/content/en/books/handbook/zfs/_index.adoc:1
#, no-wrap
msgid "ZFS is an advanced file system designed to solve major problems found in previous storage subsystem software"
msgstr "ZFS는 이전 스토리지 하위 시스템 소프트웨어에서 발견된 주요 문제를 해결하기 위해 설계된 고급 파일 시스템입니다"

#. type: YAML Front Matter: part
#: documentation/content/en/books/handbook/zfs/_index.adoc:1
#, no-wrap
msgid "Part III. System Administration"
msgstr "파트 III. 시스템 관리"

#. type: YAML Front Matter: title
#: documentation/content/en/books/handbook/zfs/_index.adoc:1
#, no-wrap
msgid "Chapter 22. The Z File System (ZFS)"
msgstr "22장. Z 파일 시스템(ZFS)"

#. type: Title =
#: documentation/content/en/books/handbook/zfs/_index.adoc:15
#, no-wrap
msgid "The Z File System (ZFS)"
msgstr "Z 파일 시스템(ZFS)"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:53
msgid ""
"ZFS is an advanced file system designed to solve major problems found in "
"previous storage subsystem software."
msgstr ""
"ZFS는 이전 스토리지 하위 시스템 소프트웨어에서 발견된 주요 문제를 해결하기 위"
"해 설계된 고급 파일 시스템입니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:55
msgid ""
"Originally developed at Sun(TM), ongoing open source ZFS development has "
"moved to the http://open-zfs.org[OpenZFS Project]."
msgstr ""
"원래 Sun(TM)에서 개발되었으나 현재 진행 중인 오픈 소스 ZFS 개발은 http://"
"open-zfs.org[OpenZFS 프로젝트]로 이전되었습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:57
msgid "ZFS has three major design goals:"
msgstr "ZFS에는 세 가지 주요 설계 목표가 있습니다:"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:59
#, fuzzy
#| msgid ""
#| "Data integrity: All data includes a <<zfs-term-checksum,checksum>> of the "
#| "data. ZFS calculates checksums and writes them along with the data. When "
#| "reading that data later, ZFS recalculates the checksums. If the checksums "
#| "do not match, meaning detecting one or more data errors, ZFS will attempt "
#| "to automatically correct errors when ditto-, mirror-, or parity-blocks "
#| "are available."
msgid ""
"Data integrity: All data includes a crossref:zfs[zfs-term-checksum,checksum] "
"of the data. ZFS calculates checksums and writes them along with the data. "
"When reading that data later, ZFS recalculates the checksums. If the "
"checksums do not match, meaning detecting one or more data errors, ZFS will "
"attempt to automatically correct errors when ditto-, mirror-, or parity-"
"blocks are available."
msgstr ""
"데이터 무결성: 모든 데이터에는 데이터의 <<zfs-term-checksum,checksum>> 이 포"
"함됩니다. ZFS는 체크섬을 계산하여 데이터와 함께 기록합니다. 나중에 해당 데이"
"터를 읽을 때 ZFS는 체크섬을 다시 계산합니다. 체크섬이 일치하지 않는 경우(즉, "
"하나 이상의 데이터 오류가 감지되는 경우) ZFS는 동일 블록, 미러 블록 또는 패리"
"티 블록을 사용할 수 있을 때 자동으로 오류를 수정하려고 시도합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:60
msgid ""
"Pooled storage: adding physical storage devices to a pool, and allocating "
"storage space from that shared pool. Space is available to all file systems "
"and volumes, and increases by adding new storage devices to the pool."
msgstr ""
"풀 스토리지 (Pooled storage) : 풀(Pool)에 물리적 스토리지 장치를 추가하고 해"
"당 공유 풀에서 스토리지 공간을 할당합니다. 모든 파일 시스템과 볼륨에서 공간"
"을 사용할 수 있으며, 풀에 새 저장 장치를 추가하면 공간이 증가합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:65
#, fuzzy
#| msgid ""
#| "Performance: caching mechanisms provide increased performance. <<zfs-term-"
#| "arc,ARC>> is an advanced memory-based read cache. ZFS provides a second "
#| "level disk-based read cache with <<zfs-term-l2arc,L2ARC>>, and a disk-"
#| "based synchronous write cache named <<zfs-term-zil,ZIL>>."
msgid ""
"Performance: caching mechanisms provide increased performance.  crossref:"
"zfs[zfs-term-arc,ARC] is an advanced memory-based read cache. ZFS provides a "
"second level disk-based read cache with crossref:zfs[zfs-term-l2arc,L2ARC], "
"and a disk-based synchronous write cache named crossref:zfs[zfs-term-zil,"
"ZIL]."
msgstr ""
"성능: 캐싱 메커니즘은 향상된 성능을 제공합니다. <<zfs-term-arc,ARC>> 는 고급 "
"메모리 기반 읽기 캐시입니다. ZFS는 두 번째 레벨의 디스크 기반 읽기 캐시인 "
"<<zfs-term-l2arc,L2ARC>> 와 <<zfs-term-zil,ZIL>> 이라는 디스크 기반 동기식 쓰"
"기 캐시를 제공합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:67
#, fuzzy
#| msgid "A complete list of features and terminology is in <<zfs-term>>."
msgid ""
"A complete list of features and terminology is in crossref:zfs[zfs-term, ZFS "
"Features and Terminology]."
msgstr "기능 및 용어의 전체 목록은 <<zfs-term>>에 나와 있습니다."

#. type: Title ==
#: documentation/content/en/books/handbook/zfs/_index.adoc:69
#, no-wrap
msgid "What Makes ZFS Different"
msgstr "ZFS의 차별화 요소"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:82
msgid ""
"More than a file system, ZFS is fundamentally different from traditional "
"file systems.  Combining the traditionally separate roles of volume manager "
"and file system provides ZFS with unique advantages.  The file system is now "
"aware of the underlying structure of the disks.  Traditional file systems "
"could exist on a single disk alone at a time.  If there were two disks then "
"creating two separate file systems was necessary.  A traditional hardware "
"RAID configuration avoided this problem by presenting the operating system "
"with a single logical disk made up of the space provided by physical disks "
"on top of which the operating system placed a file system.  Even with "
"software RAID solutions like those provided by GEOM, the UFS file system "
"living on top of the RAID believes it's dealing with a single device.  ZFS' "
"combination of the volume manager and the file system solves this and allows "
"the creation of file systems that all share a pool of available storage.  "
"One big advantage of ZFS' awareness of the physical disk layout is that "
"existing file systems grow automatically when adding extra disks to the "
"pool.  This new space then becomes available to the file systems.  ZFS can "
"also apply different properties to each file system. This makes it useful to "
"create separate file systems and datasets instead of a single monolithic "
"file system."
msgstr ""
"ZFS는 파일 시스템 그 이상으로 기존 파일 시스템과 근본적으로 다릅니다.  볼륨 "
"관리자와 파일 시스템의 전통적으로 분리된 역할을 결합하여 ZFS는 고유한 이점을 "
"제공합니다.  이제 파일 시스템은 디스크의 기본 구조를 인식합니다.  기존 파일 "
"시스템은 한 번에 하나의 디스크에만 존재할 수 있었습니다.  디스크가 두 개인 경"
"우 두 개의 파일 시스템을 별도로 만들어야 했습니다.  기존의 하드웨어 RAID 구성"
"은 운영 체제에 물리적 디스크가 제공하는 공간으로 구성된 단일 논리 디스크를 제"
"공하고 그 위에 운영 체제가 파일 시스템을 배치함으로써 이 문제를 방지했습니"
"다.  GEOM에서 제공하는 것과 같은 소프트웨어 RAID 솔루션을 사용하더라도 RAID "
"위에 있는 UFS 파일 시스템은 단일 장치를 처리하는 것으로 간주합니다.  ZFS는 볼"
"륨 관리자와 파일 시스템을 결합하여 이 문제를 해결하고 사용 가능한 스토리지 풀"
"을 모두 공유하는 파일 시스템을 생성할 수 있습니다.  물리적 디스크 레이아웃을 "
"인식하는 ZFS의 큰 장점 중 하나는 풀에 디스크를 추가할 때 기존 파일 시스템이 "
"자동으로 커진다는 점입니다.  그러면 이 새로운 공간을 파일 시스템에서 사용할 "
"수 있게 됩니다.  또한 ZFS는 각 파일 시스템에 서로 다른 속성을 적용할 수 있습"
"니다. 따라서 단일 모놀리식 파일 시스템 대신 별도의 파일 시스템과 데이터 세트"
"를 생성하는 데 유용합니다."

#. type: Title ==
#: documentation/content/en/books/handbook/zfs/_index.adoc:84
#, no-wrap
msgid "Quick Start Guide"
msgstr "빠른 시작 가이드"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:88
msgid ""
"FreeBSD can mount ZFS pools and datasets during system initialization.  To "
"enable it, add this line to [.filename]#/etc/rc.conf#:"
msgstr ""
"FreeBSD는 시스템 초기화 중에 ZFS 풀과 데이터세트를 마운트할 수 있습니다.  이 "
"기능을 활성화하려면 [.filename]#/etc/rc.conf# 에 다음 줄을 추가하세요:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:92
#, no-wrap
msgid "zfs_enable=\"YES\"\n"
msgstr "zfs_enable=\"YES\"\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:95
msgid "Then start the service:"
msgstr "이제 서비스를 시작합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:99
#, no-wrap
msgid "# service zfs start\n"
msgstr "# service zfs start\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:103
msgid ""
"The examples in this section assume three SCSI disks with the device names [."
"filename]#da0#, [.filename]#da1#, and [.filename]#da2#.  Users of SATA "
"hardware should instead use [.filename]#ada# device names."
msgstr ""
"이 섹션의 예제에서는 장치 이름이 [.filename]#da0#, [.filename]#da1#, [."
"filename]#da2# 인 3개의 SCSI 디스크가 있다고 가정합니다.  SATA 하드웨어 사용"
"자는 대신 [.filename]#ada# 장치 이름을 사용해야 합니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:105
#, no-wrap
msgid "Single Disk Pool"
msgstr "단일 디스크 풀"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:108
msgid "To create a simple, non-redundant pool using a single disk device:"
msgstr ""
"단일 디스크 장치를 사용하여 간단한 비중복 풀(non-redundant)을 만들려면 다음"
"과 같이 하세요:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:112
#, no-wrap
msgid "# zpool create example /dev/da0\n"
msgstr "# zpool create example /dev/da0\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:115
msgid "To view the new pool, review the output of `df`:"
msgstr "새 풀을 확인하려면 `df` 의 출력을 검토합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:124
#, no-wrap
msgid ""
"# df\n"
"Filesystem  1K-blocks    Used    Avail Capacity  Mounted on\n"
"/dev/ad0s1a   2026030  235230  1628718    13%    /\n"
"devfs               1       1        0   100%    /dev\n"
"/dev/ad0s1d  54098308 1032846 48737598     2%    /usr\n"
"example      17547136       0 17547136     0%    /example\n"
msgstr ""
"# df\n"
"Filesystem  1K-blocks    Used    Avail Capacity  Mounted on\n"
"/dev/ad0s1a   2026030  235230  1628718    13%    /\n"
"devfs               1       1        0   100%    /dev\n"
"/dev/ad0s1d  54098308 1032846 48737598     2%    /usr\n"
"example      17547136       0 17547136     0%    /example\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:128
msgid ""
"This output shows creating and mounting of the `example` pool, and that is "
"now accessible as a file system.  Create files for users to browse:"
msgstr ""
"이 출력은 이제 파일 시스템으로 액세스할 수 있는 `example` 풀의 생성 및 마운트"
"를 보여줍니다.  사용자가 찾아볼 수 있는 파일을 생성합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:139
#, no-wrap
msgid ""
"# cd /example\n"
"# ls\n"
"# touch testfile\n"
"# ls -al\n"
"total 4\n"
"drwxr-xr-x   2 root  wheel    3 Aug 29 23:15 .\n"
"drwxr-xr-x  21 root  wheel  512 Aug 29 23:12 ..\n"
"-rw-r--r--   1 root  wheel    0 Aug 29 23:15 testfile\n"
msgstr ""
"# cd /example\n"
"# ls\n"
"# touch testfile\n"
"# ls -al\n"
"total 4\n"
"drwxr-xr-x   2 root  wheel    3 Aug 29 23:15 .\n"
"drwxr-xr-x  21 root  wheel  512 Aug 29 23:12 ..\n"
"-rw-r--r--   1 root  wheel    0 Aug 29 23:15 testfile\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:143
msgid ""
"This pool is not using any advanced ZFS features and properties yet.  To "
"create a dataset on this pool with compression enabled:"
msgstr ""
"이 풀은 아직 고급 ZFS 기능 및 속성을 사용하고 있지 않습니다.  압축을 활성화"
"한 상태에서 이 풀에 데이터 세트를 생성하려면 다음과 같이 하세요:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:148
#, no-wrap
msgid ""
"# zfs create example/compressed\n"
"# zfs set compression=gzip example/compressed\n"
msgstr ""
"# zfs create example/compressed\n"
"# zfs set compression=gzip example/compressed\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:152
msgid ""
"The `example/compressed` dataset is now a ZFS compressed file system.  Try "
"copying some large files to [.filename]#/example/compressed#."
msgstr ""
"`example/compressed` 데이터 세트는 이제 ZFS 압축 파일 시스템입니다.  대용량 "
"파일을 [.filename]#/example/compressed# 에 복사해 보세요."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:154
msgid "Disable compression with:"
msgstr "다음으로 압축을 비활성화합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:158
#, no-wrap
msgid "# zfs set compression=off example/compressed\n"
msgstr "# zfs set compression=off example/compressed\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:161
msgid "To unmount a file system, use `zfs umount` and then verify with `df`:"
msgstr ""
"파일 시스템을 마운트 해제하려면 `zfs umount` 를 사용한 다음 `df` 로 확인합니"
"다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:171
#, no-wrap
msgid ""
"# zfs umount example/compressed\n"
"# df\n"
"Filesystem  1K-blocks    Used    Avail Capacity  Mounted on\n"
"/dev/ad0s1a   2026030  235232  1628716    13%    /\n"
"devfs               1       1        0   100%    /dev\n"
"/dev/ad0s1d  54098308 1032864 48737580     2%    /usr\n"
"example      17547008       0 17547008     0%    /example\n"
msgstr ""
"# zfs umount example/compressed\n"
"# df\n"
"Filesystem  1K-blocks    Used    Avail Capacity  Mounted on\n"
"/dev/ad0s1a   2026030  235232  1628716    13%    /\n"
"devfs               1       1        0   100%    /dev\n"
"/dev/ad0s1d  54098308 1032864 48737580     2%    /usr\n"
"example      17547008       0 17547008     0%    /example\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:174
msgid ""
"To re-mount the file system to make it accessible again, use `zfs mount` and "
"verify with `df`:"
msgstr ""
"파일 시스템을 다시 마운트하여 액세스할 수 있도록 하려면 `zfs mount` 를 사용하"
"고 `df` 로 확인합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:185
#, no-wrap
msgid ""
"# zfs mount example/compressed\n"
"# df\n"
"Filesystem         1K-blocks    Used    Avail Capacity  Mounted on\n"
"/dev/ad0s1a          2026030  235234  1628714    13%    /\n"
"devfs                      1       1        0   100%    /dev\n"
"/dev/ad0s1d         54098308 1032864 48737580     2%    /usr\n"
"example             17547008       0 17547008     0%    /example\n"
"example/compressed  17547008       0 17547008     0%    /example/compressed\n"
msgstr ""
"# zfs mount example/compressed\n"
"# df\n"
"Filesystem         1K-blocks    Used    Avail Capacity  Mounted on\n"
"/dev/ad0s1a          2026030  235234  1628714    13%    /\n"
"devfs                      1       1        0   100%    /dev\n"
"/dev/ad0s1d         54098308 1032864 48737580     2%    /usr\n"
"example             17547008       0 17547008     0%    /example\n"
"example/compressed  17547008       0 17547008     0%    /example/compressed\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:188
msgid "Running `mount` shows the pool and file systems:"
msgstr "`mount` 를 실행하면 풀과 파일 시스템이 표시됩니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:197
#, no-wrap
msgid ""
"# mount\n"
"/dev/ad0s1a on / (ufs, local)\n"
"devfs on /dev (devfs, local)\n"
"/dev/ad0s1d on /usr (ufs, local, soft-updates)\n"
"example on /example (zfs, local)\n"
"example/compressed on /example/compressed (zfs, local)\n"
msgstr ""
"# mount\n"
"/dev/ad0s1a on / (ufs, local)\n"
"devfs on /dev (devfs, local)\n"
"/dev/ad0s1d on /usr (ufs, local, soft-updates)\n"
"example on /example (zfs, local)\n"
"example/compressed on /example/compressed (zfs, local)\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:203
msgid ""
"Use ZFS datasets like any file system after creation.  Set other available "
"features on a per-dataset basis when needed.  The example below creates a "
"new file system called `data`.  It assumes the file system contains "
"important files and configures it to store two copies of each data block."
msgstr ""
"생성 후 다른 파일 시스템처럼 ZFS 데이터세트를 사용하세요.  필요한 경우 데이터"
"세트별로 다른 기능을 설정할 수 있습니다.  아래 예는 `data` 라는 새 파일 시스"
"템을 생성합니다.  파일 시스템에 중요한 파일이 포함되어 있다고 가정하고 각 데"
"이터 블록의 사본 두 개를 저장하도록 구성합니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:208
#, no-wrap
msgid ""
"# zfs create example/data\n"
"# zfs set copies=2 example/data\n"
msgstr ""
"# zfs create example/data\n"
"# zfs set copies=2 example/data\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:211
msgid "Use `df` to see the data and space usage:"
msgstr "데이터 및 공간 사용량을 확인하려면 `df` 를 사용합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:222
#, no-wrap
msgid ""
"# df\n"
"Filesystem         1K-blocks    Used    Avail Capacity  Mounted on\n"
"/dev/ad0s1a          2026030  235234  1628714    13%    /\n"
"devfs                      1       1        0   100%    /dev\n"
"/dev/ad0s1d         54098308 1032864 48737580     2%    /usr\n"
"example             17547008       0 17547008     0%    /example\n"
"example/compressed  17547008       0 17547008     0%    /example/compressed\n"
"example/data        17547008       0 17547008     0%    /example/data\n"
msgstr ""
"# df\n"
"Filesystem         1K-blocks    Used    Avail Capacity  Mounted on\n"
"/dev/ad0s1a          2026030  235234  1628714    13%    /\n"
"devfs                      1       1        0   100%    /dev\n"
"/dev/ad0s1d         54098308 1032864 48737580     2%    /usr\n"
"example             17547008       0 17547008     0%    /example\n"
"example/compressed  17547008       0 17547008     0%    /example/compressed\n"
"example/data        17547008       0 17547008     0%    /example/data\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:227
msgid ""
"Notice that all file systems in the pool have the same available space.  "
"Using `df` in these examples shows that the file systems use the space they "
"need and all draw from the same pool.  ZFS gets rid of concepts such as "
"volumes and partitions, and allows several file systems to share the same "
"pool."
msgstr ""
"풀에 있는 모든 파일 시스템의 사용 가능한 공간이 동일하다는 것을 알 수 있습니"
"다.  이 예제에서 `df` 를 사용하면 파일 시스템이 필요한 공간을 사용하며 모두 "
"동일한 풀에서 끌어온다는 것을 알 수 있습니다.  ZFS는 볼륨 및 파티션과 같은 개"
"념을 없애고 여러 파일 시스템이 동일한 풀을 공유할 수 있도록 합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:229
msgid "To destroy the file systems and then the pool that is no longer needed:"
msgstr "파일 시스템을 삭제한 다음 더 이상 필요하지 않은 풀을 삭제합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:235
#, no-wrap
msgid ""
"# zfs destroy example/compressed\n"
"# zfs destroy example/data\n"
"# zpool destroy example\n"
msgstr ""
"# zfs destroy example/compressed\n"
"# zfs destroy example/data\n"
"# zpool destroy example\n"

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:238
#, no-wrap
msgid "RAID-Z"
msgstr "RAID-Z"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:244
msgid ""
"Disks fail.  One way to avoid data loss from disk failure is to use RAID.  "
"ZFS supports this feature in its pool design.  RAID-Z pools require three or "
"more disks but provide more usable space than mirrored pools."
msgstr ""
"디스크 장애.  디스크 장애로 인한 데이터 손실을 방지하는 한 가지 방법은 RAID"
"를 사용하는 것입니다.  ZFS는 풀 설계에서 이 기능을 지원합니다.  RAID-Z 풀은 3"
"개 이상의 디스크가 필요하지만 미러 풀보다 더 많은 사용 가능한 공간을 제공합니"
"다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:246
msgid ""
"This example creates a RAID-Z pool, specifying the disks to add to the pool:"
msgstr "이 예에서는 풀에 추가할 디스크를 지정하여 RAID-Z 풀을 생성합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:250
#, no-wrap
msgid "# zpool create storage raidz da0 da1 da2\n"
msgstr "# zpool create storage raidz da0 da1 da2\n"

#. type: delimited block = 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:258
msgid ""
"Sun(TM) recommends that the number of devices used in a RAID-Z configuration "
"be between three and nine.  For environments requiring a single pool "
"consisting of 10 disks or more, consider breaking it up into smaller RAID-Z "
"groups.  If two disks are available, ZFS mirroring provides redundancy if "
"required.  Refer to man:zpool[8] for more details."
msgstr ""
"Sun(TM)에서는 RAID-Z 구성에 사용되는 장치 수를 3개에서 9개 사이로 권장합니"
"다.  10개 이상의 디스크로 구성된 단일 풀이 필요한 환경에서는 이를 더 작은 "
"RAID-Z 그룹으로 분할하는 것이 좋습니다.  두 개의 디스크를 사용할 수 있는 경"
"우, 필요한 경우 ZFS 미러링이 중복성을 제공합니다.  자세한 내용은 man:zpool[8]"
"을 참조하세요."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:262
msgid ""
"The previous example created the `storage` zpool.  This example makes a new "
"file system called `home` in that pool:"
msgstr ""
"이전 예제에서는 `storage` zpool을 생성했습니다.  이 예제에서는 해당 풀에 "
"`home` 이라는 새 파일 시스템을 만듭니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:266
#, no-wrap
msgid "# zfs create storage/home\n"
msgstr "# zfs create storage/home\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:269
msgid "Enable compression and store an extra copy of directories and files:"
msgstr "압축을 활성화하고 디렉토리와 파일의 추가 복사본을 저장합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:274
#, no-wrap
msgid ""
"# zfs set copies=2 storage/home\n"
"# zfs set compression=gzip storage/home\n"
msgstr ""
"# zfs set copies=2 storage/home\n"
"# zfs set compression=gzip storage/home\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:277
msgid ""
"To make this the new home directory for users, copy the user data to this "
"directory and create the appropriate symbolic links:"
msgstr ""
"이 디렉터리를 사용자의 새 홈 디렉터리로 설정하려면 사용자 데이터를 이 디렉터"
"리에 복사하고 적절한 심볼릭 링크를 만듭니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:284
#, no-wrap
msgid ""
"# cp -rp /home/* /storage/home\n"
"# rm -rf /home /usr/home\n"
"# ln -s /storage/home /home\n"
"# ln -s /storage/home /usr/home\n"
msgstr ""
"# cp -rp /home/* /storage/home\n"
"# rm -rf /home /usr/home\n"
"# ln -s /storage/home /home\n"
"# ln -s /storage/home /usr/home\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:288
msgid ""
"Users data is now stored on the freshly-created [.filename]#/storage/home#.  "
"Test by adding a new user and logging in as that user."
msgstr ""
"이제 사용자 데이터가 새로 생성된 [.filename]#/storage/home# 에 저장됩니다.  "
"새 사용자를 추가하고 해당 사용자로 로그인하여 테스트합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:290
msgid "Create a file system snapshot to roll back to later:"
msgstr "나중에 롤백할 파일 시스템 스냅샷을 만듭니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:294
#, no-wrap
msgid "# zfs snapshot storage/home@08-30-08\n"
msgstr "# zfs snapshot storage/home@08-30-08\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:297
msgid "ZFS creates snapshots of a dataset, not a single directory or file."
msgstr "ZFS는 단일 디렉토리나 파일이 아닌 데이터 세트의 스냅샷을 생성합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:300
msgid ""
"The `@` character is a delimiter between the file system name or the volume "
"name.  Before deleting an important directory, back up the file system, then "
"roll back to an earlier snapshot in which the directory still exists:"
msgstr ""
"`@` 문자는 파일 시스템 이름 또는 볼륨 이름 사이의 구분 기호입니다.  중요한 디"
"렉터리를 삭제하기 전에 파일 시스템을 백업한 다음 디렉터리가 아직 존재하는 이"
"전 스냅샷으로 롤백하세요:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:304
#, no-wrap
msgid "# zfs rollback storage/home@08-30-08\n"
msgstr "# zfs rollback storage/home@08-30-08\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:308
msgid ""
"To list all available snapshots, run `ls` in the file system's [.filename]#."
"zfs/snapshot# directory.  For example, to see the snapshot taken:"
msgstr ""
"사용 가능한 모든 스냅샷을 나열하려면 파일 시스템의 [.filename]#.zfs/"
"snapshot# 디렉터리에서 `ls` 를 실행합니다.  예를 들어, 생성된 스냅샷을 확인하"
"려면 다음과 같이 하세요:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:312
#, no-wrap
msgid "# ls /storage/home/.zfs/snapshot\n"
msgstr "# ls /storage/home/.zfs/snapshot\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:317
msgid ""
"Write a script to take regular snapshots of user data.  Over time, snapshots "
"can use up a lot of disk space.  Remove the previous snapshot using the "
"command:"
msgstr ""
"사용자 데이터의 정기적인 스냅샷을 생성하는 스크립트를 작성하세요.  시간이 지"
"나면 스냅샷이 디스크 공간을 많이 차지할 수 있습니다.  명령을 사용하여 이전 스"
"냅샷을 제거하세요:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:321
#, no-wrap
msgid "# zfs destroy storage/home@08-30-08\n"
msgstr "# zfs destroy storage/home@08-30-08\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:325
msgid ""
"After testing, make [.filename]#/storage/home# the real [.filename]#/home# "
"with this command:"
msgstr ""
"테스트가 끝나면 다음 명령을 사용하여 [.filename]#/storage/home# 을 실제 [."
"filename]#/home# 으로 만듭니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:329
#, no-wrap
msgid "# zfs set mountpoint=/home storage/home\n"
msgstr "# zfs set mountpoint=/home storage/home\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:332
msgid ""
"Run `df` and `mount` to confirm that the system now treats the file system "
"as the real [.filename]#/home#:"
msgstr ""
"`df` 및 `mount` 를 실행하여 시스템이 이제 파일 시스템을 실제 [.filename]#/"
"home# 으로 취급하는지 확인합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:348
#, no-wrap
msgid ""
"# mount\n"
"/dev/ad0s1a on / (ufs, local)\n"
"devfs on /dev (devfs, local)\n"
"/dev/ad0s1d on /usr (ufs, local, soft-updates)\n"
"storage on /storage (zfs, local)\n"
"storage/home on /home (zfs, local)\n"
"# df\n"
"Filesystem   1K-blocks    Used    Avail Capacity  Mounted on\n"
"/dev/ad0s1a    2026030  235240  1628708    13%    /\n"
"devfs                1       1        0   100%    /dev\n"
"/dev/ad0s1d   54098308 1032826 48737618     2%    /usr\n"
"storage       26320512       0 26320512     0%    /storage\n"
"storage/home  26320512       0 26320512     0%    /home\n"
msgstr ""
"# mount\n"
"/dev/ad0s1a on / (ufs, local)\n"
"devfs on /dev (devfs, local)\n"
"/dev/ad0s1d on /usr (ufs, local, soft-updates)\n"
"storage on /storage (zfs, local)\n"
"storage/home on /home (zfs, local)\n"
"# df\n"
"Filesystem   1K-blocks    Used    Avail Capacity  Mounted on\n"
"/dev/ad0s1a    2026030  235240  1628708    13%    /\n"
"devfs                1       1        0   100%    /dev\n"
"/dev/ad0s1d   54098308 1032826 48737618     2%    /usr\n"
"storage       26320512       0 26320512     0%    /storage\n"
"storage/home  26320512       0 26320512     0%    /home\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:352
msgid ""
"This completes the RAID-Z configuration.  Add daily status updates about the "
"created file systems to the nightly man:periodic[8] runs by adding this line "
"to [.filename]#/etc/periodic.conf#:"
msgstr ""
"이것으로 RAID-Z 구성이 완료됩니다.  생성된 파일 시스템에 대한 일일 상태 업데"
"이트를 [.filename]#/etc/periodic.conf# 에 다음 줄을 추가하여 야간 man:"
"periodic[8] 작업에 추가합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:356
#, no-wrap
msgid "daily_status_zfs_enable=\"YES\"\n"
msgstr "daily_status_zfs_enable=\"YES\"\n"

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:359
#, no-wrap
msgid "Recovering RAID-Z"
msgstr "RAID-Z 복구하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:363
msgid ""
"Every software RAID has a method of monitoring its `state`.  View the status "
"of RAID-Z devices using:"
msgstr ""
"모든 소프트웨어 RAID에는 `state` 를 모니터링하는 방법이 있습니다.  다음을 사"
"용하여 RAID-Z 장치의 상태를 확인합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:367
#, no-wrap
msgid "# zpool status -x\n"
msgstr "# zpool status -x\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:370
#, fuzzy
#| msgid ""
#| "If all pools are <<zfs-term-online,Online>> and everything is normal, the "
#| "message shows:"
msgid ""
"If all pools are crossref:zfs[zfs-term-online,Online] and everything is "
"normal, the message shows:"
msgstr ""
"모든 풀이 <<zfs-term-online,Online>> 이고 모든 것이 정상인 경우 메시지가 표시"
"됩니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:374
#, no-wrap
msgid "all pools are healthy\n"
msgstr "all pools are healthy\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:378
#, fuzzy
#| msgid ""
#| "If there is a problem, perhaps a disk being in the <<zfs-term-offline,"
#| "Offline>> state, the pool state will look like this:"
msgid ""
"If there is a problem, perhaps a disk being in the crossref:zfs[zfs-term-"
"offline,Offline] state, the pool state will look like this:"
msgstr ""
"디스크에 문제가 있는 경우(예: 디스크가 <<zfs-term-offline,Offline>> 상태인 경"
"우) 풀 상태는 다음과 같이 표시됩니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:390
#, no-wrap
msgid ""
"  pool: storage\n"
" state: DEGRADED\n"
"status: One or more devices has been taken offline by the administrator.\n"
"\tSufficient replicas exist for the pool to continue functioning in a\n"
"\tdegraded state.\n"
"action: Online the device using 'zpool online' or replace the device with\n"
"\t'zpool replace'.\n"
" scrub: none requested\n"
"config:\n"
msgstr ""
"  pool: storage\n"
" state: DEGRADED\n"
"status: One or more devices has been taken offline by the administrator.\n"
"\tSufficient replicas exist for the pool to continue functioning in a\n"
"\tdegraded state.\n"
"action: Online the device using 'zpool online' or replace the device with\n"
"\t'zpool replace'.\n"
" scrub: none requested\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:397
#, no-wrap
msgid ""
"\tNAME        STATE     READ WRITE CKSUM\n"
"\tstorage     DEGRADED     0     0     0\n"
"\t  raidz1    DEGRADED     0     0     0\n"
"\t    da0     ONLINE       0     0     0\n"
"\t    da1     OFFLINE      0     0     0\n"
"\t    da2     ONLINE       0     0     0\n"
msgstr ""
"\tNAME        STATE     READ WRITE CKSUM\n"
"\tstorage     DEGRADED     0     0     0\n"
"\t  raidz1    DEGRADED     0     0     0\n"
"\t    da0     ONLINE       0     0     0\n"
"\t    da1     OFFLINE      0     0     0\n"
"\t    da2     ONLINE       0     0     0\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:399
#: documentation/content/en/books/handbook/zfs/_index.adoc:434
#: documentation/content/en/books/handbook/zfs/_index.adoc:480
#: documentation/content/en/books/handbook/zfs/_index.adoc:525
#: documentation/content/en/books/handbook/zfs/_index.adoc:548
#: documentation/content/en/books/handbook/zfs/_index.adoc:580
#: documentation/content/en/books/handbook/zfs/_index.adoc:659
#: documentation/content/en/books/handbook/zfs/_index.adoc:713
#: documentation/content/en/books/handbook/zfs/_index.adoc:750
#: documentation/content/en/books/handbook/zfs/_index.adoc:779
#: documentation/content/en/books/handbook/zfs/_index.adoc:859
#: documentation/content/en/books/handbook/zfs/_index.adoc:935
#: documentation/content/en/books/handbook/zfs/_index.adoc:967
#: documentation/content/en/books/handbook/zfs/_index.adoc:1067
#: documentation/content/en/books/handbook/zfs/_index.adoc:1111
#: documentation/content/en/books/handbook/zfs/_index.adoc:1136
#: documentation/content/en/books/handbook/zfs/_index.adoc:1157
#, no-wrap
msgid "errors: No known data errors\n"
msgstr "errors: No known data errors\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:402
msgid ""
"\"OFFLINE\" shows the administrator took [.filename]#da1# offline using:"
msgstr ""
"\"OFFLINE\"은 관리자가 [.filename]#da1# 을 오프라인으로 전환했음을 나타냅니"
"다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:406
#, no-wrap
msgid "# zpool offline storage da1\n"
msgstr "# zpool offline storage da1\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:410
msgid ""
"Power down the computer now and replace [.filename]#da1#.  Power up the "
"computer and return [.filename]#da1# to the pool:"
msgstr ""
"지금 컴퓨터의 전원을 끄고 [.filename]#da1# 을 바꾸세요.  컴퓨터의 전원을 켜"
"고 [.filename]#da1# 을 풀로 되돌립니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:414
#, no-wrap
msgid "# zpool replace storage da1\n"
msgstr "# zpool replace storage da1\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:417
msgid ""
"Next, check the status again, this time without `-x` to display all pools:"
msgstr ""
"그런 다음 이번에는 `-x` 없이 상태를 다시 확인하여 모든 풀을 표시합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:425
#, no-wrap
msgid ""
"# zpool status storage\n"
" pool: storage\n"
" state: ONLINE\n"
" scrub: resilver completed with 0 errors on Sat Aug 30 19:44:11 2008\n"
"config:\n"
msgstr ""
"# zpool status storage\n"
" pool: storage\n"
" state: ONLINE\n"
" scrub: resilver completed with 0 errors on Sat Aug 30 19:44:11 2008\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:432
#: documentation/content/en/books/handbook/zfs/_index.adoc:478
#, no-wrap
msgid ""
"\tNAME        STATE     READ WRITE CKSUM\n"
"\tstorage     ONLINE       0     0     0\n"
"\t  raidz1    ONLINE       0     0     0\n"
"\t    da0     ONLINE       0     0     0\n"
"\t    da1     ONLINE       0     0     0\n"
"\t    da2     ONLINE       0     0     0\n"
msgstr ""
"\tNAME        STATE     READ WRITE CKSUM\n"
"\tstorage     ONLINE       0     0     0\n"
"\t  raidz1    ONLINE       0     0     0\n"
"\t    da0     ONLINE       0     0     0\n"
"\t    da1     ONLINE       0     0     0\n"
"\t    da2     ONLINE       0     0     0\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:437
msgid "In this example, everything is normal."
msgstr "이 예에서는 모든 것이 정상입니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:439
#, no-wrap
msgid "Data Verification"
msgstr "데이터 검증"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:443
msgid ""
"ZFS uses checksums to verify the integrity of stored data.  Creating file "
"systems automatically enables them."
msgstr ""
"ZFS는 체크섬을 사용하여 저장된 데이터의 무결성을 확인합니다.  파일 시스템을 "
"생성하면 자동으로 활성화됩니다."

#. type: delimited block = 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:450
msgid ""
"Disabling Checksums is possible but _not_ recommended! Checksums take little "
"storage space and provide data integrity.  Most ZFS features will not work "
"properly with checksums disabled.  Disabling these checksums will not "
"increase performance noticeably."
msgstr ""
"체크섬을 비활성화하는 것은 가능하지만 _권장하지 않습니다!_ 체크섬은 저장 공간"
"을 거의 차지하지 않고 데이터 무결성을 제공합니다.  체크섬을 비활성화하면 대부"
"분의 ZFS 기능이 제대로 작동하지 않습니다.  체크섬을 비활성화해도 성능이 눈에 "
"띄게 향상되지는 않습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:453
msgid ""
"Verifying the data checksums (called _scrubbing_) ensures integrity of the "
"`storage` pool with:"
msgstr ""
"데이터 체크섬 확인( _scrubbing_ 이라고 함)을 하면 `storage` 풀의 무결성이 보"
"장됩니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:457
#, no-wrap
msgid "# zpool scrub storage\n"
msgstr "# zpool scrub storage\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:463
msgid ""
"The duration of a scrub depends on the amount of data stored.  Larger "
"amounts of data will take proportionally longer to verify.  Since scrubbing "
"is I/O intensive, ZFS allows a single scrub to run at a time.  After "
"scrubbing completes, view the status with `zpool status`:"
msgstr ""
"스크럽 기간은 저장된 데이터의 양에 따라 다릅니다.  데이터 양이 많을수록 검증"
"하는 데 비례해 더 오래 걸립니다.  스크러빙은 I/O 집약적이기 때문에, ZFS에서"
"는 한 번에 하나의 스크러브만 실행할 수 있습니다.  스크러빙이 완료되면 `zpool "
"status` 로 상태를 확인합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:471
#, no-wrap
msgid ""
"# zpool status storage\n"
" pool: storage\n"
" state: ONLINE\n"
" scrub: scrub completed with 0 errors on Sat Jan 26 19:57:37 2013\n"
"config:\n"
msgstr ""
"# zpool status storage\n"
" pool: storage\n"
" state: ONLINE\n"
" scrub: scrub completed with 0 errors on Sat Jan 26 19:57:37 2013\n"
"config:\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:484
msgid ""
"Displaying the completion date of the last scrubbing helps decide when to "
"start another.  Routine scrubs help protect data from silent corruption and "
"ensure the integrity of the pool."
msgstr ""
"마지막 스크러빙의 완료 날짜를 표시하면 다른 스크러빙을 시작할 시기를 결정하"
"는 데 도움이 됩니다.  정기적인 스크러빙은 조용한 손상으로부터 데이터를 보호하"
"고 풀의 무결성을 보장하는 데 도움이 됩니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:486
msgid "Refer to man:zfs[8] and man:zpool[8] for other ZFS options."
msgstr "다른 ZFS 옵션은 man:zfs[8] 및 man:zpool[8]을 참조하세요."

#. type: Title ==
#: documentation/content/en/books/handbook/zfs/_index.adoc:488
#, no-wrap
msgid "`zpool` Administration"
msgstr "`zpool` 관리"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:495
#, fuzzy
#| msgid ""
#| "ZFS administration uses two main utilities.  The `zpool` utility controls "
#| "the operation of the pool and allows adding, removing, replacing, and "
#| "managing disks.  The <<zfs-zfs,`zfs`>> utility allows creating, "
#| "destroying, and managing datasets, both <<zfs-term-filesystem,file "
#| "systems>> and <<zfs-term-volume,volumes>>."
msgid ""
"ZFS administration uses two main utilities.  The `zpool` utility controls "
"the operation of the pool and allows adding, removing, replacing, and "
"managing disks.  The crossref:zfs[zfs-zfs,`zfs`] utility allows creating, "
"destroying, and managing datasets, both crossref:zfs[zfs-term-filesystem,"
"file systems] and crossref:zfs[zfs-term-volume,volumes]."
msgstr ""
"ZFS 관리는 두 가지 주요 유틸리티를 사용합니다.  `zpool` 유틸리티는 풀의 작동"
"을 제어하고 디스크를 추가, 제거, 교체 및 관리할 수 있습니다.  <<zfs-zfs,"
"`zfs`>> 유틸리티를 사용하면 <<zfs-term-filesystem,file systems>> 및 <<zfs-"
"term-volume,volumes>> 데이터 세트를 생성, 삭제 및 관리할 수 있습니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:497
#, no-wrap
msgid "Creating and Destroying Storage Pools"
msgstr "스토리지 풀 만들기 및 삭제하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:506
#, fuzzy
#| msgid ""
#| "Creating a ZFS storage pool requires permanent decisions, as the pool "
#| "structure cannot change after creation.  The most important decision is "
#| "which types of vdevs to group the physical disks into.  See the list of "
#| "<<zfs-term-vdev,vdev types>> for details about the possible options.  "
#| "After creating the pool, most vdev types do not allow adding disks to the "
#| "vdev.  The exceptions are mirrors, which allow adding new disks to the "
#| "vdev, and stripes, which upgrade to mirrors by attaching a new disk to "
#| "the vdev.  Although adding new vdevs expands a pool, the pool layout "
#| "cannot change after pool creation.  Instead, back up the data, destroy "
#| "the pool, and recreate it."
msgid ""
"Creating a ZFS storage pool requires permanent decisions, as the pool "
"structure cannot change after creation.  The most important decision is "
"which types of vdevs to group the physical disks into.  See the list of "
"crossref:zfs[zfs-term-vdev,vdev types] for details about the possible "
"options.  After creating the pool, most vdev types do not allow adding disks "
"to the vdev.  The exceptions are mirrors, which allow adding new disks to "
"the vdev, and stripes, which upgrade to mirrors by attaching a new disk to "
"the vdev.  Although adding new vdevs expands a pool, the pool layout cannot "
"change after pool creation.  Instead, back up the data, destroy the pool, "
"and recreate it."
msgstr ""
"ZFS 스토리지 풀을 생성하려면 풀 구조를 생성한 후에는 변경할 수 없으므로 영구"
"적 결정이 필요합니다.  가장 중요한 결정은 물리적 디스크를 그룹화할 vdev 유형"
"입니다.  가능한 옵션에 대한 자세한 내용은 <<zfs-term-vdev,vdev types>> 목록"
"을 참조하세요.  풀을 생성한 후에는 대부분의 vdev 유형에서 디스크를 vdev에 추"
"가할 수 없습니다.  예외적으로 미러는 vdev에 새 디스크를 추가할 수 있으며, 스"
"트라이프는 vdev에 새 디스크를 연결하여 미러로 업그레이드할 수 있습니다.  새 "
"vdev를 추가하면 풀이 확장되지만 풀 생성 후에는 풀 레이아웃을 변경할 수 없습니"
"다.  대신 데이터를 백업하고 풀을 파괴한 다음 다시 생성해야 합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:508
msgid "Create a simple mirror pool:"
msgstr "간단한 미러 풀을 만듭니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:517
#, no-wrap
msgid ""
"# zpool create mypool mirror /dev/ada1 /dev/ada2\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: none requested\n"
"config:\n"
msgstr ""
"# zpool create mypool mirror /dev/ada1 /dev/ada2\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: none requested\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:523
#, no-wrap
msgid ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          mirror-0  ONLINE       0     0     0\n"
"            ada1    ONLINE       0     0     0\n"
"            ada2    ONLINE       0     0     0\n"
msgstr ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          mirror-0  ONLINE       0     0     0\n"
"            ada1    ONLINE       0     0     0\n"
"            ada2    ONLINE       0     0     0\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:528
msgid ""
"To create more than one vdev with a single command, specify groups of disks "
"separated by the vdev type keyword, `mirror` in this example:"
msgstr ""
"단일 명령으로 둘 이상의 vdev를 생성하려면 예제에서 vdev 유형 키워드인 "
"`mirror` 로 구분된 디스크 그룹을 지정합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:537
#, no-wrap
msgid ""
"# zpool create mypool mirror /dev/ada1 /dev/ada2 mirror /dev/ada3 /dev/ada4\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: none requested\n"
"config:\n"
msgstr ""
"# zpool create mypool mirror /dev/ada1 /dev/ada2 mirror /dev/ada3 /dev/ada4\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: none requested\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:546
#, no-wrap
msgid ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          mirror-0  ONLINE       0     0     0\n"
"            ada1    ONLINE       0     0     0\n"
"            ada2    ONLINE       0     0     0\n"
"          mirror-1  ONLINE       0     0     0\n"
"            ada3    ONLINE       0     0     0\n"
"            ada4    ONLINE       0     0     0\n"
msgstr ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          mirror-0  ONLINE       0     0     0\n"
"            ada1    ONLINE       0     0     0\n"
"            ada2    ONLINE       0     0     0\n"
"          mirror-1  ONLINE       0     0     0\n"
"            ada3    ONLINE       0     0     0\n"
"            ada4    ONLINE       0     0     0\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:557
msgid ""
"Pools can also use partitions rather than whole disks.  Putting ZFS in a "
"separate partition allows the same disk to have other partitions for other "
"purposes.  In particular, it allows adding partitions with bootcode and file "
"systems needed for booting.  This allows booting from disks that are also "
"members of a pool.  ZFS adds no performance penalty on FreeBSD when using a "
"partition rather than a whole disk.  Using partitions also allows the "
"administrator to _under-provision_ the disks, using less than the full "
"capacity.  If a future replacement disk of the same nominal size as the "
"original actually has a slightly smaller capacity, the smaller partition "
"will still fit, using the replacement disk."
msgstr ""
"풀은 전체 디스크가 아닌 파티션 형태로 사용할 수도 있습니다.  ZFS를 별도의 파"
"티션에 넣으면 동일한 디스크에 다른 용도의 다른 파티션을 만들 수 있습니다.  특"
"히 부팅에 필요한 부트코드와 파일 시스템이 있는 파티션을 추가할 수 있습니다.  "
"이렇게 하면 풀의 구성원이기도 한 디스크에서 부팅할 수 있습니다.  ZFS는 전체 "
"디스크가 아닌 파티션을 사용할 때 FreeBSD에 성능 저하를 주지 않습니다.  또한 "
"파티션을 사용하면 관리자가 전체 용량보다 적은 용량을 사용하여 디스크를 _언더 "
"프로비저닝_ 할 수 있습니다.  나중에 원본과 명목상 동일한 크기의 대체 디스크"
"가 실제로는 약간 더 작은 용량을 가지고 있다면, 대체 디스크를 사용하기 위해 "
"더 작은 파티션이 적합합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:559
#, fuzzy
#| msgid "Create a <<zfs-term-vdev-raidz,RAID-Z2>> pool using partitions:"
msgid ""
"Create a crossref:zfs[zfs-term-vdev-raidz,RAID-Z2] pool using partitions:"
msgstr "파티션을 사용하여 <<zfs-term-vdev-raidz,RAID-Z2>> 풀을 생성합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:568
#, no-wrap
msgid ""
"# zpool create mypool raidz2 /dev/ada0p3 /dev/ada1p3 /dev/ada2p3 /dev/ada3p3 /dev/ada4p3 /dev/ada5p3\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: none requested\n"
"config:\n"
msgstr ""
"# zpool create mypool raidz2 /dev/ada0p3 /dev/ada1p3 /dev/ada2p3 /dev/ada3p3 /dev/ada4p3 /dev/ada5p3\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: none requested\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:578
#: documentation/content/en/books/handbook/zfs/_index.adoc:777
#: documentation/content/en/books/handbook/zfs/_index.adoc:965
#, no-wrap
msgid ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          raidz2-0  ONLINE       0     0     0\n"
"            ada0p3  ONLINE       0     0     0\n"
"            ada1p3  ONLINE       0     0     0\n"
"            ada2p3  ONLINE       0     0     0\n"
"            ada3p3  ONLINE       0     0     0\n"
"            ada4p3  ONLINE       0     0     0\n"
"            ada5p3  ONLINE       0     0     0\n"
msgstr ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          raidz2-0  ONLINE       0     0     0\n"
"            ada0p3  ONLINE       0     0     0\n"
"            ada1p3  ONLINE       0     0     0\n"
"            ada2p3  ONLINE       0     0     0\n"
"            ada3p3  ONLINE       0     0     0\n"
"            ada4p3  ONLINE       0     0     0\n"
"            ada5p3  ONLINE       0     0     0\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:587
msgid ""
"Destroy a pool that is no longer needed to reuse the disks.  Destroying a "
"pool requires unmounting the file systems in that pool first.  If any "
"dataset is in use, the unmount operation fails without destroying the pool.  "
"Force the pool destruction with `-f`.  This can cause undefined behavior in "
"applications which had open files on those datasets."
msgstr ""
"디스크를 재사용하기 위해 더 이상 필요하지 않은 풀을 삭제합니다.  풀을 삭제하"
"려면 먼저 해당 풀의 파일 시스템을 마운트 해제해야 합니다.  사용 중인 데이터 "
"세트가 있으면 풀을 파괴하지 않으며 마운트 해제 작업이 실패합니다.  `-f` 를 사"
"용하여 풀을 강제 삭제합니다.  이로 인해 해당 데이터세트에 파일이 열려 있는 애"
"플리케이션에서 정의되지 않은 동작이 발생할 수 있습니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:589
#, no-wrap
msgid "Adding and Removing Devices"
msgstr "장치 추가 및 제거하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:593
#, fuzzy
#| msgid ""
#| "Two ways exist for adding disks to a pool: attaching a disk to an "
#| "existing vdev with `zpool attach`, or adding vdevs to the pool with "
#| "`zpool add`.  Some <<zfs-term-vdev,vdev types>> allow adding disks to the "
#| "vdev after creation."
msgid ""
"Two ways exist for adding disks to a pool: attaching a disk to an existing "
"vdev with `zpool attach`, or adding vdevs to the pool with `zpool add`.  "
"Some crossref:zfs[zfs-term-vdev,vdev types] allow adding disks to the vdev "
"after creation."
msgstr ""
"풀에 디스크를 추가하는 방법은 두 가지가 있습니다: `zpool attach` 를 사용하여 "
"기존 vdev에 디스크를 연결하거나 `zpool add` 를 사용하여 풀에 vdev를 추가하는 "
"것입니다.  일부 <<zfs-term-vdev,vdev types>> 은 생성 후 디스크를 vdev에 추가"
"할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:602
#, fuzzy
#| msgid ""
#| "A pool created with a single disk lacks redundancy.  It can detect "
#| "corruption but can not repair it, because there is no other copy of the "
#| "data.  The <<zfs-term-copies,copies>> property may be able to recover "
#| "from a small failure such as a bad sector, but does not provide the same "
#| "level of protection as mirroring or RAID-Z.  Starting with a pool "
#| "consisting of a single disk vdev, use `zpool attach` to add a new disk to "
#| "the vdev, creating a mirror.  Also use `zpool attach` to add new disks to "
#| "a mirror group, increasing redundancy and read performance.  When "
#| "partitioning the disks used for the pool, replicate the layout of the "
#| "first disk on to the second.  Use `gpart backup` and `gpart restore` to "
#| "make this process easier."
msgid ""
"A pool created with a single disk lacks redundancy.  It can detect "
"corruption but can not repair it, because there is no other copy of the "
"data.  The crossref:zfs[zfs-term-copies,copies] property may be able to "
"recover from a small failure such as a bad sector, but does not provide the "
"same level of protection as mirroring or RAID-Z.  Starting with a pool "
"consisting of a single disk vdev, use `zpool attach` to add a new disk to "
"the vdev, creating a mirror.  Also use `zpool attach` to add new disks to a "
"mirror group, increasing redundancy and read performance.  When partitioning "
"the disks used for the pool, replicate the layout of the first disk on to "
"the second.  Use `gpart backup` and `gpart restore` to make this process "
"easier."
msgstr ""
"단일 디스크로 생성된 풀은 중복성이 부족합니다.  데이터의 다른 복사본이 없기 "
"때문에 손상을 감지할 수는 있지만 복구할 수는 없습니다.  <<zfs-term-copies,"
"copies>> 속성은 불량 섹터와 같은 작은 장애로부터 복구할 수 있지만 미러링 또"
"는 RAID-Z와 동일한 수준의 보호 기능을 제공하지는 않습니다.  단일 디스크 vdev"
"로 구성된 풀에서 시작하여 `zpool attach` 를 사용하여 새 디스크를 vdev에 추가"
"하고 미러를 생성합니다.  또한 `zpool attach` 를 사용하여 미러 그룹에 새 디스"
"크를 추가하여 중복성 및 읽기 성능을 향상시킬 수 있습니다.  풀에 사용되는 디스"
"크를 파티션할 때 첫 번째 디스크의 레이아웃을 두 번째 디스크에 복제합니다.  "
"이 프로세스를 더 쉽게 수행하려면 `gpart backup` 및 `gpart restore` 을 사용합"
"니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:604
msgid ""
"Upgrade the single disk (stripe) vdev [.filename]#ada0p3# to a mirror by "
"attaching [.filename]#ada1p3#:"
msgstr ""
"단일 디스크(스트라이프) vdev [.filename]#ada0p3# 을 [.filename]#ada1p3# 에 연"
"결하여 미러로 업그레이드합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:612
#: documentation/content/en/books/handbook/zfs/_index.adoc:809
#, no-wrap
msgid ""
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: none requested\n"
"config:\n"
msgstr ""
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: none requested\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:616
#, no-wrap
msgid ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          ada0p3    ONLINE       0     0     0\n"
msgstr ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          ada0p3    ONLINE       0     0     0\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:620
#, no-wrap
msgid ""
"errors: No known data errors\n"
"# zpool attach mypool ada0p3 ada1p3\n"
"Make sure to wait until resilvering finishes before rebooting.\n"
msgstr ""
"errors: No known data errors\n"
"# zpool attach mypool ada0p3 ada1p3\n"
"Make sure to wait until resilvering finishes before rebooting.\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:622
#, no-wrap
msgid "If you boot from pool 'mypool', you may need to update boot code on newly attached disk _ada1p3_.\n"
msgstr "`mypool` 풀에서 부팅하는 경우, 새로 연결된 디스크 _ada1p3_ 에서 부트 코드를 업데이트해야 할 수 있습니다.\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:624
#, no-wrap
msgid "Assuming you use GPT partitioning and _da0_ is your new boot disk you may use the following command:\n"
msgstr "GPT 파티션을 사용하고 _da0_ 이 새 부팅 디스크라고 가정하면 다음 명령을 사용할 수 있습니다:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:638
#, no-wrap
msgid ""
"        gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 da0\n"
"# gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada1\n"
"bootcode written to ada1\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"status: One or more devices is currently being resilvered.  The pool will\n"
"        continue to function, possibly in a degraded state.\n"
"action: Wait for the resilver to complete.\n"
"  scan: resilver in progress since Fri May 30 08:19:19 2014\n"
"        527M scanned out of 781M at 47.9M/s, 0h0m to go\n"
"        527M resilvered, 67.53% done\n"
"config:\n"
msgstr ""
"        gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 da0\n"
"# gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada1\n"
"bootcode written to ada1\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"status: One or more devices is currently being resilvered.  The pool will\n"
"        continue to function, possibly in a degraded state.\n"
"action: Wait for the resilver to complete.\n"
"  scan: resilver in progress since Fri May 30 08:19:19 2014\n"
"        527M scanned out of 781M at 47.9M/s, 0h0m to go\n"
"        527M resilvered, 67.53% done\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:644
#, no-wrap
msgid ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          mirror-0  ONLINE       0     0     0\n"
"            ada0p3  ONLINE       0     0     0\n"
"            ada1p3  ONLINE       0     0     0  (resilvering)\n"
msgstr ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          mirror-0  ONLINE       0     0     0\n"
"            ada0p3  ONLINE       0     0     0\n"
"            ada1p3  ONLINE       0     0     0  (resilvering)\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:651
#, no-wrap
msgid ""
"errors: No known data errors\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: resilvered 781M in 0h0m with 0 errors on Fri May 30 08:15:58 2014\n"
"config:\n"
msgstr ""
"errors: No known data errors\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: resilvered 781M in 0h0m with 0 errors on Fri May 30 08:15:58 2014\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:657
#: documentation/content/en/books/handbook/zfs/_index.adoc:690
#: documentation/content/en/books/handbook/zfs/_index.adoc:748
#: documentation/content/en/books/handbook/zfs/_index.adoc:815
#, no-wrap
msgid ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          mirror-0  ONLINE       0     0     0\n"
"            ada0p3  ONLINE       0     0     0\n"
"            ada1p3  ONLINE       0     0     0\n"
msgstr ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          mirror-0  ONLINE       0     0     0\n"
"            ada0p3  ONLINE       0     0     0\n"
"            ada1p3  ONLINE       0     0     0\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:667
msgid ""
"When adding disks to the existing vdev is not an option, as for RAID-Z, an "
"alternative method is to add another vdev to the pool.  Adding vdevs "
"provides higher performance by distributing writes across the vdevs.  Each "
"vdev provides its own redundancy.  Mixing vdev types like `mirror` and `RAID-"
"Z` is possible but discouraged.  Adding a non-redundant vdev to a pool "
"containing mirror or RAID-Z vdevs risks the data on the entire pool.  "
"Distributing writes means a failure of the non-redundant disk will result in "
"the loss of a fraction of every block written to the pool."
msgstr ""
"RAID-Z와 같이 기존 vdev에 디스크를 추가할 수 없는 경우, 다른 방법으로 풀에 다"
"른 vdev를 추가할 수 있습니다.  가상 디바이스를 추가하면 가상 디바이스 간에 쓰"
"기를 분산하여 더 높은 성능을 제공합니다.  각 vdev는 자체 중복성을 제공합니"
"다.  `mirror` 및 `RAID-Z` 와 같은 vdev 유형을 혼합하는 것은 가능하지만 권장하"
"지 않습니다.  미러 또는 RAID-Z vdev가 포함된 풀에 중복성이 없는 vdev를 추가하"
"면 전체 풀의 데이터가 위험해집니다.  쓰기를 분산하면 중복되지 않은 디스크에 "
"장애가 발생했을 때, 풀에 기록된 모든 블록의 일부분이 각각 손실될 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:672
msgid ""
"ZFS stripes data across each of the vdevs.  For example, with two mirror "
"vdevs, this is effectively a RAID 10 that stripes writes across two sets of "
"mirrors.  ZFS allocates space so that each vdev reaches 100% full at the "
"same time.  Having vdevs with different amounts of free space will lower "
"performance, as more data writes go to the less full vdev."
msgstr ""
"ZFS는 각 가상 디바이스에 걸쳐 데이터를 스트라이핑합니다.  예를 들어, 미러 가"
"상 디바이스가 2개인 경우, 이는 사실상 두 세트의 미러에 걸쳐 쓰기를 스트라이핑"
"하는 RAID 10입니다.  ZFS는 각 vdev가 동시에 100%가 되도록 공간을 할당합니"
"다.  여유 공간의 양이 다른 vdev를 사용하면 용량이 남는 vdev로 더 많은 데이터 "
"쓰기가 집중되므로 성능이 저하됩니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:674
msgid ""
"When attaching new devices to a boot pool, remember to update the bootcode."
msgstr "새 장치를 부트 풀에 연결할 때는 부트코드를 업데이트해야 합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:676
msgid ""
"Attach a second mirror group ([.filename]#ada2p3# and [.filename]#ada3p3#) "
"to the existing mirror:"
msgstr ""
"기존 미러에 두 번째 미러 그룹( [.filename]#ada2p3# 및 [.filename]#ada3p3# )"
"을 연결합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:684
#, no-wrap
msgid ""
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: resilvered 781M in 0h0m with 0 errors on Fri May 30 08:19:35 2014\n"
"config:\n"
msgstr ""
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: resilvered 781M in 0h0m with 0 errors on Fri May 30 08:19:35 2014\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:702
#, no-wrap
msgid ""
"errors: No known data errors\n"
"# zpool add mypool mirror ada2p3 ada3p3\n"
"# gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada2\n"
"bootcode written to ada2\n"
"# gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada3\n"
"bootcode written to ada3\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: scrub repaired 0 in 0h0m with 0 errors on Fri May 30 08:29:51 2014\n"
"config:\n"
msgstr ""
"errors: No known data errors\n"
"# zpool add mypool mirror ada2p3 ada3p3\n"
"# gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada2\n"
"bootcode written to ada2\n"
"# gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada3\n"
"bootcode written to ada3\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: scrub repaired 0 in 0h0m with 0 errors on Fri May 30 08:29:51 2014\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:711
#, no-wrap
msgid ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          mirror-0  ONLINE       0     0     0\n"
"            ada0p3  ONLINE       0     0     0\n"
"            ada1p3  ONLINE       0     0     0\n"
"          mirror-1  ONLINE       0     0     0\n"
"            ada2p3  ONLINE       0     0     0\n"
"            ada3p3  ONLINE       0     0     0\n"
msgstr ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          mirror-0  ONLINE       0     0     0\n"
"            ada0p3  ONLINE       0     0     0\n"
"            ada1p3  ONLINE       0     0     0\n"
"          mirror-1  ONLINE       0     0     0\n"
"            ada2p3  ONLINE       0     0     0\n"
"            ada3p3  ONLINE       0     0     0\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:717
msgid ""
"Removing vdevs from a pool is impossible and removal of disks from a mirror "
"is exclusive if there is enough remaining redundancy.  If a single disk "
"remains in a mirror group, that group ceases to be a mirror and becomes a "
"stripe, risking the entire pool if that remaining disk fails."
msgstr ""
"풀에서 가상 디바이스를 제거하는 것은 불가능하며, 미러에서 디스크를 제거하는 "
"것도 중복성이 충분히 남아 있는 경우에만 가능합니다.  미러 그룹에 단일 디스크"
"가 남아 있으면 해당 그룹은 미러가 아닌 스트라이프가 되어 나머지 디스크에 장애"
"가 발생했을 때 전체 풀이 위험에 처하게 됩니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:719
msgid "Remove a disk from a three-way mirror group:"
msgstr "three-way 미러 그룹에서 디스크를 제거합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:727
#, no-wrap
msgid ""
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: scrub repaired 0 in 0h0m with 0 errors on Fri May 30 08:29:51 2014\n"
"config:\n"
msgstr ""
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: scrub repaired 0 in 0h0m with 0 errors on Fri May 30 08:29:51 2014\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:734
#, no-wrap
msgid ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          mirror-0  ONLINE       0     0     0\n"
"            ada0p3  ONLINE       0     0     0\n"
"            ada1p3  ONLINE       0     0     0\n"
"            ada2p3  ONLINE       0     0     0\n"
msgstr ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          mirror-0  ONLINE       0     0     0\n"
"            ada0p3  ONLINE       0     0     0\n"
"            ada1p3  ONLINE       0     0     0\n"
"            ada2p3  ONLINE       0     0     0\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:742
#, no-wrap
msgid ""
"errors: No known data errors\n"
"# zpool detach mypool ada2p3\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: scrub repaired 0 in 0h0m with 0 errors on Fri May 30 08:29:51 2014\n"
"config:\n"
msgstr ""
"errors: No known data errors\n"
"# zpool detach mypool ada2p3\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: scrub repaired 0 in 0h0m with 0 errors on Fri May 30 08:29:51 2014\n"
"config:\n"

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:753
#, no-wrap
msgid "Checking the Status of a Pool"
msgstr "풀의 상태 확인하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:759
#, fuzzy
#| msgid ""
#| "Pool status is important.  If a drive goes offline or ZFS detects a read, "
#| "write, or checksum error, the corresponding error count increases.  The "
#| "`status` output shows the configuration and status of each device in the "
#| "pool and the status of the entire pool.  Actions to take and details "
#| "about the last <<zfs-zpool-scrub,`scrub`>> are also shown."
msgid ""
"Pool status is important.  If a drive goes offline or ZFS detects a read, "
"write, or checksum error, the corresponding error count increases.  The "
"`status` output shows the configuration and status of each device in the "
"pool and the status of the entire pool.  Actions to take and details about "
"the last crossref:zfs[zfs-zpool-scrub,`scrub`] are also shown."
msgstr ""
"풀 상태는 중요합니다.  드라이브가 오프라인 상태가 되거나 ZFS가 읽기, 쓰기 또"
"는 체크섬 오류를 감지하면 상응하는 오류 카운트가 증가합니다.  `status` 출력에"
"는 풀에 있는 각 장치의 구성 및 상태와 전체 풀의 상태가 표시됩니다.  수행해야 "
"할 작업과 마지막 <<zfs-zpool-scrub,`scrub`>> 에 대한 세부 정보도 표시됩니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:767
#, no-wrap
msgid ""
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: scrub repaired 0 in 2h25m with 0 errors on Sat Sep 14 04:25:50 2013\n"
"config:\n"
msgstr ""
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: scrub repaired 0 in 2h25m with 0 errors on Sat Sep 14 04:25:50 2013\n"
"config:\n"

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:782
#, no-wrap
msgid "Clearing Errors"
msgstr "오류 제거하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:788
msgid ""
"When detecting an error, ZFS increases the read, write, or checksum error "
"counts.  Clear the error message and reset the counts with `zpool clear "
"_mypool_`.  Clearing the error state can be important for automated scripts "
"that alert the administrator when the pool encounters an error.  Without "
"clearing old errors, the scripts may fail to report further errors."
msgstr ""
"오류를 감지하면 ZFS는 읽기, 쓰기 또는 체크섬 오류 카운트를 증가시킵니다.  "
"`zpool clear _mypool_` 로 오류 메시지를 지우고 횟수를 재설정합니다.  오류 상"
"태를 지우는 것은 풀에 오류가 발생했을 때 관리자에게 경고하는 자동화된 스크립"
"트에서 주의해야 할 문제입니다.  하지만 이전 오류를 지우지 않으면 스크립트에"
"서 추가 오류를 보고하지 못할 수 있습니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:790
#, no-wrap
msgid "Replacing a Functioning Device"
msgstr "작동 중인 장치 교체하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:799
#, fuzzy
#| msgid ""
#| "It may be desirable to replace one disk with a different disk.  When "
#| "replacing a working disk, the process keeps the old disk online during "
#| "the replacement.  The pool never enters a <<zfs-term-degraded,degraded>> "
#| "state, reducing the risk of data loss.  Running `zpool replace` copies "
#| "the data from the old disk to the new one.  After the operation "
#| "completes, ZFS disconnects the old disk from the vdev.  If the new disk "
#| "is larger than the old disk, it may be possible to grow the zpool, using "
#| "the new space.  See <<zfs-zpool-online,Growing a Pool>>."
msgid ""
"It may be desirable to replace one disk with a different disk.  When "
"replacing a working disk, the process keeps the old disk online during the "
"replacement.  The pool never enters a crossref:zfs[zfs-term-degraded,"
"degraded] state, reducing the risk of data loss.  Running `zpool replace` "
"copies the data from the old disk to the new one.  After the operation "
"completes, ZFS disconnects the old disk from the vdev.  If the new disk is "
"larger than the old disk, it may be possible to grow the zpool, using the "
"new space.  See crossref:zfs[zfs-zpool-online,Growing a Pool]."
msgstr ""
"하나의 디스크를 다른 디스크로 교체하는 것이 바람직할 수 있습니다.  작동 중인 "
"디스크를 교체할 때 프로세스는 교체하는 동안 이전 디스크를 온라인 상태로 유지"
"합니다.  풀은 <<zfs-term-degraded,degraded>> 상태가 되지 않으므로 데이터 손실"
"의 위험이 줄어듭니다.  `zpool replace` 를 실행하면 이전 디스크의 데이터가 새 "
"디스크에 복사됩니다.  작업이 완료되면 ZFS는 이전 디스크를 vdev에서 분리합니"
"다.  새 디스크가 이전 디스크보다 큰 경우 새 공간을 사용하여 zpool을 늘릴 수 "
"있습니다.  <<zfs-zpool-online,Growing a Pool>> 를 참고합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:801
msgid "Replace a functioning device in the pool:"
msgstr "풀에서 작동하는 장치를 교체합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:819
#, no-wrap
msgid ""
"errors: No known data errors\n"
"# zpool replace mypool ada1p3 ada2p3\n"
"Make sure to wait until resilvering finishes before rebooting.\n"
msgstr ""
"errors: No known data errors\n"
"# zpool replace mypool ada1p3 ada2p3\n"
"Make sure to wait until resilvering finishes before rebooting.\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:821
#, no-wrap
msgid "When booting from the pool 'zroot', update the boot code on the newly attached disk 'ada2p3'.\n"
msgstr "풀 'zroot' 에서 부팅할 때 새로 연결한 디스크 'ada2p3' 에서 부팅 코드를 업데이트합니다.\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:823
#, no-wrap
msgid "Assuming GPT partitioning is used and [.filename]#da0# is the new boot disk, use the following command:\n"
msgstr "GPT 파티셔닝이 사용되고 [.filename]#da0# 이 새 부팅 디스크라고 가정하면 다음 명령을 사용합니다:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:836
#, no-wrap
msgid ""
"        gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 da0\n"
"# gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada2\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"status: One or more devices is currently being resilvered.  The pool will\n"
"        continue to function, possibly in a degraded state.\n"
"action: Wait for the resilver to complete.\n"
"  scan: resilver in progress since Mon Jun  2 14:21:35 2014\n"
"        604M scanned out of 781M at 46.5M/s, 0h0m to go\n"
"        604M resilvered, 77.39% done\n"
"config:\n"
msgstr ""
"        gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 da0\n"
"# gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada2\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"status: One or more devices is currently being resilvered.  The pool will\n"
"        continue to function, possibly in a degraded state.\n"
"action: Wait for the resilver to complete.\n"
"  scan: resilver in progress since Mon Jun  2 14:21:35 2014\n"
"        604M scanned out of 781M at 46.5M/s, 0h0m to go\n"
"        604M resilvered, 77.39% done\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:844
#, no-wrap
msgid ""
"        NAME             STATE     READ WRITE CKSUM\n"
"        mypool           ONLINE       0     0     0\n"
"          mirror-0       ONLINE       0     0     0\n"
"            ada0p3       ONLINE       0     0     0\n"
"            replacing-1  ONLINE       0     0     0\n"
"              ada1p3     ONLINE       0     0     0\n"
"              ada2p3     ONLINE       0     0     0  (resilvering)\n"
msgstr ""
"        NAME             STATE     READ WRITE CKSUM\n"
"        mypool           ONLINE       0     0     0\n"
"          mirror-0       ONLINE       0     0     0\n"
"            ada0p3       ONLINE       0     0     0\n"
"            replacing-1  ONLINE       0     0     0\n"
"              ada1p3     ONLINE       0     0     0\n"
"              ada2p3     ONLINE       0     0     0  (resilvering)\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:851
#, no-wrap
msgid ""
"errors: No known data errors\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: resilvered 781M in 0h0m with 0 errors on Mon Jun  2 14:21:52 2014\n"
"config:\n"
msgstr ""
"errors: No known data errors\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: resilvered 781M in 0h0m with 0 errors on Mon Jun  2 14:21:52 2014\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:857
#: documentation/content/en/books/handbook/zfs/_index.adoc:933
#, no-wrap
msgid ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          mirror-0  ONLINE       0     0     0\n"
"            ada0p3  ONLINE       0     0     0\n"
"            ada2p3  ONLINE       0     0     0\n"
msgstr ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          mirror-0  ONLINE       0     0     0\n"
"            ada0p3  ONLINE       0     0     0\n"
"            ada2p3  ONLINE       0     0     0\n"

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:862
#, no-wrap
msgid "Dealing with Failed Devices"
msgstr "고장난 장치 처리하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:871
#, fuzzy
#| msgid ""
#| "When a disk in a pool fails, the vdev to which the disk belongs enters "
#| "the <<zfs-term-degraded,degraded>> state.  The data is still available, "
#| "but with reduced performance because ZFS computes missing data from the "
#| "available redundancy.  To restore the vdev to a fully functional state, "
#| "replace the failed physical device.  ZFS is then instructed to begin the "
#| "<<zfs-term-resilver,resilver>> operation.  ZFS recomputes data on the "
#| "failed device from available redundancy and writes it to the replacement "
#| "device.  After completion, the vdev returns to <<zfs-term-online,online>> "
#| "status."
msgid ""
"When a disk in a pool fails, the vdev to which the disk belongs enters the "
"crossref:zfs[zfs-term-degraded,degraded] state.  The data is still "
"available, but with reduced performance because ZFS computes missing data "
"from the available redundancy.  To restore the vdev to a fully functional "
"state, replace the failed physical device.  ZFS is then instructed to begin "
"the crossref:zfs[zfs-term-resilver,resilver] operation.  ZFS recomputes data "
"on the failed device from available redundancy and writes it to the "
"replacement device.  After completion, the vdev returns to crossref:zfs[zfs-"
"term-online,online] status."
msgstr ""
"풀의 디스크에 장애가 발생하면 해당 디스크가 속한 vdev는 <<zfs-term-degraded,"
"degraded>> 상태가 됩니다.  데이터는 계속 사용할 수 있지만 ZFS가 사용 가능한 "
"중복성에서 누락된 데이터를 계산하기 때문에 성능이 저하됩니다.  vdev를 완전한 "
"상태로 복원하려면 장애가 발생한 물리적 장치를 교체합니다.  그러면 ZFS가 "
"<<zfs-term-resilver,resilver>> 작업을 시작하라는 지시를 받습니다.  ZFS는 사"
"용 가능한 중복성에서 장애가 발생한 디바이스의 데이터를 다시 계산하여 교체 디"
"바이스에 씁니다.  완료 후 vdev는 <<zfs-term-online,online>> 상태로 돌아갑니"
"다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:876
#, fuzzy
#| msgid ""
#| "If the vdev does not have any redundancy, or if devices have failed and "
#| "there is not enough redundancy to compensate, the pool enters the <<zfs-"
#| "term-faulted,faulted>> state.  Unless enough devices can reconnect the "
#| "pool becomes inoperative requiring a data restore from backups."
msgid ""
"If the vdev does not have any redundancy, or if devices have failed and "
"there is not enough redundancy to compensate, the pool enters the crossref:"
"zfs[zfs-term-faulted,faulted] state.  Unless enough devices can reconnect "
"the pool becomes inoperative requiring a data restore from backups."
msgstr ""
"vdev에 중복성이 없거나 디바이스에 장애가 발생하고 이를 보완할 충분한 중복성"
"이 없는 경우 풀은 <<zfs-term-faulted,faulted>> 상태로 전환됩니다.  충분한 장"
"치를 다시 연결할 수 없으면 풀이 작동하지 않게 되어 백업에서 데이터를 복원해"
"야 합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:879
msgid ""
"When replacing a failed disk, the name of the failed disk changes to the "
"GUID of the new disk.  A new device name parameter for `zpool replace` is "
"not required if the replacement device has the same device name."
msgstr ""
"장애가 발생한 디스크를 교체할 때 장애가 발생한 디스크의 이름이 새 디스크의 "
"GUID로 변경됩니다.  교체 장치의 장치 이름이 동일한 경우 `zpool replace` 에 대"
"한 새 장치 이름 매개 변수가 필요하지 않습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:881
msgid "Replace a failed disk using `zpool replace`:"
msgstr "`zpool replace` 를 사용하여 고장난 디스크를 교체합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:893
#, no-wrap
msgid ""
"# zpool status\n"
"  pool: mypool\n"
" state: DEGRADED\n"
"status: One or more devices could not be opened.  Sufficient replicas exist for\n"
"        the pool to continue functioning in a degraded state.\n"
"action: Attach the missing device and online it using 'zpool online'.\n"
"   see: http://illumos.org/msg/ZFS-8000-2Q\n"
"  scan: none requested\n"
"config:\n"
msgstr ""
"# zpool status\n"
"  pool: mypool\n"
" state: DEGRADED\n"
"status: One or more devices could not be opened.  Sufficient replicas exist for\n"
"        the pool to continue functioning in a degraded state.\n"
"action: Attach the missing device and online it using 'zpool online'.\n"
"   see: http://illumos.org/msg/ZFS-8000-2Q\n"
"  scan: none requested\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:899
#, no-wrap
msgid ""
"        NAME                    STATE     READ WRITE CKSUM\n"
"        mypool                  DEGRADED     0     0     0\n"
"          mirror-0              DEGRADED     0     0     0\n"
"            ada0p3              ONLINE       0     0     0\n"
"            316502962686821739  UNAVAIL      0     0     0  was /dev/ada1p3\n"
msgstr ""
"        NAME                    STATE     READ WRITE CKSUM\n"
"        mypool                  DEGRADED     0     0     0\n"
"          mirror-0              DEGRADED     0     0     0\n"
"            ada0p3              ONLINE       0     0     0\n"
"            316502962686821739  UNAVAIL      0     0     0  was /dev/ada1p3\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:912
#, no-wrap
msgid ""
"errors: No known data errors\n"
"# zpool replace mypool 316502962686821739 ada2p3\n"
"# zpool status\n"
"  pool: mypool\n"
" state: DEGRADED\n"
"status: One or more devices is currently being resilvered.  The pool will\n"
"        continue to function, possibly in a degraded state.\n"
"action: Wait for the resilver to complete.\n"
"  scan: resilver in progress since Mon Jun  2 14:52:21 2014\n"
"        641M scanned out of 781M at 49.3M/s, 0h0m to go\n"
"        640M resilvered, 82.04% done\n"
"config:\n"
msgstr ""
"errors: No known data errors\n"
"# zpool replace mypool 316502962686821739 ada2p3\n"
"# zpool status\n"
"  pool: mypool\n"
" state: DEGRADED\n"
"status: One or more devices is currently being resilvered.  The pool will\n"
"        continue to function, possibly in a degraded state.\n"
"action: Wait for the resilver to complete.\n"
"  scan: resilver in progress since Mon Jun  2 14:52:21 2014\n"
"        641M scanned out of 781M at 49.3M/s, 0h0m to go\n"
"        640M resilvered, 82.04% done\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:920
#, no-wrap
msgid ""
"        NAME                        STATE     READ WRITE CKSUM\n"
"        mypool                      DEGRADED     0     0     0\n"
"          mirror-0                  DEGRADED     0     0     0\n"
"            ada0p3                  ONLINE       0     0     0\n"
"            replacing-1             UNAVAIL      0     0     0\n"
"              15732067398082357289  UNAVAIL      0     0     0  was /dev/ada1p3/old\n"
"              ada2p3                ONLINE       0     0     0  (resilvering)\n"
msgstr ""
"        NAME                        STATE     READ WRITE CKSUM\n"
"        mypool                      DEGRADED     0     0     0\n"
"          mirror-0                  DEGRADED     0     0     0\n"
"            ada0p3                  ONLINE       0     0     0\n"
"            replacing-1             UNAVAIL      0     0     0\n"
"              15732067398082357289  UNAVAIL      0     0     0  was /dev/ada1p3/old\n"
"              ada2p3                ONLINE       0     0     0  (resilvering)\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:927
#, no-wrap
msgid ""
"errors: No known data errors\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: resilvered 781M in 0h0m with 0 errors on Mon Jun  2 14:52:38 2014\n"
"config:\n"
msgstr ""
"errors: No known data errors\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: resilvered 781M in 0h0m with 0 errors on Mon Jun  2 14:52:38 2014\n"
"config:\n"

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:938
#, no-wrap
msgid "Scrubbing a Pool"
msgstr "풀 스크러빙하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:944
#, fuzzy
#| msgid ""
#| "Routinely <<zfs-term-scrub,scrub>> pools, ideally at least once every "
#| "month.  The `scrub` operation is disk-intensive and will reduce "
#| "performance while running.  Avoid high-demand periods when scheduling "
#| "`scrub` or use <<zfs-advanced-tuning-scrub_delay,`vfs.zfs.scrub_delay`>> "
#| "to adjust the relative priority of the `scrub` to keep it from slowing "
#| "down other workloads."
msgid ""
"Routinely crossref:zfs[zfs-term-scrub,scrub] pools, ideally at least once "
"every month.  The `scrub` operation is disk-intensive and will reduce "
"performance while running.  Avoid high-demand periods when scheduling "
"`scrub` or use crossref:zfs[zfs-advanced-tuning-scrub_delay,`vfs.zfs."
"scrub_delay`] to adjust the relative priority of the `scrub` to keep it from "
"slowing down other workloads."
msgstr ""
"정기적으로 풀을, 이상적으로는 매월 한 번 이상 <<zfs-term-scrub,scrub>> 합니"
"다.  `scrub` 작업은 디스크 집약적이며 실행 중 성능이 저하됩니다.  `scrub` 을 "
"예약할 때 수요가 많은 기간을 피하거나, 다른 워크로드의 속도를 늦추지 않도록 "
"<<zfs-advanced-tuning-scrub_delay,`vfs.zfs.scrub_delay`>> 를 사용하여 "
"`scrub` 의 상대적 우선순위를 조정합니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:955
#, no-wrap
msgid ""
"# zpool scrub mypool\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: scrub in progress since Wed Feb 19 20:52:54 2014\n"
"        116G scanned out of 8.60T at 649M/s, 3h48m to go\n"
"        0 repaired, 1.32% done\n"
"config:\n"
msgstr ""
"# zpool scrub mypool\n"
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"  scan: scrub in progress since Wed Feb 19 20:52:54 2014\n"
"        116G scanned out of 8.60T at 649M/s, 3h48m to go\n"
"        0 repaired, 1.32% done\n"
"config:\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:970
msgid "To cancel a scrub operation if needed, run `zpool scrub -s _mypool_`."
msgstr ""
"필요한 경우 스크럽 작업을 취소하려면 `zpool scrub -s _mypool_` 을 실행합니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:972
#, no-wrap
msgid "Self-Healing"
msgstr "Self-Healing"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:983
msgid ""
"The checksums stored with data blocks enable the file system to _self-"
"heal_.  This feature will automatically repair data whose checksum does not "
"match the one recorded on another device that is part of the storage pool.  "
"For example, a mirror configuration with two disks where one drive is "
"starting to malfunction and cannot properly store the data any more.  This "
"is worse when the data was not accessed for a long time, as with long term "
"archive storage.  Traditional file systems need to run commands that check "
"and repair the data like man:fsck[8].  These commands take time, and in "
"severe cases, an administrator has to decide which repair operation to "
"perform.  When ZFS detects a data block with a mismatched checksum, it tries "
"to read the data from the mirror disk.  If that disk can provide the correct "
"data, ZFS will give that to the application and correct the data on the disk "
"with the wrong checksum.  This happens without any interaction from a system "
"administrator during normal pool operation."
msgstr ""
"데이터 블록과 함께 저장된 체크섬은 파일 시스템이 _자가 치유_ 될 수 있도록 합"
"니다.  이 기능은 체크섬이 스토리지 풀의 일부인 다른 장치에 기록된 체크섬과 일"
"치하지 않는 데이터를 자동으로 복구합니다.  예를 들어, 두 개의 디스크로 구성"
"된 미러 구성에서 하나의 드라이브가 오작동하기 시작하여 더 이상 데이터를 제대"
"로 저장할 수 없는 경우를 들 수 있습니다.  장기 아카이브 스토리지와 같이 오랫"
"동안 데이터에 액세스하지 않은 경우에는 이러한 문제가 더욱 심각합니다.  기존 "
"파일 시스템에서는 man:fsck[8]와 같이 데이터를 검사하고 복구하는 명령을 실행해"
"야 합니다.  이러한 명령은 시간이 걸리며, 심한 경우 관리자가 어떤 복구 작업을 "
"수행할지 결정해야 합니다.  체크섬이 일치하지 않는 데이터 블록을 감지하면 ZFS"
"는 미러 디스크에서 데이터를 읽으려고 시도합니다.  해당 디스크가 올바른 데이터"
"를 제공할 수 있는 경우, ZFS는 해당 데이터를 애플리케이션에 제공하고 잘못된 체"
"크섬이 있는 디스크의 데이터를 수정합니다.  이는 정상적인 풀 작동 중에 시스템 "
"관리자의 개입 없이 발생합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:985
msgid ""
"The next example shows this self-healing behavior by creating a mirrored "
"pool of disks [.filename]#/dev/ada0# and [.filename]#/dev/ada1#."
msgstr ""
"다음 예제에서는 [.filename]#/dev/ada0# 및 [.filename]#/dev/ada1# 디스크의 미"
"러링된 풀을 생성하여 이러한 자가 복구 동작을 보여 줍니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:994
#, no-wrap
msgid ""
"# zpool create healer mirror /dev/ada0 /dev/ada1\n"
"# zpool status healer\n"
"  pool: healer\n"
" state: ONLINE\n"
"  scan: none requested\n"
"config:\n"
msgstr ""
"# zpool create healer mirror /dev/ada0 /dev/ada1\n"
"# zpool status healer\n"
"  pool: healer\n"
" state: ONLINE\n"
"  scan: none requested\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1000
#: documentation/content/en/books/handbook/zfs/_index.adoc:1155
#, no-wrap
msgid ""
"    NAME        STATE     READ WRITE CKSUM\n"
"    healer      ONLINE       0     0     0\n"
"      mirror-0  ONLINE       0     0     0\n"
"       ada0     ONLINE       0     0     0\n"
"       ada1     ONLINE       0     0     0\n"
msgstr ""
"    NAME        STATE     READ WRITE CKSUM\n"
"    healer      ONLINE       0     0     0\n"
"      mirror-0  ONLINE       0     0     0\n"
"       ada0     ONLINE       0     0     0\n"
"       ada1     ONLINE       0     0     0\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1005
#, no-wrap
msgid ""
"errors: No known data errors\n"
"# zpool list\n"
"NAME     SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG   CAP  DEDUP  HEALTH  ALTROOT\n"
"healer   960M  92.5K   960M         -         -     0%    0%  1.00x  ONLINE  -\n"
msgstr ""
"errors: No known data errors\n"
"# zpool list\n"
"NAME     SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG   CAP  DEDUP  HEALTH  ALTROOT\n"
"healer   960M  92.5K   960M         -         -     0%    0%  1.00x  ONLINE  -\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1008
msgid ""
"Copy some important data to the pool to protect from data errors using the "
"self-healing feature and create a checksum of the pool for later comparison."
msgstr ""
"자가 복구 기능을 사용하여 데이터 오류로부터 보호하기 위해 일부 중요한 데이터"
"를 풀에 복사하고 나중에 비교할 수 있도록 풀의 체크섬을 생성합니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1018
#, no-wrap
msgid ""
"# cp /some/important/data /healer\n"
"# zfs list\n"
"NAME     SIZE  ALLOC   FREE    CAP  DEDUP  HEALTH  ALTROOT\n"
"healer   960M  67.7M   892M     7%  1.00x  ONLINE  -\n"
"# sha1 /healer > checksum.txt\n"
"# cat checksum.txt\n"
"SHA1 (/healer) = 2753eff56d77d9a536ece6694bf0a82740344d1f\n"
msgstr ""
"# cp /some/important/data /healer\n"
"# zfs list\n"
"NAME     SIZE  ALLOC   FREE    CAP  DEDUP  HEALTH  ALTROOT\n"
"healer   960M  67.7M   892M     7%  1.00x  ONLINE  -\n"
"# sha1 /healer > checksum.txt\n"
"# cat checksum.txt\n"
"SHA1 (/healer) = 2753eff56d77d9a536ece6694bf0a82740344d1f\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1022
msgid ""
"Simulate data corruption by writing random data to the beginning of one of "
"the disks in the mirror.  To keep ZFS from healing the data when detected, "
"export the pool before the corruption and import it again afterwards."
msgstr ""
"미러에 있는 디스크 중 하나의 시작 부분에 임의의 데이터를 써서 데이터 손상을 "
"시뮬레이션합니다.  데이터가 손상되었을 때 ZFS가 데이터를 복구하지 못하도록 하"
"려면 손상되기 전에 풀을 내보내고 나중에 다시 가져오세요."

#. type: delimited block = 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1030
msgid ""
"This is a dangerous operation that can destroy vital data, shown here for "
"demonstration alone.  *Do not try* it during normal operation of a storage "
"pool.  Nor should this intentional corruption example run on any disk with a "
"file system not using ZFS on another partition in it.  Do not use any other "
"disk device names other than the ones that are part of the pool.  Ensure "
"proper backups of the pool exist and test them before running the command!"
msgstr ""
"이 작업은 중요한 데이터를 파괴할 수 있는 위험한 작업이며, 여기서는 데모로만 "
"설명합니다.  스토리지 풀이 정상적으로 작동하는 동안에는 *시도하지 마세요*.  "
"또한 이 의도적인 손상 예제는 다른 파티션에 ZFS를 사용하지 않는 파일 시스템이 "
"있는 디스크에서 실행해서는 안 됩니다.  풀의 일부인 디스크 장치 이름 외에 다"
"른 디스크 장치 이름을 사용하지 마세요.  명령을 실행하기 전에 풀의 적절한 백업"
"이 있는지 확인하고 테스트하세요!"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1040
#, no-wrap
msgid ""
"# zpool export healer\n"
"# dd if=/dev/random of=/dev/ada1 bs=1m count=200\n"
"200+0 records in\n"
"200+0 records out\n"
"209715200 bytes transferred in 62.992162 secs (3329227 bytes/sec)\n"
"# zpool import healer\n"
msgstr ""
"# zpool export healer\n"
"# dd if=/dev/random of=/dev/ada1 bs=1m count=200\n"
"200+0 records in\n"
"200+0 records out\n"
"209715200 bytes transferred in 62.992162 secs (3329227 bytes/sec)\n"
"# zpool import healer\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1046
msgid ""
"The pool status shows that one device has experienced an error.  Note that "
"applications reading data from the pool did not receive any incorrect data.  "
"ZFS provided data from the [.filename]#ada0# device with the correct "
"checksums.  To find the device with the wrong checksum, look for one whose "
"`CKSUM` column contains a nonzero value."
msgstr ""
"풀 상태는 한 장치에서 오류가 발생했음을 보여줍니다.  풀에서 데이터를 읽는 애"
"플리케이션은 잘못된 데이터를 수신하지 않았습니다.  ZFS는 올바른 체크섬을 가"
"진 [.filename]#ada0# 장치에서 데이터를 제공했습니다.  잘못된 체크섬을 가진 장"
"치를 찾으려면 `CKSUM` 열에 0이 아닌 값이 포함된 장치를 찾습니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1059
#, no-wrap
msgid ""
"# zpool status healer\n"
"    pool: healer\n"
"   state: ONLINE\n"
"  status: One or more devices has experienced an unrecoverable error.  An\n"
"          attempt was made to correct the error.  Applications are unaffected.\n"
"  action: Determine if the device needs to be replaced, and clear the errors\n"
"          using 'zpool clear' or replace the device with 'zpool replace'.\n"
"     see: http://illumos.org/msg/ZFS-8000-4J\n"
"    scan: none requested\n"
"  config:\n"
msgstr ""
"# zpool status healer\n"
"    pool: healer\n"
"   state: ONLINE\n"
"  status: One or more devices has experienced an unrecoverable error.  An\n"
"          attempt was made to correct the error.  Applications are unaffected.\n"
"  action: Determine if the device needs to be replaced, and clear the errors\n"
"          using 'zpool clear' or replace the device with 'zpool replace'.\n"
"     see: http://illumos.org/msg/ZFS-8000-4J\n"
"    scan: none requested\n"
"  config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1065
#, no-wrap
msgid ""
"      NAME        STATE     READ WRITE CKSUM\n"
"      healer      ONLINE       0     0     0\n"
"        mirror-0  ONLINE       0     0     0\n"
"         ada0     ONLINE       0     0     0\n"
"         ada1     ONLINE       0     0     1\n"
msgstr ""
"      NAME        STATE     READ WRITE CKSUM\n"
"      healer      ONLINE       0     0     0\n"
"        mirror-0  ONLINE       0     0     0\n"
"         ada0     ONLINE       0     0     0\n"
"         ada1     ONLINE       0     0     1\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1071
msgid ""
"ZFS detected the error and handled it by using the redundancy present in the "
"unaffected [.filename]#ada0# mirror disk.  A checksum comparison with the "
"original one will reveal whether the pool is consistent again."
msgstr ""
"ZFS가 오류를 감지하고 영향을 받지 않는 [.filename]#ada0# 미러 디스크에 있는 "
"중복성을 사용하여 오류를 처리했습니다.  원본과 체크섬을 비교하면 풀이 다시 일"
"관성이 있는지 확인할 수 있습니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1078
#, no-wrap
msgid ""
"# sha1 /healer >> checksum.txt\n"
"# cat checksum.txt\n"
"SHA1 (/healer) = 2753eff56d77d9a536ece6694bf0a82740344d1f\n"
"SHA1 (/healer) = 2753eff56d77d9a536ece6694bf0a82740344d1f\n"
msgstr ""
"# sha1 /healer >> checksum.txt\n"
"# cat checksum.txt\n"
"SHA1 (/healer) = 2753eff56d77d9a536ece6694bf0a82740344d1f\n"
"SHA1 (/healer) = 2753eff56d77d9a536ece6694bf0a82740344d1f\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1087
msgid ""
"Generate checksums before and after the intentional tampering while the pool "
"data still matches.  This shows how ZFS is capable of detecting and "
"correcting any errors automatically when the checksums differ.  Note this is "
"possible with enough redundancy present in the pool.  A pool consisting of a "
"single device has no self-healing capabilities.  That is also the reason why "
"checksums are so important in ZFS; do not disable them for any reason.  ZFS "
"requires no man:fsck[8] or similar file system consistency check program to "
"detect and correct this, and keeps the pool available while there is a "
"problem.  A scrub operation is now required to overwrite the corrupted data "
"on [.filename]#ada1#."
msgstr ""
"풀 데이터가 여전히 일치하는 동안 의도적인 변조 전과 후의 체크섬을 생성합니"
"다.  이는 체크섬이 다를 때 ZFS가 어떻게 자동으로 오류를 감지하고 수정할 수 있"
"는지 보여줍니다.  이는 풀에 충분한 중복성이 있을 때 가능하다는 점에 유의하세"
"요.  단일 장치로 구성된 풀은 자체 복구 기능이 없습니다.  이것이 바로 ZFS에서 "
"체크섬이 중요한 이유이기도 하므로 어떤 이유로든 체크섬을 비활성화하지 마세"
"요.  ZFS는 이를 감지하고 수정하기 위해 man:fsck[8] 또는 이와 유사한 파일 시스"
"템 일관성 검사 프로그램이 필요하지 않으며, 문제가 있는 동안에도 풀을 계속 사"
"용할 수 있도록 유지합니다.  이제 [.filename]#ada1# 의 손상된 데이터를 덮어쓰"
"려면 스크럽 작업이 필요합니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1103
#, no-wrap
msgid ""
"# zpool scrub healer\n"
"# zpool status healer\n"
"  pool: healer\n"
" state: ONLINE\n"
"status: One or more devices has experienced an unrecoverable error.  An\n"
"            attempt was made to correct the error.  Applications are unaffected.\n"
"action: Determine if the device needs to be replaced, and clear the errors\n"
"            using 'zpool clear' or replace the device with 'zpool replace'.\n"
"   see: http://illumos.org/msg/ZFS-8000-4J\n"
"  scan: scrub in progress since Mon Dec 10 12:23:30 2012\n"
"        10.4M scanned out of 67.0M at 267K/s, 0h3m to go\n"
"        9.63M repaired, 15.56% done\n"
"config:\n"
msgstr ""
"# zpool scrub healer\n"
"# zpool status healer\n"
"  pool: healer\n"
" state: ONLINE\n"
"status: One or more devices has experienced an unrecoverable error.  An\n"
"            attempt was made to correct the error.  Applications are unaffected.\n"
"action: Determine if the device needs to be replaced, and clear the errors\n"
"            using 'zpool clear' or replace the device with 'zpool replace'.\n"
"   see: http://illumos.org/msg/ZFS-8000-4J\n"
"  scan: scrub in progress since Mon Dec 10 12:23:30 2012\n"
"        10.4M scanned out of 67.0M at 267K/s, 0h3m to go\n"
"        9.63M repaired, 15.56% done\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1109
#, no-wrap
msgid ""
"    NAME        STATE     READ WRITE CKSUM\n"
"    healer      ONLINE       0     0     0\n"
"      mirror-0  ONLINE       0     0     0\n"
"       ada0     ONLINE       0     0     0\n"
"       ada1     ONLINE       0     0   627  (repairing)\n"
msgstr ""
"    NAME        STATE     READ WRITE CKSUM\n"
"    healer      ONLINE       0     0     0\n"
"      mirror-0  ONLINE       0     0     0\n"
"       ada0     ONLINE       0     0     0\n"
"       ada1     ONLINE       0     0   627  (repairing)\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1115
msgid ""
"The scrub operation reads data from [.filename]#ada0# and rewrites any data "
"with a wrong checksum on [.filename]#ada1#, shown by the `(repairing)` "
"output from `zpool status`.  After the operation is complete, the pool "
"status changes to:"
msgstr ""
"스크럽 작업은 [.filename]#ada0# 에서 데이터를 읽고 `zpool status` 의 "
"'(repairing)` 출력에 표시된 것처럼 [.filename]#ada1# 에 잘못된 체크섬이 있는 "
"데이터를 다시 씁니다.  작업이 완료되면 풀 상태가 다음과 같이 변경됩니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1128
#, no-wrap
msgid ""
"# zpool status healer\n"
"  pool: healer\n"
" state: ONLINE\n"
"status: One or more devices has experienced an unrecoverable error.  An\n"
"        attempt was made to correct the error.  Applications are unaffected.\n"
"action: Determine if the device needs to be replaced, and clear the errors\n"
"             using 'zpool clear' or replace the device with 'zpool replace'.\n"
"   see: http://illumos.org/msg/ZFS-8000-4J\n"
"  scan: scrub repaired 66.5M in 0h2m with 0 errors on Mon Dec 10 12:26:25 2012\n"
"config:\n"
msgstr ""
"# zpool status healer\n"
"  pool: healer\n"
" state: ONLINE\n"
"status: One or more devices has experienced an unrecoverable error.  An\n"
"        attempt was made to correct the error.  Applications are unaffected.\n"
"action: Determine if the device needs to be replaced, and clear the errors\n"
"             using 'zpool clear' or replace the device with 'zpool replace'.\n"
"   see: http://illumos.org/msg/ZFS-8000-4J\n"
"  scan: scrub repaired 66.5M in 0h2m with 0 errors on Mon Dec 10 12:26:25 2012\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1134
#, no-wrap
msgid ""
"    NAME        STATE     READ WRITE CKSUM\n"
"    healer      ONLINE       0     0     0\n"
"      mirror-0  ONLINE       0     0     0\n"
"       ada0     ONLINE       0     0     0\n"
"       ada1     ONLINE       0     0 2.72K\n"
msgstr ""
"    NAME        STATE     READ WRITE CKSUM\n"
"    healer      ONLINE       0     0     0\n"
"      mirror-0  ONLINE       0     0     0\n"
"       ada0     ONLINE       0     0     0\n"
"       ada1     ONLINE       0     0 2.72K\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1140
#, fuzzy
#| msgid ""
#| "After the scrubbing operation completes with all the data synchronized "
#| "from [.filename]#ada0# to [.filename]#ada1#, <<zfs-zpool-clear,clear>> "
#| "the error messages from the pool status by running `zpool clear`."
msgid ""
"After the scrubbing operation completes with all the data synchronized from "
"[.filename]#ada0# to [.filename]#ada1#, crossref:zfs[zfs-zpool-clear,clear] "
"the error messages from the pool status by running `zpool clear`."
msgstr ""
"스크러빙 작업이 완료되어 [.filename]#ada0# 에서 [.filename]#ada1# 로 모든 데"
"이터가 동기화되면, `zpool clear` 를 실행하여 풀 상태의 오류 메시지를 삭제합니"
"다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1149
#, no-wrap
msgid ""
"# zpool clear healer\n"
"# zpool status healer\n"
"  pool: healer\n"
" state: ONLINE\n"
"  scan: scrub repaired 66.5M in 0h2m with 0 errors on Mon Dec 10 12:26:25 2012\n"
"config:\n"
msgstr ""
"# zpool clear healer\n"
"# zpool status healer\n"
"  pool: healer\n"
" state: ONLINE\n"
"  scan: scrub repaired 66.5M in 0h2m with 0 errors on Mon Dec 10 12:26:25 2012\n"
"config:\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1160
msgid ""
"The pool is now back to a fully working state, with all error counts now "
"zero."
msgstr ""
"이제 풀이 완전히 작동하는 상태로 돌아왔으며 모든 오류 카운트가 0이 되었습니"
"다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:1162
#, no-wrap
msgid "Growing a Pool"
msgstr "풀 확장하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1172
#, fuzzy
#| msgid ""
#| "The smallest device in each vdev limits the usable size of a redundant "
#| "pool.  Replace the smallest device with a larger device.  After "
#| "completing a <<zfs-zpool-replace,replace>> or <<zfs-term-resilver,"
#| "resilver>> operation, the pool can grow to use the capacity of the new "
#| "device.  For example, consider a mirror of a 1 TB drive and a 2 TB "
#| "drive.  The usable space is 1 TB.  When replacing the 1 TB drive with "
#| "another 2 TB drive, the resilvering process copies the existing data onto "
#| "the new drive.  As both of the devices now have 2 TB capacity, the "
#| "mirror's available space grows to 2 TB."
msgid ""
"The smallest device in each vdev limits the usable size of a redundant "
"pool.  Replace the smallest device with a larger device.  After completing a "
"crossref:zfs[zfs-zpool-replace,replace] or crossref:zfs[zfs-term-resilver,"
"resilver] operation, the pool can grow to use the capacity of the new "
"device.  For example, consider a mirror of a 1 TB drive and a 2 TB drive.  "
"The usable space is 1 TB.  When replacing the 1 TB drive with another 2 TB "
"drive, the resilvering process copies the existing data onto the new drive.  "
"As both of the devices now have 2 TB capacity, the mirror's available space "
"grows to 2 TB."
msgstr ""
"각 가상 디바이스에서 가장 작은 디바이스는 중복 풀의 사용 가능한 크기를 제한합"
"니다.  가장 작은 장치를 더 큰 장치로 교체합니다.  <<zfs-zpool-replace,"
"replace>> 또는 <<zfs-term-resilver,resilver>> 작업을 완료하면 새 장치의 용량"
"을 사용하도록 풀을 확장할 수 있습니다.  예를 들어 1TB 드라이브와 2TB 드라이브"
"의 미러를 생각해 보겠습니다.  사용 가능한 공간은 1TB입니다.  1TB 드라이브를 "
"다른 2TB 드라이브로 교체할 때 복원 프로세스는 기존 데이터를 새 드라이브에 복"
"사합니다.  이제 두 장치의 용량이 모두 2TB이므로 미러의 사용 가능한 공간은 2TB"
"로 증가합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1175
msgid ""
"Start expansion by using `zpool online -e` on each device.  After expanding "
"all devices, the extra space becomes available to the pool."
msgstr ""
"각 장치에서 `zpool online -e` 를 사용하여 확장을 시작하세요.  모든 장치를 확"
"장하면 풀에서 여분의 공간을 사용할 수 있게 됩니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:1177
#, no-wrap
msgid "Importing and Exporting Pools"
msgstr "풀 가져오기 및 내보내기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1185
msgid ""
"_Export_ pools before moving them to another system.  ZFS unmounts all "
"datasets, marking each device as exported but still locked to prevent use by "
"other disks.  This allows pools to be _imported_ on other machines, other "
"operating systems that support ZFS, and even different hardware "
"architectures (with some caveats, see man:zpool[8]).  When a dataset has "
"open files, use `zpool export -f` to force exporting the pool.  Use this "
"with caution.  The datasets are forcibly unmounted, potentially resulting in "
"unexpected behavior by the applications which had open files on those "
"datasets."
msgstr ""
"풀을 다른 시스템으로 옮기기 전에 _Export_ pool을 하세요.  ZFS는 모든 데이터세"
"트를 마운트 해제하여 각 장치를 내보낸 것으로 표시하지만 다른 디스크에서 사용"
"하지 못하도록 여전히 잠궈 둡니다.  이렇게 하면 다른 시스템, ZFS를 지원하는 다"
"른 운영 체제, 심지어 다른 하드웨어 아키텍처에서도 풀을 _임포트_할 수 있습니다"
"(몇 가지 주의 사항, man:zpool[8] 참조).  데이터 세트에 열려 있는 파일이 있는 "
"경우, `zpool export -f` 를 사용하여 풀을 강제로 내보냅니다.  주의해서 사용하"
"세요.  데이터 세트가 강제로 마운트 해제되므로 해당 데이터 세트에 열린 파일이 "
"있는 응용 프로그램에서 예기치 않은 동작이 발생할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1187
msgid "Export a pool that is not in use:"
msgstr "사용하지 않는 풀을 내보냅니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1191
#, no-wrap
msgid "# zpool export mypool\n"
msgstr "# zpool export mypool\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1199
msgid ""
"Importing a pool automatically mounts the datasets.  If this is undesired "
"behavior, use `zpool import -N` to prevent it.  `zpool import -o` sets "
"temporary properties for this specific import.  `zpool import altroot=` "
"allows importing a pool with a base mount point instead of the root of the "
"file system.  If the pool was last used on a different system and was not "
"properly exported, force the import using `zpool import -f`.  `zpool import -"
"a` imports all pools that do not appear to be in use by another system."
msgstr ""
"풀을 가져오면 데이터 세트가 자동으로 마운트됩니다.  원치 않는 동작인 경우 "
"`zpool import -N` 을 사용하여 이를 방지합니다.  `zpool import -o` 는 이 특정 "
"가져오기에 대한 임시 속성을 설정합니다.  `zpool import altroot=` 를 사용하면 "
"파일 시스템의 루트 대신 기본 마운트 지점을 사용하여 풀을 가져올 수 있습니"
"다.  풀이 다른 시스템에서 마지막으로 사용되었고 제대로 내보내지지 않은 경우 "
"`zpool import -f` 를 사용하여 강제로 가져옵니다.  `zpool import -a` 는 다른 "
"시스템에서 사용 중인 것으로 보이지 않는 모든 풀을 가져옵니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1201
msgid "List all available pools for import:"
msgstr "가져올 수 있는 모든 풀을 나열합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1210
#, no-wrap
msgid ""
"# zpool import\n"
"   pool: mypool\n"
"     id: 9930174748043525076\n"
"  state: ONLINE\n"
" action: The pool can be imported using its name or numeric identifier.\n"
" config:\n"
msgstr ""
"# zpool import\n"
"   pool: mypool\n"
"     id: 9930174748043525076\n"
"  state: ONLINE\n"
" action: The pool can be imported using its name or numeric identifier.\n"
" config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1213
#, no-wrap
msgid ""
"        mypool      ONLINE\n"
"          ada2p3    ONLINE\n"
msgstr ""
"        mypool      ONLINE\n"
"          ada2p3    ONLINE\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1216
msgid "Import the pool with an alternative root directory:"
msgstr "대체 루트 디렉터리로 풀을 가져옵니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1224
#, no-wrap
msgid ""
"# zpool import -o altroot=/mnt mypool\n"
"# zfs list\n"
"zfs list\n"
"NAME                 USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool               110K  47.0G    31K  /mnt/mypool\n"
msgstr ""
"# zpool import -o altroot=/mnt mypool\n"
"# zfs list\n"
"zfs list\n"
"NAME                 USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool               110K  47.0G    31K  /mnt/mypool\n"

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:1227
#, no-wrap
msgid "Upgrading a Storage Pool"
msgstr "스토리지 풀 업그레이드하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1233
msgid ""
"After upgrading FreeBSD, or if importing a pool from a system using an older "
"version, manually upgrade the pool to the latest ZFS version to support "
"newer features.  Consider whether the pool may ever need importing on an "
"older system before upgrading.  Upgrading is a one-way process.  Upgrade "
"older pools is possible, but downgrading pools with newer features is not."
msgstr ""
"FreeBSD를 업그레이드한 후 또는 이전 버전을 사용하는 시스템에서 풀을 가져오는 "
"경우, 최신 기능을 지원하려면 풀을 최신 ZFS 버전으로 수동 업그레이드해야 합니"
"다.  업그레이드하기 전에 이전 시스템에서 풀을 가져와야 하는지 여부를 고려하세"
"요.  업그레이드는 단방향 프로세스입니다.  이전 풀의 업그레이드는 가능하지만 "
"최신 기능이 포함된 풀의 다운그레이드는 불가능합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1235
msgid "Upgrade a v28 pool to support `Feature Flags`:"
msgstr "`Feature Flags` 를 지원하도록 v28 풀을 업그레이드합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1248
#, no-wrap
msgid ""
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"status: The pool is formatted using a legacy on-disk format.  The pool can\n"
"        still be used, but some features are unavailable.\n"
"action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the\n"
"        pool will no longer be accessible on software that does not support feat\n"
"        flags.\n"
"  scan: none requested\n"
"config:\n"
msgstr ""
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"status: The pool is formatted using a legacy on-disk format.  The pool can\n"
"        still be used, but some features are unavailable.\n"
"action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the\n"
"        pool will no longer be accessible on software that does not support feat\n"
"        flags.\n"
"  scan: none requested\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1254
#: documentation/content/en/books/handbook/zfs/_index.adoc:1302
#, no-wrap
msgid ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          mirror-0  ONLINE       0     0     0\n"
"\t    ada0    ONLINE       0     0     0\n"
"\t    ada1    ONLINE       0     0     0\n"
msgstr ""
"        NAME        STATE     READ WRITE CKSUM\n"
"        mypool      ONLINE       0     0     0\n"
"          mirror-0  ONLINE       0     0     0\n"
"\t    ada0    ONLINE       0     0     0\n"
"\t    ada1    ONLINE       0     0     0\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1258
#: documentation/content/en/books/handbook/zfs/_index.adoc:1306
#, no-wrap
msgid ""
"errors: No known data errors\n"
"# zpool upgrade\n"
"This system supports ZFS pool feature flags.\n"
msgstr ""
"errors: No known data errors\n"
"# zpool upgrade\n"
"This system supports ZFS pool feature flags.\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1261
#, no-wrap
msgid ""
"The following pools are formatted with legacy version numbers and are upgraded to use feature flags.\n"
"After being upgraded, these pools will no longer be accessible by software that does not support feature flags.\n"
msgstr ""
"다음 풀은 레거시 버전 번호로 포맷되어 있으며 feature flags를 사용하도록 업그레이드되었습니다.\n"
"업그레이드된 후에는 feature flags를 지원하지 않는 소프트웨어에서는 이러한 풀에 더 이상 액세스할 수 없습니다.\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1265
#, no-wrap
msgid ""
"VER  POOL\n"
"---  ------------\n"
"28   mypool\n"
msgstr ""
"VER  POOL\n"
"---  ------------\n"
"28   mypool\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1270
#, no-wrap
msgid ""
"Use 'zpool upgrade -v' for a list of available legacy versions.\n"
"Every feature flags pool has all supported features enabled.\n"
"# zpool upgrade mypool\n"
"This system supports ZFS pool feature flags.\n"
msgstr ""
"Use 'zpool upgrade -v' for a list of available legacy versions.\n"
"Every feature flags pool has all supported features enabled.\n"
"# zpool upgrade mypool\n"
"This system supports ZFS pool feature flags.\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1277
#, no-wrap
msgid ""
"Successfully upgraded 'mypool' from version 28 to feature flags.\n"
"Enabled the following features on 'mypool':\n"
"  async_destroy\n"
"  empty_bpobj\n"
"  lz4_compress\n"
"  multi_vdev_crash_dump\n"
msgstr ""
"Successfully upgraded 'mypool' from version 28 to feature flags.\n"
"Enabled the following features on 'mypool':\n"
"  async_destroy\n"
"  empty_bpobj\n"
"  lz4_compress\n"
"  multi_vdev_crash_dump\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1281
msgid ""
"The newer features of ZFS will not be available until `zpool upgrade` has "
"completed.  Use `zpool upgrade -v` to see what new features the upgrade "
"provides, as well as which features are already supported."
msgstr ""
"ZFS의 최신 기능은 `zpool upgrade` 가 완료될 때까지 사용할 수 없습니다.  업그"
"레이드가 제공하는 새로운 기능과 이미 지원되는 기능을 확인하려면 `zpool "
"upgrade -v` 를 사용하세요."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1283
msgid "Upgrade a pool to support new feature flags:"
msgstr "새로운 feature flags를 지원하도록 풀을 업그레이드합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1296
#, no-wrap
msgid ""
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"status: Some supported features are not enabled on the pool. The pool can\n"
"        still be used, but some features are unavailable.\n"
"action: Enable all features using 'zpool upgrade'. Once this is done,\n"
"        the pool may no longer be accessible by software that does not support\n"
"        the features. See zpool-features(7) for details.\n"
"  scan: none requested\n"
"config:\n"
msgstr ""
"# zpool status\n"
"  pool: mypool\n"
" state: ONLINE\n"
"status: Some supported features are not enabled on the pool. The pool can\n"
"        still be used, but some features are unavailable.\n"
"action: Enable all features using 'zpool upgrade'. Once this is done,\n"
"        the pool may no longer be accessible by software that does not support\n"
"        the features. See zpool-features(7) for details.\n"
"  scan: none requested\n"
"config:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1308
#, no-wrap
msgid "All pools are formatted using feature flags.\n"
msgstr "All pools are formatted using feature flags.\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1312
#, no-wrap
msgid ""
"Some supported features are not enabled on the following pools. Once a\n"
"feature is enabled the pool may become incompatible with software\n"
"that does not support the feature. See zpool-features(7) for details.\n"
msgstr ""
"Some supported features are not enabled on the following pools. Once a\n"
"feature is enabled the pool may become incompatible with software\n"
"that does not support the feature. See zpool-features(7) for details.\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1325
#, no-wrap
msgid ""
"POOL  FEATURE\n"
"---------------\n"
"zstore\n"
"      multi_vdev_crash_dump\n"
"      spacemap_histogram\n"
"      enabled_txg\n"
"      hole_birth\n"
"      extensible_dataset\n"
"      bookmarks\n"
"      filesystem_limits\n"
"# zpool upgrade mypool\n"
"This system supports ZFS pool feature flags.\n"
msgstr ""
"POOL  FEATURE\n"
"---------------\n"
"zstore\n"
"      multi_vdev_crash_dump\n"
"      spacemap_histogram\n"
"      enabled_txg\n"
"      hole_birth\n"
"      extensible_dataset\n"
"      bookmarks\n"
"      filesystem_limits\n"
"# zpool upgrade mypool\n"
"This system supports ZFS pool feature flags.\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1333
#, no-wrap
msgid ""
"Enabled the following features on 'mypool':\n"
"  spacemap_histogram\n"
"  enabled_txg\n"
"  hole_birth\n"
"  extensible_dataset\n"
"  bookmarks\n"
"  filesystem_limits\n"
msgstr ""
"Enabled the following features on 'mypool':\n"
"  spacemap_histogram\n"
"  enabled_txg\n"
"  hole_birth\n"
"  extensible_dataset\n"
"  bookmarks\n"
"  filesystem_limits\n"

#. type: delimited block = 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1340
msgid ""
"Update the boot code on systems that boot from a pool to support the new "
"pool version.  Use `gpart bootcode` on the partition that contains the boot "
"code.  Two types of bootcode are available, depending on way the system "
"boots: GPT (the most common option) and EFI (for more modern systems)."
msgstr ""
"풀에서 부팅하는 시스템의 부트 코드를 업데이트하여 새 풀 버전을 지원하도록 합"
"니다.  부트 코드가 포함된 파티션에서 `gpart bootcode` 를 사용합니다.  시스템 "
"부팅 방식에 따라 두 가지 유형의 부트코드를 사용할 수 있습니다: GPT(가장 일반"
"적인 옵션) 및 EFI(최신 시스템용)."

#. type: delimited block = 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1342
msgid "For legacy boot using GPT, use the following command:"
msgstr "GPT를 사용하는 레거시 부팅의 경우 다음 명령을 사용합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1346
#, no-wrap
msgid "# gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada1\n"
msgstr "# gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada1\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1349
msgid "For systems using EFI to boot, execute the following command:"
msgstr "EFI를 사용하여 부팅하는 시스템의 경우 다음 명령을 실행합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1353
#, no-wrap
msgid "# gpart bootcode -p /boot/boot1.efifat -i 1 ada1\n"
msgstr "# gpart bootcode -p /boot/boot1.efifat -i 1 ada1\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1357
msgid ""
"Apply the bootcode to all bootable disks in the pool.  See man:gpart[8] for "
"more information."
msgstr ""
"풀의 모든 부팅 가능한 디스크에 부트 코드를 적용합니다.  자세한 내용은 man:"
"gpart[8]을 참조하세요."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:1360
#, no-wrap
msgid "Displaying Recorded Pool History"
msgstr "기록된 풀 기록 표시하기"

#. type: delimited block = 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1366
msgid ""
"ZFS records commands that change the pool, including creating datasets, "
"changing properties, or replacing a disk.  Reviewing history about a pool's "
"creation is useful, as is checking which user performed a specific action "
"and when.  History is not kept in a log file, but is part of the pool "
"itself.  The command to review this history is aptly named `zpool history`:"
msgstr ""
"ZFS는 데이터 세트 생성, 속성 변경, 디스크 교체 등 풀을 변경하는 명령을 기록합"
"니다.  풀 생성에 대한 기록을 검토하는 것은 어떤 사용자가 언제 특정 작업을 수"
"행했는지 확인하는 것과 마찬가지로 유용합니다.  기록은 로그 파일에 보관되지 않"
"고 풀 자체의 일부입니다.  이 기록을 검토하는 명령의 이름은 `zpool history` 입"
"니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1375
#, no-wrap
msgid ""
"# zpool history\n"
"History for 'tank':\n"
"2013-02-26.23:02:35 zpool create tank mirror /dev/ada0 /dev/ada1\n"
"2013-02-27.18:50:58 zfs set atime=off tank\n"
"2013-02-27.18:51:09 zfs set checksum=fletcher4 tank\n"
"2013-02-27.18:51:18 zfs create tank/backup\n"
msgstr ""
"# zpool history\n"
"History for 'tank':\n"
"2013-02-26.23:02:35 zpool create tank mirror /dev/ada0 /dev/ada1\n"
"2013-02-27.18:50:58 zfs set atime=off tank\n"
"2013-02-27.18:51:09 zfs set checksum=fletcher4 tank\n"
"2013-02-27.18:51:18 zfs create tank/backup\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1380
msgid ""
"The output shows `zpool` and `zfs` commands altering the pool in some way "
"along with a timestamp.  Commands like `zfs list` are not included.  When "
"specifying no pool name, ZFS displays history of all pools."
msgstr ""
"출력에는 타임스탬프와 함께 어떤 식으로든 풀을 변경하는 `zpool` 및 `zfs` 명령"
"이 표시됩니다.  `zfs list` 와 같은 명령은 포함되지 않습니다.  풀 이름을 지정"
"하지 않으면 ZFS는 모든 풀의 기록을 표시합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1383
msgid ""
"`zpool history` can show even more information when providing the options `-"
"i` or `-l`.  `-i` displays user-initiated events as well as internally "
"logged ZFS events."
msgstr ""
"`zpool history` 는 `-i` 또는 `-l` 옵션을 사용하면 더 많은 정보를 표시할 수 있"
"습니다.  `-i` 는 내부적으로 기록된 ZFS 이벤트뿐만 아니라 사용자가 시작한 이벤"
"트도 표시합니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1395
#, no-wrap
msgid ""
"# zpool history -i\n"
"History for 'tank':\n"
"2013-02-26.23:02:35 [internal pool create txg:5] pool spa 28; zfs spa 28; zpl 5;uts  9.1-RELEASE 901000 amd64\n"
"2013-02-27.18:50:53 [internal property set txg:50] atime=0 dataset = 21\n"
"2013-02-27.18:50:58 zfs set atime=off tank\n"
"2013-02-27.18:51:04 [internal property set txg:53] checksum=7 dataset = 21\n"
"2013-02-27.18:51:09 zfs set checksum=fletcher4 tank\n"
"2013-02-27.18:51:13 [internal create txg:55] dataset = 39\n"
"2013-02-27.18:51:18 zfs create tank/backup\n"
msgstr ""
"# zpool history -i\n"
"History for 'tank':\n"
"2013-02-26.23:02:35 [internal pool create txg:5] pool spa 28; zfs spa 28; zpl 5;uts  9.1-RELEASE 901000 amd64\n"
"2013-02-27.18:50:53 [internal property set txg:50] atime=0 dataset = 21\n"
"2013-02-27.18:50:58 zfs set atime=off tank\n"
"2013-02-27.18:51:04 [internal property set txg:53] checksum=7 dataset = 21\n"
"2013-02-27.18:51:09 zfs set checksum=fletcher4 tank\n"
"2013-02-27.18:51:13 [internal create txg:55] dataset = 39\n"
"2013-02-27.18:51:18 zfs create tank/backup\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1399
msgid ""
"Show more details by adding `-l`.  Showing history records in a long format, "
"including information like the name of the user who issued the command and "
"the hostname on which the change happened."
msgstr ""
"`-l` 을 추가하여 자세한 내용을 표시합니다.  명령을 실행한 사용자 이름, 변경"
"이 발생한 호스트 이름 등의 정보를 포함하여 긴 형식의 기록 기록을 표시합니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1408
#, no-wrap
msgid ""
"# zpool history -l\n"
"History for 'tank':\n"
"2013-02-26.23:02:35 zpool create tank mirror /dev/ada0 /dev/ada1 [user 0 (root) on :global]\n"
"2013-02-27.18:50:58 zfs set atime=off tank [user 0 (root) on myzfsbox:global]\n"
"2013-02-27.18:51:09 zfs set checksum=fletcher4 tank [user 0 (root) on myzfsbox:global]\n"
"2013-02-27.18:51:18 zfs create tank/backup [user 0 (root) on myzfsbox:global]\n"
msgstr ""
"# zpool history -l\n"
"History for 'tank':\n"
"2013-02-26.23:02:35 zpool create tank mirror /dev/ada0 /dev/ada1 [user 0 (root) on :global]\n"
"2013-02-27.18:50:58 zfs set atime=off tank [user 0 (root) on myzfsbox:global]\n"
"2013-02-27.18:51:09 zfs set checksum=fletcher4 tank [user 0 (root) on myzfsbox:global]\n"
"2013-02-27.18:51:18 zfs create tank/backup [user 0 (root) on myzfsbox:global]\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1414
msgid ""
"The output shows that the `root` user created the mirrored pool with disks [."
"filename]#/dev/ada0# and [.filename]#/dev/ada1#.  The hostname `myzfsbox` is "
"also shown in the commands after the pool's creation.  The hostname display "
"becomes important when exporting the pool from one system and importing on "
"another.  It's possible to distinguish the commands issued on the other "
"system by the hostname recorded for each command."
msgstr ""
"출력에는 `root` 사용자가 [.filename]#/dev/ada0# 및 [.filename]#/dev/ada1# 디"
"스크로 미러링된 풀을 생성했음을 보여줍니다.  풀 생성 후 명령에 호스트 이름 "
"`myzfsbox` 도 표시됩니다.  호스트 이름 표시는 한 시스템에서 풀을 내보내고 다"
"른 시스템에서 가져올 때 중요합니다.  각 명령에 대해 기록된 호스트 이름을 통"
"해 다른 시스템에서 실행된 명령을 구별할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1417
msgid ""
"Combine both options to `zpool history` to give the most detailed "
"information possible for any given pool.  Pool history provides valuable "
"information when tracking down the actions performed or when needing more "
"detailed output for debugging."
msgstr ""
"두 옵션을 모두 'zpool history' 로 결합하면 특정 풀에 대해 가능한 가장 자세한 "
"정보를 얻을 수 있습니다.  풀 히스토리는 수행한 작업을 추적하거나 디버깅을 위"
"해 더 자세한 출력이 필요할 때 유용한 정보를 제공합니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:1419
#, no-wrap
msgid "Performance Monitoring"
msgstr "성능 모니터링"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1426
msgid ""
"A built-in monitoring system can display pool I/O statistics in real time.  "
"It shows the amount of free and used space on the pool, read and write "
"operations performed per second, and I/O bandwidth used.  By default, ZFS "
"monitors and displays all pools in the system.  Provide a pool name to limit "
"monitoring to that pool.  A basic example:"
msgstr ""
"내장된 모니터링 시스템은 풀 I/O 통계를 실시간으로 표시할 수 있습니다.  풀의 "
"여유 공간과 사용 공간, 초당 수행되는 읽기 및 쓰기 작업, 사용된 I/O 대역폭이 "
"표시됩니다.  기본적으로 ZFS는 시스템의 모든 풀을 모니터링하고 표시합니다.  모"
"니터링을 해당 풀로 제한하려면 풀 이름을 입력합니다.  기본 예시입니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1434
#, no-wrap
msgid ""
"# zpool iostat\n"
"               capacity     operations    bandwidth\n"
"pool        alloc   free   read  write   read  write\n"
"----------  -----  -----  -----  -----  -----  -----\n"
"data         288G  1.53T      2     11  11.3K  57.1K\n"
msgstr ""
"# zpool iostat\n"
"               capacity     operations    bandwidth\n"
"pool        alloc   free   read  write   read  write\n"
"----------  -----  -----  -----  -----  -----  -----\n"
"data         288G  1.53T      2     11  11.3K  57.1K\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1440
msgid ""
"To continuously see I/O activity, specify a number as the last parameter, "
"indicating an interval in seconds to wait between updates.  The next "
"statistic line prints after each interval.  Press kbd:[Ctrl+C] to stop this "
"continuous monitoring.  Give a second number on the command line after the "
"interval to specify the total number of statistics to display."
msgstr ""
"I/O 활동을 계속 확인하려면 마지막 매개변수로 숫자를 지정하여 업데이트 사이에 "
"대기할 간격(초)을 표시합니다.  다음 통계 라인은 각 간격 후에 인쇄됩니다.  이 "
"연속 모니터링을 중지하려면 kbd:[Ctrl+C]를 누릅니다.  간격 뒤에 명령줄에 두 번"
"째 숫자를 입력하여 표시할 총 통계 수를 지정합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1445
msgid ""
"Display even more detailed I/O statistics with `-v`.  Each device in the "
"pool appears with a statistics line.  This is useful for seeing read and "
"write operations performed on each device, and can help determine if any "
"individual device is slowing down the pool.  This example shows a mirrored "
"pool with two devices:"
msgstr ""
"`-v` 를 사용하면 더 자세한 I/O 통계를 표시할 수 있습니다.  풀의 각 장치가 통"
"계 라인과 함께 표시됩니다.  이는 각 장치에서 수행되는 읽기 및 쓰기 작업을 확"
"인하는 데 유용하며, 개별 장치로 인해 풀의 속도가 느려지는지 확인하는 데 도움"
"이 될 수 있습니다.  이 예는 두 개의 장치가 있는 미러링된 풀을 보여줍니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1457
#, fuzzy, no-wrap
#| msgid ""
#| "# zpool iostat -v \n"
#| "                            capacity     operations    bandwidth\n"
#| "pool                     alloc   free   read  write   read  write\n"
#| "-----------------------  -----  -----  -----  -----  -----  -----\n"
#| "data                      288G  1.53T      2     12  9.23K  61.5K\n"
#| "  mirror                  288G  1.53T      2     12  9.23K  61.5K\n"
#| "    ada1                     -      -      0      4  5.61K  61.7K\n"
#| "    ada2                     -      -      1      4  5.04K  61.7K\n"
#| "-----------------------  -----  -----  -----  -----  -----  -----\n"
msgid ""
"# zpool iostat -v\n"
"                            capacity     operations    bandwidth\n"
"pool                     alloc   free   read  write   read  write\n"
"-----------------------  -----  -----  -----  -----  -----  -----\n"
"data                      288G  1.53T      2     12  9.23K  61.5K\n"
"  mirror                  288G  1.53T      2     12  9.23K  61.5K\n"
"    ada1                     -      -      0      4  5.61K  61.7K\n"
"    ada2                     -      -      1      4  5.04K  61.7K\n"
"-----------------------  -----  -----  -----  -----  -----  -----\n"
msgstr ""
"# zpool iostat -v \n"
"                            capacity     operations    bandwidth\n"
"pool                     alloc   free   read  write   read  write\n"
"-----------------------  -----  -----  -----  -----  -----  -----\n"
"data                      288G  1.53T      2     12  9.23K  61.5K\n"
"  mirror                  288G  1.53T      2     12  9.23K  61.5K\n"
"    ada1                     -      -      0      4  5.61K  61.7K\n"
"    ada2                     -      -      1      4  5.04K  61.7K\n"
"-----------------------  -----  -----  -----  -----  -----  -----\n"

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:1460
#, no-wrap
msgid "Splitting a Storage Pool"
msgstr "스토리지 풀 분할하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1467
msgid ""
"ZFS can split a pool consisting of one or more mirror vdevs into two pools.  "
"Unless otherwise specified, ZFS detaches the last member of each mirror and "
"creates a new pool containing the same data.  Be sure to make a dry run of "
"the operation with `-n` first.  This displays the details of the requested "
"operation without actually performing it.  This helps confirm that the "
"operation will do what the user intends."
msgstr ""
"ZFS는 하나 이상의 미러 vdev로 구성된 풀을 두 개의 풀로 분할할 수 있습니다.  "
"달리 지정하지 않는 한, ZFS는 각 미러의 마지막 멤버를 분리하고 동일한 데이터"
"를 포함하는 새 풀을 생성합니다.  먼저 `-n` 을 사용하여 드라이런 작업을 수행해"
"야 합니다.  이렇게 하면 요청된 작업을 실제로 수행하지 않고도 요청된 작업의 세"
"부 정보가 표시됩니다.  이렇게 하면 사용자가 의도한 대로 작업이 수행되는지 확"
"인할 수 있습니다."

#. type: Title ==
#: documentation/content/en/books/handbook/zfs/_index.adoc:1469
#, no-wrap
msgid "`zfs` Administration"
msgstr "`zfs` 관리"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1473
#, fuzzy
#| msgid ""
#| "The `zfs` utility can create, destroy, and manage all existing ZFS "
#| "datasets within a pool.  To manage the pool itself, use <<zfs-zpool,"
#| "`zpool`>>."
msgid ""
"The `zfs` utility can create, destroy, and manage all existing ZFS datasets "
"within a pool.  To manage the pool itself, use crossref:zfs[zfs-zpool,"
"`zpool`]."
msgstr ""
"`zfs` 유틸리티는 풀 내의 모든 기존 ZFS 데이터세트를 생성, 삭제 및 관리할 수 "
"있습니다.  풀 자체를 관리하려면 <<zfs-zpool,`zpool`>> 을 사용합니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:1475
#, no-wrap
msgid "Creating and Destroying Datasets"
msgstr "데이터 세트 생성 및 삭제하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1486
#, fuzzy
#| msgid ""
#| "Unlike traditional disks and volume managers, space in ZFS is _not_ "
#| "preallocated.  With traditional file systems, after partitioning and "
#| "assigning the space, there is no way to add a new file system without "
#| "adding a new disk.  With ZFS, creating new file systems is possible at "
#| "any time.  Each <<zfs-term-dataset,_dataset_>> has properties including "
#| "features like compression, deduplication, caching, and quotas, as well as "
#| "other useful properties like readonly, case sensitivity, network file "
#| "sharing, and a mount point.  Nesting datasets within each other is "
#| "possible and child datasets will inherit properties from their "
#| "ancestors.  <<zfs-zfs-allow,Delegate>>, <<zfs-zfs-send,replicate>>, <<zfs-"
#| "zfs-snapshot,snapshot>>, <<zfs-zfs-jail,jail>> allows administering and "
#| "destroying each dataset as a unit.  Creating a separate dataset for each "
#| "different type or set of files has advantages.  The drawbacks to having a "
#| "large number of datasets are that some commands like `zfs list` will be "
#| "slower, and that mounting of hundreds or even thousands of datasets will "
#| "slow the FreeBSD boot process."
msgid ""
"Unlike traditional disks and volume managers, space in ZFS is _not_ "
"preallocated.  With traditional file systems, after partitioning and "
"assigning the space, there is no way to add a new file system without adding "
"a new disk.  With ZFS, creating new file systems is possible at any time.  "
"Each crossref:zfs[zfs-term-dataset,_dataset_] has properties including "
"features like compression, deduplication, caching, and quotas, as well as "
"other useful properties like readonly, case sensitivity, network file "
"sharing, and a mount point.  Nesting datasets within each other is possible "
"and child datasets will inherit properties from their ancestors.  crossref:"
"zfs[zfs-zfs-allow,Delegate], crossref:zfs[zfs-zfs-send,replicate], crossref:"
"zfs[zfs-zfs-snapshot,snapshot], crossref:zfs[zfs-zfs-jail,jail] allows "
"administering and destroying each dataset as a unit.  Creating a separate "
"dataset for each different type or set of files has advantages.  The "
"drawbacks to having a large number of datasets are that some commands like "
"`zfs list` will be slower, and that mounting of hundreds or even thousands "
"of datasets will slow the FreeBSD boot process."
msgstr ""
"기존 디스크 및 볼륨 관리자와 달리 ZFS의 공간은 _사전 할당되지_ 않습니다.  기"
"존 파일 시스템에서는 파티션을 나누고 공간을 할당하고 나면 새 디스크를 추가하"
"지 않고는 새 파일 시스템을 추가할 방법이 없습니다.  ZFS를 사용하면 언제든지 "
"새 파일 시스템을 만들 수 있습니다.  각 <<zfs-term-dataset,_dataset_>> 에는 압"
"축, 중복 제거, 캐싱, 할당량 등의 기능과 읽기 전용, 대소문자 구분, 네트워크 파"
"일 공유, 마운트 지점과 같은 기타 유용한 속성이 있습니다.  데이터 세트를 서로 "
"중첩할 수 있으며, 자식 데이터 세트은 상위 데이터 세트으로부터 속성을 상속받습"
"니다.  <<zfs-zfs-allow,Delegate>>, <<zfs-zfs-send,replicate>>, <<zfs-zfs-"
"snapshot,snapshot>>, <<zfs-zfs-jail,jail>> 은 각 데이터셋을 하나의 단위로 관"
"리 및 파기할 수 있도록 합니다.  각기 다른 유형 또는 파일 세트에 대해 별도의 "
"데이터세트를 생성하면 장점이 있습니다.  데이터 세트가 많으면 `zfs list` 와 같"
"은 일부 명령의 속도가 느려지고 수백 또는 수천 개의 데이터 세트를 마운트하면 "
"FreeBSD 부팅 프로세스가 느려질 수 있다는 단점이 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1488
#, fuzzy
#| msgid ""
#| "Create a new dataset and enable <<zfs-term-compression-lz4,LZ4 "
#| "compression>> on it:"
msgid ""
"Create a new dataset and enable crossref:zfs[zfs-term-compression-lz4,LZ4 "
"compression] on it:"
msgstr ""
"새 데이터 세트를 생성하고 <<zfs-term-compression-lz4,LZ4 compression>> 을 활"
"성화합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1523
#, no-wrap
msgid ""
"# zfs list\n"
"NAME                  USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool                781M  93.2G   144K  none\n"
"mypool/ROOT           777M  93.2G   144K  none\n"
"mypool/ROOT/default   777M  93.2G   777M  /\n"
"mypool/tmp            176K  93.2G   176K  /tmp\n"
"mypool/usr            616K  93.2G   144K  /usr\n"
"mypool/usr/home       184K  93.2G   184K  /usr/home\n"
"mypool/usr/ports      144K  93.2G   144K  /usr/ports\n"
"mypool/usr/src        144K  93.2G   144K  /usr/src\n"
"mypool/var           1.20M  93.2G   608K  /var\n"
"mypool/var/crash      148K  93.2G   148K  /var/crash\n"
"mypool/var/log        178K  93.2G   178K  /var/log\n"
"mypool/var/mail       144K  93.2G   144K  /var/mail\n"
"mypool/var/tmp        152K  93.2G   152K  /var/tmp\n"
"# zfs create -o compress=lz4 mypool/usr/mydataset\n"
"# zfs list\n"
"NAME                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool                 781M  93.2G   144K  none\n"
"mypool/ROOT            777M  93.2G   144K  none\n"
"mypool/ROOT/default    777M  93.2G   777M  /\n"
"mypool/tmp             176K  93.2G   176K  /tmp\n"
"mypool/usr             704K  93.2G   144K  /usr\n"
"mypool/usr/home        184K  93.2G   184K  /usr/home\n"
"mypool/usr/mydataset  87.5K  93.2G  87.5K  /usr/mydataset\n"
"mypool/usr/ports       144K  93.2G   144K  /usr/ports\n"
"mypool/usr/src         144K  93.2G   144K  /usr/src\n"
"mypool/var            1.20M  93.2G   610K  /var\n"
"mypool/var/crash       148K  93.2G   148K  /var/crash\n"
"mypool/var/log         178K  93.2G   178K  /var/log\n"
"mypool/var/mail        144K  93.2G   144K  /var/mail\n"
"mypool/var/tmp         152K  93.2G   152K  /var/tmp\n"
msgstr ""
"# zfs list\n"
"NAME                  USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool                781M  93.2G   144K  none\n"
"mypool/ROOT           777M  93.2G   144K  none\n"
"mypool/ROOT/default   777M  93.2G   777M  /\n"
"mypool/tmp            176K  93.2G   176K  /tmp\n"
"mypool/usr            616K  93.2G   144K  /usr\n"
"mypool/usr/home       184K  93.2G   184K  /usr/home\n"
"mypool/usr/ports      144K  93.2G   144K  /usr/ports\n"
"mypool/usr/src        144K  93.2G   144K  /usr/src\n"
"mypool/var           1.20M  93.2G   608K  /var\n"
"mypool/var/crash      148K  93.2G   148K  /var/crash\n"
"mypool/var/log        178K  93.2G   178K  /var/log\n"
"mypool/var/mail       144K  93.2G   144K  /var/mail\n"
"mypool/var/tmp        152K  93.2G   152K  /var/tmp\n"
"# zfs create -o compress=lz4 mypool/usr/mydataset\n"
"# zfs list\n"
"NAME                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool                 781M  93.2G   144K  none\n"
"mypool/ROOT            777M  93.2G   144K  none\n"
"mypool/ROOT/default    777M  93.2G   777M  /\n"
"mypool/tmp             176K  93.2G   176K  /tmp\n"
"mypool/usr             704K  93.2G   144K  /usr\n"
"mypool/usr/home        184K  93.2G   184K  /usr/home\n"
"mypool/usr/mydataset  87.5K  93.2G  87.5K  /usr/mydataset\n"
"mypool/usr/ports       144K  93.2G   144K  /usr/ports\n"
"mypool/usr/src         144K  93.2G   144K  /usr/src\n"
"mypool/var            1.20M  93.2G   610K  /var\n"
"mypool/var/crash       148K  93.2G   148K  /var/crash\n"
"mypool/var/log         178K  93.2G   178K  /var/log\n"
"mypool/var/mail        144K  93.2G   144K  /var/mail\n"
"mypool/var/tmp         152K  93.2G   152K  /var/tmp\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1526
msgid ""
"Destroying a dataset is much quicker than deleting the files on the dataset, "
"as it does not involve scanning the files and updating the corresponding "
"metadata."
msgstr ""
"데이터 세트를 삭제하는 것은 파일을 스캔하고 해당 메타데이터를 업데이트할 필요"
"가 없기 때문에 데이터 세트의 파일을 삭제하는 것보다 훨씬 빠릅니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1528
msgid "Destroy the created dataset:"
msgstr "셍성된 데이터 세트를 삭제합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1563
#, no-wrap
msgid ""
"# zfs list\n"
"NAME                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool                 880M  93.1G   144K  none\n"
"mypool/ROOT            777M  93.1G   144K  none\n"
"mypool/ROOT/default    777M  93.1G   777M  /\n"
"mypool/tmp             176K  93.1G   176K  /tmp\n"
"mypool/usr             101M  93.1G   144K  /usr\n"
"mypool/usr/home        184K  93.1G   184K  /usr/home\n"
"mypool/usr/mydataset   100M  93.1G   100M  /usr/mydataset\n"
"mypool/usr/ports       144K  93.1G   144K  /usr/ports\n"
"mypool/usr/src         144K  93.1G   144K  /usr/src\n"
"mypool/var            1.20M  93.1G   610K  /var\n"
"mypool/var/crash       148K  93.1G   148K  /var/crash\n"
"mypool/var/log         178K  93.1G   178K  /var/log\n"
"mypool/var/mail        144K  93.1G   144K  /var/mail\n"
"mypool/var/tmp         152K  93.1G   152K  /var/tmp\n"
"# zfs destroy mypool/usr/mydataset\n"
"# zfs list\n"
"NAME                  USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool                781M  93.2G   144K  none\n"
"mypool/ROOT           777M  93.2G   144K  none\n"
"mypool/ROOT/default   777M  93.2G   777M  /\n"
"mypool/tmp            176K  93.2G   176K  /tmp\n"
"mypool/usr            616K  93.2G   144K  /usr\n"
"mypool/usr/home       184K  93.2G   184K  /usr/home\n"
"mypool/usr/ports      144K  93.2G   144K  /usr/ports\n"
"mypool/usr/src        144K  93.2G   144K  /usr/src\n"
"mypool/var           1.21M  93.2G   612K  /var\n"
"mypool/var/crash      148K  93.2G   148K  /var/crash\n"
"mypool/var/log        178K  93.2G   178K  /var/log\n"
"mypool/var/mail       144K  93.2G   144K  /var/mail\n"
"mypool/var/tmp        152K  93.2G   152K  /var/tmp\n"
msgstr ""
"# zfs list\n"
"NAME                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool                 880M  93.1G   144K  none\n"
"mypool/ROOT            777M  93.1G   144K  none\n"
"mypool/ROOT/default    777M  93.1G   777M  /\n"
"mypool/tmp             176K  93.1G   176K  /tmp\n"
"mypool/usr             101M  93.1G   144K  /usr\n"
"mypool/usr/home        184K  93.1G   184K  /usr/home\n"
"mypool/usr/mydataset   100M  93.1G   100M  /usr/mydataset\n"
"mypool/usr/ports       144K  93.1G   144K  /usr/ports\n"
"mypool/usr/src         144K  93.1G   144K  /usr/src\n"
"mypool/var            1.20M  93.1G   610K  /var\n"
"mypool/var/crash       148K  93.1G   148K  /var/crash\n"
"mypool/var/log         178K  93.1G   178K  /var/log\n"
"mypool/var/mail        144K  93.1G   144K  /var/mail\n"
"mypool/var/tmp         152K  93.1G   152K  /var/tmp\n"
"# zfs destroy mypool/usr/mydataset\n"
"# zfs list\n"
"NAME                  USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool                781M  93.2G   144K  none\n"
"mypool/ROOT           777M  93.2G   144K  none\n"
"mypool/ROOT/default   777M  93.2G   777M  /\n"
"mypool/tmp            176K  93.2G   176K  /tmp\n"
"mypool/usr            616K  93.2G   144K  /usr\n"
"mypool/usr/home       184K  93.2G   184K  /usr/home\n"
"mypool/usr/ports      144K  93.2G   144K  /usr/ports\n"
"mypool/usr/src        144K  93.2G   144K  /usr/src\n"
"mypool/var           1.21M  93.2G   612K  /var\n"
"mypool/var/crash      148K  93.2G   148K  /var/crash\n"
"mypool/var/log        178K  93.2G   178K  /var/log\n"
"mypool/var/mail       144K  93.2G   144K  /var/mail\n"
"mypool/var/tmp        152K  93.2G   152K  /var/tmp\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1571
#, fuzzy
#| msgid ""
#| "In modern versions of ZFS, `zfs destroy` is asynchronous, and the free "
#| "space might take minutes to appear in the pool.  Use `zpool get freeing "
#| "_poolname_` to see the `freeing` property, that shows which datasets are "
#| "having their blocks freed in the background.  If there are child "
#| "datasets, like <<zfs-term-snapshot,snapshots>> or other datasets, "
#| "destroying the parent is impossible.  To destroy a dataset and its "
#| "children, use `-r` to recursively destroy the dataset and its children.  "
#| "Use `-n -v` to list datasets and snapshots destroyed by this operation, "
#| "without actually destroy anything.  Space reclaimed by destroying "
#| "snapshots is also shown."
msgid ""
"In modern versions of ZFS, `zfs destroy` is asynchronous, and the free space "
"might take minutes to appear in the pool.  Use `zpool get freeing "
"_poolname_` to see the `freeing` property, that shows which datasets are "
"having their blocks freed in the background.  If there are child datasets, "
"like crossref:zfs[zfs-term-snapshot,snapshots] or other datasets, destroying "
"the parent is impossible.  To destroy a dataset and its children, use `-r` "
"to recursively destroy the dataset and its children.  Use `-n -v` to list "
"datasets and snapshots destroyed by this operation, without actually destroy "
"anything.  Space reclaimed by destroying snapshots is also shown."
msgstr ""
"최신 버전의 ZFS에서 `zfs destroy` 는 비동기식이며, 여유 공간이 풀에 나타나는 "
"데 몇 분 정도 걸릴 수 있습니다.  `zpool get freeing _poolname_` 을 사용하면 "
"어떤 데이터 세트가 백그라운드에서 블록을 해제하고 있는지 보여주는 `freeing` "
"속성을 확인할 수 있습니다.  <<zfs-term-snapshot,snapshots>> 또는 다른 데이터 "
"세트와 같은 하위 데이터 세트가 있는 경우, 상위 데이터 세트를 삭제할 수 없습니"
"다.  데이터 세트와 그 자식을 파기하려면 `-r` 을 사용하여 데이터 세트와 그 자"
"식을 재귀적으로 파기해야 합니다.  실제로 아무것도 파괴하지 않고 이 작업으로 "
"파괴된 데이터세트와 스냅샷을 나열하려면 `-n -v` 를 사용합니다.  스냅샷을 파괴"
"하여 회수된 공간도 표시됩니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:1573
#, no-wrap
msgid "Creating and Destroying Volumes"
msgstr "볼륨 생성 및 삭제하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1578
msgid ""
"A volume is a special dataset type.  Rather than mounting as a file system, "
"expose it as a block device under [.filename]#/dev/zvol/poolname/dataset#.  "
"This allows using the volume for other file systems, to back the disks of a "
"virtual machine, or to make it available to other network hosts using "
"protocols like iSCSI or HAST."
msgstr ""
"볼륨은 특수한 데이터 세트 유형입니다.  파일 시스템으로 마운트하는 대신 [."
"filename]#/dev/zvol/poolname/dataset# 에서 블록 장치로 노출시키세요.  이렇게 "
"하면 다른 파일 시스템에 볼륨을 사용하거나, 가상 머신의 디스크를 백업하거나, "
"iSCSI 또는 HAST와 같은 프로토콜을 사용하여 다른 네트워크 호스트에서 볼륨을 사"
"용할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1583
msgid ""
"Format a volume with any file system or without a file system to store raw "
"data.  To the user, a volume appears to be a regular disk.  Putting ordinary "
"file systems on these _zvols_ provides features that ordinary disks or file "
"systems do not have.  For example, using the compression property on a 250 "
"MB volume allows creation of a compressed FAT file system."
msgstr ""
"원시 데이터를 저장하기 위해 파일 시스템을 사용하거나 파일 시스템 없이 볼륨을 "
"포맷합니다.  사용자에게는 볼륨이 일반 디스크로 보입니다.  이러한 _zvols_ 에 "
"일반 파일 시스템을 넣으면 일반 디스크나 파일 시스템에는 없는 기능이 제공됩니"
"다.  예를 들어 250MB 볼륨에 압축 속성을 사용하면 압축된 FAT 파일 시스템을 만"
"들 수 있습니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1597
#, no-wrap
msgid ""
"# zfs create -V 250m -o compression=on tank/fat32\n"
"# zfs list tank\n"
"NAME USED AVAIL REFER MOUNTPOINT\n"
"tank 258M  670M   31K /tank\n"
"# newfs_msdos -F32 /dev/zvol/tank/fat32\n"
"# mount -t msdosfs /dev/zvol/tank/fat32 /mnt\n"
"# df -h /mnt | grep fat32\n"
"Filesystem           Size Used Avail Capacity Mounted on\n"
"/dev/zvol/tank/fat32 249M  24k  249M     0%   /mnt\n"
"# mount | grep fat32\n"
"/dev/zvol/tank/fat32 on /mnt (msdosfs, local)\n"
msgstr ""
"# zfs create -V 250m -o compression=on tank/fat32\n"
"# zfs list tank\n"
"NAME USED AVAIL REFER MOUNTPOINT\n"
"tank 258M  670M   31K /tank\n"
"# newfs_msdos -F32 /dev/zvol/tank/fat32\n"
"# mount -t msdosfs /dev/zvol/tank/fat32 /mnt\n"
"# df -h /mnt | grep fat32\n"
"Filesystem           Size Used Avail Capacity Mounted on\n"
"/dev/zvol/tank/fat32 249M  24k  249M     0%   /mnt\n"
"# mount | grep fat32\n"
"/dev/zvol/tank/fat32 on /mnt (msdosfs, local)\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1601
msgid ""
"Destroying a volume is much the same as destroying a regular file system "
"dataset.  The operation is nearly instantaneous, but it may take minutes to "
"reclaim the free space in the background."
msgstr ""
"볼륨을 삭제하는 것은 일반 파일 시스템 데이터 세트를 삭제하는 것과 거의 동일합"
"니다.  작업은 거의 즉각적으로 이루어지지만 백그라운드에서 여유 공간을 확보하"
"는 데 몇 분 정도 걸릴 수 있습니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:1603
#, no-wrap
msgid "Renaming a Dataset"
msgstr "데이터 세트 이름 바꾸기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1610
msgid ""
"To change the name of a dataset, use `zfs rename`.  To change the parent of "
"a dataset, use this command as well.  Renaming a dataset to have a different "
"parent dataset will change the value of those properties inherited from the "
"parent dataset.  Renaming a dataset unmounts then remounts it in the new "
"location (inherited from the new parent dataset).  To prevent this behavior, "
"use `-u`."
msgstr ""
"데이터 세트의 이름을 변경하려면 `zfs rename` 을 사용합니다.  데이터 세트의 상"
"위 데이터 세트을 변경하려면 이 명령도 사용합니다.  데이터 세트의 이름을 다른 "
"상위 데이터 세트로 변경하면 상위 데이터 세트에서 상속된 속성 값이 변경됩니"
"다.  데이터 세트의 이름을 바꾸면 마운트가 해제된 후 새 위치(새 상위 데이터 세"
"트에서 상속됨)에 다시 마운트됩니다.  이 동작을 방지하려면 `-u` 를 사용하십시"
"오."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1612
msgid "Rename a dataset and move it to be under a different parent dataset:"
msgstr "데이터 세트의 이름을 바꾸고 다른 상위 데이터 세트 아래로 이동합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1648
#, no-wrap
msgid ""
"# zfs list\n"
"NAME                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool                 780M  93.2G   144K  none\n"
"mypool/ROOT            777M  93.2G   144K  none\n"
"mypool/ROOT/default    777M  93.2G   777M  /\n"
"mypool/tmp             176K  93.2G   176K  /tmp\n"
"mypool/usr             704K  93.2G   144K  /usr\n"
"mypool/usr/home        184K  93.2G   184K  /usr/home\n"
"mypool/usr/mydataset  87.5K  93.2G  87.5K  /usr/mydataset\n"
"mypool/usr/ports       144K  93.2G   144K  /usr/ports\n"
"mypool/usr/src         144K  93.2G   144K  /usr/src\n"
"mypool/var            1.21M  93.2G   614K  /var\n"
"mypool/var/crash       148K  93.2G   148K  /var/crash\n"
"mypool/var/log         178K  93.2G   178K  /var/log\n"
"mypool/var/mail        144K  93.2G   144K  /var/mail\n"
"mypool/var/tmp         152K  93.2G   152K  /var/tmp\n"
"# zfs rename mypool/usr/mydataset mypool/var/newname\n"
"# zfs list\n"
"NAME                  USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool                780M  93.2G   144K  none\n"
"mypool/ROOT           777M  93.2G   144K  none\n"
"mypool/ROOT/default   777M  93.2G   777M  /\n"
"mypool/tmp            176K  93.2G   176K  /tmp\n"
"mypool/usr            616K  93.2G   144K  /usr\n"
"mypool/usr/home       184K  93.2G   184K  /usr/home\n"
"mypool/usr/ports      144K  93.2G   144K  /usr/ports\n"
"mypool/usr/src        144K  93.2G   144K  /usr/src\n"
"mypool/var           1.29M  93.2G   614K  /var\n"
"mypool/var/crash      148K  93.2G   148K  /var/crash\n"
"mypool/var/log        178K  93.2G   178K  /var/log\n"
"mypool/var/mail       144K  93.2G   144K  /var/mail\n"
"mypool/var/newname   87.5K  93.2G  87.5K  /var/newname\n"
"mypool/var/tmp        152K  93.2G   152K  /var/tmp\n"
msgstr ""
"# zfs list\n"
"NAME                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool                 780M  93.2G   144K  none\n"
"mypool/ROOT            777M  93.2G   144K  none\n"
"mypool/ROOT/default    777M  93.2G   777M  /\n"
"mypool/tmp             176K  93.2G   176K  /tmp\n"
"mypool/usr             704K  93.2G   144K  /usr\n"
"mypool/usr/home        184K  93.2G   184K  /usr/home\n"
"mypool/usr/mydataset  87.5K  93.2G  87.5K  /usr/mydataset\n"
"mypool/usr/ports       144K  93.2G   144K  /usr/ports\n"
"mypool/usr/src         144K  93.2G   144K  /usr/src\n"
"mypool/var            1.21M  93.2G   614K  /var\n"
"mypool/var/crash       148K  93.2G   148K  /var/crash\n"
"mypool/var/log         178K  93.2G   178K  /var/log\n"
"mypool/var/mail        144K  93.2G   144K  /var/mail\n"
"mypool/var/tmp         152K  93.2G   152K  /var/tmp\n"
"# zfs rename mypool/usr/mydataset mypool/var/newname\n"
"# zfs list\n"
"NAME                  USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool                780M  93.2G   144K  none\n"
"mypool/ROOT           777M  93.2G   144K  none\n"
"mypool/ROOT/default   777M  93.2G   777M  /\n"
"mypool/tmp            176K  93.2G   176K  /tmp\n"
"mypool/usr            616K  93.2G   144K  /usr\n"
"mypool/usr/home       184K  93.2G   184K  /usr/home\n"
"mypool/usr/ports      144K  93.2G   144K  /usr/ports\n"
"mypool/usr/src        144K  93.2G   144K  /usr/src\n"
"mypool/var           1.29M  93.2G   614K  /var\n"
"mypool/var/crash      148K  93.2G   148K  /var/crash\n"
"mypool/var/log        178K  93.2G   178K  /var/log\n"
"mypool/var/mail       144K  93.2G   144K  /var/mail\n"
"mypool/var/newname   87.5K  93.2G  87.5K  /var/newname\n"
"mypool/var/tmp        152K  93.2G   152K  /var/tmp\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1653
msgid ""
"Renaming snapshots uses the same command.  Due to the nature of snapshots, "
"rename cannot change their parent dataset.  To rename a recursive snapshot, "
"specify `-r`; this will also rename all snapshots with the same name in "
"child datasets."
msgstr ""
"스냅샷 이름 바꾸기는 동일한 명령을 사용합니다.  스냅샷의 특성상 이름을 변경해"
"도 상위 데이터 세트는 변경되지 않습니다.  재귀 스냅샷의 이름을 바꾸려면 `-r` "
"을 지정하면 하위 데이터 세트에 있는 모든 스냅샷의 이름도 같은 이름으로 바뀝니"
"다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1663
#, no-wrap
msgid ""
"# zfs list -t snapshot\n"
"NAME                                USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool/var/newname@first_snapshot      0      -  87.5K  -\n"
"# zfs rename mypool/var/newname@first_snapshot new_snapshot_name\n"
"# zfs list -t snapshot\n"
"NAME                                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool/var/newname@new_snapshot_name      0      -  87.5K  -\n"
msgstr ""
"# zfs list -t snapshot\n"
"NAME                                USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool/var/newname@first_snapshot      0      -  87.5K  -\n"
"# zfs rename mypool/var/newname@first_snapshot new_snapshot_name\n"
"# zfs list -t snapshot\n"
"NAME                                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool/var/newname@new_snapshot_name      0      -  87.5K  -\n"

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:1666
#, no-wrap
msgid "Setting Dataset Properties"
msgstr "데이터 세트 속성 설정하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1676
msgid ""
"Each ZFS dataset has properties that control its behavior.  Most properties "
"are automatically inherited from the parent dataset, but can be overridden "
"locally.  Set a property on a dataset with `zfs set _property=value "
"dataset_`.  Most properties have a limited set of valid values, `zfs get` "
"will display each possible property and valid values.  Using `zfs inherit` "
"reverts most properties to their inherited values.  User-defined properties "
"are also possible.  They become part of the dataset configuration and "
"provide further information about the dataset or its contents.  To "
"distinguish these custom properties from the ones supplied as part of ZFS, "
"use a colon (`:`) to create a custom namespace for the property."
msgstr ""
"각 ZFS 데이터 세트에는 그 동작을 제어하는 속성이 있습니다.  대부분의 속성은 "
"상위 데이터세트에서 자동으로 상속되지만 로컬에서 재정의할 수 있습니다.  데이"
"터 세트의 속성을 설정하려면 `zfs set _property=value dataset_` 를 사용합니"
"다.  대부분의 속성에는 유효한 값의 집합이 제한되어 있으며, `zfs get` 은 가능"
"한 각 속성과 유효한 값을 표시합니다.  `zfs inherit` 를 사용하면 대부분의 속성"
"이 상속된 값으로 되돌아갑니다.  사용자 정의 속성도 가능합니다.  이러한 속성"
"은 데이터 세트 구성의 일부가 되며 데이터 세트 또는 그 내용에 대한 추가 정보"
"를 제공합니다.  이러한 사용자 정의 속성을 ZFS의 일부로 제공되는 속성과 구별하"
"려면 콜론( `:` )을 사용하여 속성에 대한 사용자 정의 네임스페이스를 만듭니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1683
#, no-wrap
msgid ""
"# zfs set custom:costcenter=1234 tank\n"
"# zfs get custom:costcenter tank\n"
"NAME PROPERTY           VALUE SOURCE\n"
"tank custom:costcenter  1234  local\n"
msgstr ""
"# zfs set custom:costcenter=1234 tank\n"
"# zfs get custom:costcenter tank\n"
"NAME PROPERTY           VALUE SOURCE\n"
"tank custom:costcenter  1234  local\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1687
msgid ""
"To remove a custom property, use `zfs inherit` with `-r`.  If the custom "
"property is not defined in any of the parent datasets, this option removes "
"it (but the pool's history still records the change)."
msgstr ""
"사용자 지정 속성을 제거하려면 `zfs inherit` 에 `-r` 을 사용합니다.  사용자 지"
"정 속성이 상위 데이터 세트에 정의되어 있지 않은 경우 이 옵션을 사용하면 제거"
"되지만 풀의 기록에는 여전히 변경 내용이 기록됩니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1696
#, no-wrap
msgid ""
"# zfs inherit -r custom:costcenter tank\n"
"# zfs get custom:costcenter tank\n"
"NAME    PROPERTY           VALUE              SOURCE\n"
"tank    custom:costcenter  -                  -\n"
"# zfs get all tank | grep custom:costcenter\n"
"#\n"
msgstr ""
"# zfs inherit -r custom:costcenter tank\n"
"# zfs get custom:costcenter tank\n"
"NAME    PROPERTY           VALUE              SOURCE\n"
"tank    custom:costcenter  -                  -\n"
"# zfs get all tank | grep custom:costcenter\n"
"#\n"

#. type: Title ====
#: documentation/content/en/books/handbook/zfs/_index.adoc:1699
#, no-wrap
msgid "Getting and Setting Share Properties"
msgstr "공유 속성을 얻고 설정하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1705
msgid ""
"Two commonly used and useful dataset properties are the NFS and SMB share "
"options.  Setting these defines if and how ZFS shares datasets on the "
"network.  At present, FreeBSD supports setting NFS sharing alone.  To get "
"the current status of a share, enter:"
msgstr ""
"일반적으로 사용되며 유용한 두 가지 데이터 세트 속성은 NFS 및 SMB 공유 옵션입"
"니다.  이 설정은 ZFS가 네트워크에서 데이터 세트를 공유할지 여부와 방법을 정의"
"합니다.  현재 FreeBSD는 NFS 공유 설정만 지원합니다.  공유의 현재 상태를 확인"
"하려면 다음을 입력합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1714
#, no-wrap
msgid ""
"# zfs get sharenfs mypool/usr/home\n"
"NAME             PROPERTY  VALUE    SOURCE\n"
"mypool/usr/home  sharenfs  on       local\n"
"# zfs get sharesmb mypool/usr/home\n"
"NAME             PROPERTY  VALUE    SOURCE\n"
"mypool/usr/home  sharesmb  off      local\n"
msgstr ""
"# zfs get sharenfs mypool/usr/home\n"
"NAME             PROPERTY  VALUE    SOURCE\n"
"mypool/usr/home  sharenfs  on       local\n"
"# zfs get sharesmb mypool/usr/home\n"
"NAME             PROPERTY  VALUE    SOURCE\n"
"mypool/usr/home  sharesmb  off      local\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1717
msgid "To enable sharing of a dataset, enter:"
msgstr "데이터 세트의 공유 옵션을 활성화 하려면:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1721
#, no-wrap
msgid "#  zfs set sharenfs=on mypool/usr/home\n"
msgstr "#  zfs set sharenfs=on mypool/usr/home\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1725
msgid ""
"Set other options for sharing datasets through NFS, such as `-alldirs`, `-"
"maproot` and `-network`.  To set options on a dataset shared through NFS, "
"enter:"
msgstr ""
"`-alldirs`, `-maproot`, `-network` 등 NFS를 통해 데이터 세트를 공유하기 위한 "
"다른 옵션을 설정합니다.  NFS를 통해 공유되는 데이터 세트에 대한 옵션을 설정하"
"려면 다음을 입력합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1729
#, no-wrap
msgid "#  zfs set sharenfs=\"-alldirs,-maproot=root,-network=192.168.1.0/24\" mypool/usr/home\n"
msgstr "#  zfs set sharenfs=\"-alldirs,-maproot=root,-network=192.168.1.0/24\" mypool/usr/home\n"

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:1732
#, no-wrap
msgid "Managing Snapshots"
msgstr "스냅샷 관리하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1746
#, fuzzy
#| msgid ""
#| "<<zfs-term-snapshot,Snapshots>> are one of the most powerful features of "
#| "ZFS.  A snapshot provides a read-only, point-in-time copy of the "
#| "dataset.  With Copy-On-Write (COW), ZFS creates snapshots fast by "
#| "preserving older versions of the data on disk.  If no snapshots exist, "
#| "ZFS reclaims space for future use when data is rewritten or deleted.  "
#| "Snapshots preserve disk space by recording just the differences between "
#| "the current dataset and a previous version.  Allowing snapshots on whole "
#| "datasets, not on individual files or directories.  A snapshot from a "
#| "dataset duplicates everything contained in it.  This includes the file "
#| "system properties, files, directories, permissions, and so on.  Snapshots "
#| "use no extra space when first created, but consume space as the blocks "
#| "they reference change.  Recursive snapshots taken with `-r` create "
#| "snapshots with the same name on the dataset and its children, providing a "
#| "consistent moment-in-time snapshot of the file systems.  This can be "
#| "important when an application has files on related datasets or that "
#| "depend upon each other.  Without snapshots, a backup would have copies of "
#| "the files from different points in time."
msgid ""
"crossref:zfs[zfs-term-snapshot,Snapshots] are one of the most powerful "
"features of ZFS.  A snapshot provides a read-only, point-in-time copy of the "
"dataset.  With Copy-On-Write (COW), ZFS creates snapshots fast by preserving "
"older versions of the data on disk.  If no snapshots exist, ZFS reclaims "
"space for future use when data is rewritten or deleted.  Snapshots preserve "
"disk space by recording just the differences between the current dataset and "
"a previous version.  Allowing snapshots on whole datasets, not on individual "
"files or directories.  A snapshot from a dataset duplicates everything "
"contained in it.  This includes the file system properties, files, "
"directories, permissions, and so on.  Snapshots use no extra space when "
"first created, but consume space as the blocks they reference change.  "
"Recursive snapshots taken with `-r` create snapshots with the same name on "
"the dataset and its children, providing a consistent moment-in-time snapshot "
"of the file systems.  This can be important when an application has files on "
"related datasets or that depend upon each other.  Without snapshots, a "
"backup would have copies of the files from different points in time."
msgstr ""
"<<zfs-term-snapshot,Snapshots>> 은 ZFS의 가장 강력한 기능 중 하나입니다.  스"
"냅샷은 데이터 세트의 읽기 전용, 특정 시점 복사본을 제공합니다.  COW(Copy-On-"
"Write)를 사용하면 ZFS는 디스크에 이전 버전의 데이터를 보존하여 스냅샷을 빠르"
"게 생성합니다.  스냅샷이 존재하지 않는 경우, ZFS는 데이터를 다시 쓰거나 삭제"
"할 때 나중에 사용할 수 있도록 공간을 확보합니다.  스냅샷은 현재 데이터 세트"
"와 이전 버전 간의 차이점만 기록하여 디스크 공간을 보존합니다.  개별 파일이나 "
"디렉터리가 아닌 전체 데이터 세트에 대한 스냅샷을 허용합니다.  데이터 세트의 "
"스냅샷은 그 안에 포함된 모든 것을 복제합니다.  여기에는 파일 시스템 속성, 파"
"일, 디렉터리, 권한 등이 포함됩니다.  스냅샷은 처음 만들 때는 추가 공간을 사용"
"하지 않지만 참조하는 블록이 변경되면 공간을 소비합니다.  `-r` 로 만든 재귀 스"
"냅샷은 데이터 세트와 하위 데이터 세트에 동일한 이름의 스냅샷을 생성하여 파일 "
"시스템의 일관된 특정 시점의 스냅샷을 제공합니다.  이는 애플리케이션에 관련 데"
"이터 세트에 파일이 있거나 서로 의존하는 파일이 있는 경우 중요할 수 있습니"
"다.  스냅샷이 없으면 백업에는 서로 다른 시점의 파일 사본이 생성됩니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1755
msgid ""
"Snapshots in ZFS provide a variety of features that even other file systems "
"with snapshot functionality lack.  A typical example of snapshot use is as a "
"quick way of backing up the current state of the file system when performing "
"a risky action like a software installation or a system upgrade.  If the "
"action fails, rolling back to the snapshot returns the system to the same "
"state when creating the snapshot.  If the upgrade was successful, delete the "
"snapshot to free up space.  Without snapshots, a failed upgrade often "
"requires restoring backups, which is tedious, time consuming, and may "
"require downtime during which the system is unusable.  Rolling back to "
"snapshots is fast, even while the system is running in normal operation, "
"with little or no downtime.  The time savings are enormous with multi-"
"terabyte storage systems considering the time required to copy the data from "
"backup.  Snapshots are not a replacement for a complete backup of a pool, "
"but offer a quick and easy way to store a dataset copy at a specific time."
msgstr ""
"ZFS의 스냅샷은 스냅샷 기능이 있는 다른 파일 시스템에도 없는 다양한 기능을 제"
"공합니다.  스냅샷 사용의 일반적인 예는 소프트웨어 설치 또는 시스템 업그레이드"
"와 같은 위험한 작업을 수행할 때 파일 시스템의 현재 상태를 빠르게 백업하는 것"
"입니다.  작업이 실패한 경우 스냅샷으로 롤백하면 시스템이 스냅샷을 만들 때와 "
"동일한 상태로 되돌아갑니다.  업그레이드가 성공했다면 스냅샷을 삭제하여 공간"
"을 확보하세요.  스냅샷이 없으면 업그레이드에 실패하게 되면 백업을 복원해야 하"
"는 경우가 많은데, 이 작업은 지루하고 시간이 오래 걸리며 시스템을 사용할 수 없"
"는 다운타임이 발생할 수 있습니다.  스냅샷으로 롤백하면 시스템이 정상적으로 작"
"동하는 동안에도 다운타임이 거의 또는 전혀 없이 빠르게 롤백할 수 있습니다.  백"
"업에서 데이터를 복사하는 데 필요한 시간을 고려하면, 멀티 테라바이트 스토리지 "
"시스템을 사용하는 것이 시간을 엄청나게 절약할 수 있습니다.  스냅샷은 풀의 전"
"체 백업을 대체하는 것은 아니지만, 특정 시점에 데이터 세트 사본을 빠르고 쉽게 "
"저장할 수 있는 방법을 제공합니다."

#. type: Title ====
#: documentation/content/en/books/handbook/zfs/_index.adoc:1757
#, no-wrap
msgid "Creating Snapshots"
msgstr "스냅샷 생성하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1761
msgid ""
"To create snapshots, use `zfs snapshot _dataset_@_snapshotname_`.  Adding `-"
"r` creates a snapshot recursively, with the same name on all child datasets."
msgstr ""
"스냅샷을 생성하려면 `zfs 스냅샷 _데이터셋_@_스냅샷이름_` 을 사용합니다.  `-"
"r` 을 추가하면 모든 하위 데이터 세트에 동일한 이름으로 재귀적 스냅샷이 생성됩"
"니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1763
msgid "Create a recursive snapshot of the entire pool:"
msgstr "전체 풀의 재귀적 스냅샷을 만듭니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1801
#, no-wrap
msgid ""
"# zfs list -t all\n"
"NAME                                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool                                 780M  93.2G   144K  none\n"
"mypool/ROOT                            777M  93.2G   144K  none\n"
"mypool/ROOT/default                    777M  93.2G   777M  /\n"
"mypool/tmp                             176K  93.2G   176K  /tmp\n"
"mypool/usr                             616K  93.2G   144K  /usr\n"
"mypool/usr/home                        184K  93.2G   184K  /usr/home\n"
"mypool/usr/ports                       144K  93.2G   144K  /usr/ports\n"
"mypool/usr/src                         144K  93.2G   144K  /usr/src\n"
"mypool/var                            1.29M  93.2G   616K  /var\n"
"mypool/var/crash                       148K  93.2G   148K  /var/crash\n"
"mypool/var/log                         178K  93.2G   178K  /var/log\n"
"mypool/var/mail                        144K  93.2G   144K  /var/mail\n"
"mypool/var/newname                    87.5K  93.2G  87.5K  /var/newname\n"
"mypool/var/newname@new_snapshot_name      0      -  87.5K  -\n"
"mypool/var/tmp                         152K  93.2G   152K  /var/tmp\n"
"# zfs snapshot -r mypool@my_recursive_snapshot\n"
"# zfs list -t snapshot\n"
"NAME                                        USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool@my_recursive_snapshot                   0      -   144K  -\n"
"mypool/ROOT@my_recursive_snapshot              0      -   144K  -\n"
"mypool/ROOT/default@my_recursive_snapshot      0      -   777M  -\n"
"mypool/tmp@my_recursive_snapshot               0      -   176K  -\n"
"mypool/usr@my_recursive_snapshot               0      -   144K  -\n"
"mypool/usr/home@my_recursive_snapshot          0      -   184K  -\n"
"mypool/usr/ports@my_recursive_snapshot         0      -   144K  -\n"
"mypool/usr/src@my_recursive_snapshot           0      -   144K  -\n"
"mypool/var@my_recursive_snapshot               0      -   616K  -\n"
"mypool/var/crash@my_recursive_snapshot         0      -   148K  -\n"
"mypool/var/log@my_recursive_snapshot           0      -   178K  -\n"
"mypool/var/mail@my_recursive_snapshot          0      -   144K  -\n"
"mypool/var/newname@new_snapshot_name           0      -  87.5K  -\n"
"mypool/var/newname@my_recursive_snapshot       0      -  87.5K  -\n"
"mypool/var/tmp@my_recursive_snapshot           0      -   152K  -\n"
msgstr ""
"# zfs list -t all\n"
"NAME                                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool                                 780M  93.2G   144K  none\n"
"mypool/ROOT                            777M  93.2G   144K  none\n"
"mypool/ROOT/default                    777M  93.2G   777M  /\n"
"mypool/tmp                             176K  93.2G   176K  /tmp\n"
"mypool/usr                             616K  93.2G   144K  /usr\n"
"mypool/usr/home                        184K  93.2G   184K  /usr/home\n"
"mypool/usr/ports                       144K  93.2G   144K  /usr/ports\n"
"mypool/usr/src                         144K  93.2G   144K  /usr/src\n"
"mypool/var                            1.29M  93.2G   616K  /var\n"
"mypool/var/crash                       148K  93.2G   148K  /var/crash\n"
"mypool/var/log                         178K  93.2G   178K  /var/log\n"
"mypool/var/mail                        144K  93.2G   144K  /var/mail\n"
"mypool/var/newname                    87.5K  93.2G  87.5K  /var/newname\n"
"mypool/var/newname@new_snapshot_name      0      -  87.5K  -\n"
"mypool/var/tmp                         152K  93.2G   152K  /var/tmp\n"
"# zfs snapshot -r mypool@my_recursive_snapshot\n"
"# zfs list -t snapshot\n"
"NAME                                        USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool@my_recursive_snapshot                   0      -   144K  -\n"
"mypool/ROOT@my_recursive_snapshot              0      -   144K  -\n"
"mypool/ROOT/default@my_recursive_snapshot      0      -   777M  -\n"
"mypool/tmp@my_recursive_snapshot               0      -   176K  -\n"
"mypool/usr@my_recursive_snapshot               0      -   144K  -\n"
"mypool/usr/home@my_recursive_snapshot          0      -   184K  -\n"
"mypool/usr/ports@my_recursive_snapshot         0      -   144K  -\n"
"mypool/usr/src@my_recursive_snapshot           0      -   144K  -\n"
"mypool/var@my_recursive_snapshot               0      -   616K  -\n"
"mypool/var/crash@my_recursive_snapshot         0      -   148K  -\n"
"mypool/var/log@my_recursive_snapshot           0      -   178K  -\n"
"mypool/var/mail@my_recursive_snapshot          0      -   144K  -\n"
"mypool/var/newname@new_snapshot_name           0      -  87.5K  -\n"
"mypool/var/newname@my_recursive_snapshot       0      -  87.5K  -\n"
"mypool/var/tmp@my_recursive_snapshot           0      -   152K  -\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1806
msgid ""
"Snapshots are not shown by a normal `zfs list` operation.  To list "
"snapshots, append `-t snapshot` to `zfs list`.  `-t all` displays both file "
"systems and snapshots."
msgstr ""
"스냅샷은 일반적인 `zfs list` 작업으로는 표시되지 않습니다.  스냅샷을 나열하려"
"면 `-t snapshot` 을 `zfs list` 에 추가합니다.  `-t all` 는 파일 시스템과 스냅"
"샷을 모두 표시합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1810
msgid ""
"Snapshots are not mounted directly, showing no path in the `MOUNTPOINT` "
"column.  ZFS does not mention available disk space in the `AVAIL` column, as "
"snapshots are read-only after their creation.  Compare the snapshot to the "
"original dataset:"
msgstr ""
"스냅샷은 직접 마운트되지 않으므로 `MOUNTPOINT` 열에 경로가 표시되지 않습니"
"다.  스냅샷은 생성 후 읽기 전용이므로 ZFS는 `AVAIL` 열에 사용 가능한 디스크 "
"공간을 언급하지 않습니다.  스냅샷을 원본 데이터 세트와 비교합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1817
#, no-wrap
msgid ""
"# zfs list -rt all mypool/usr/home\n"
"NAME                                    USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool/usr/home                         184K  93.2G   184K  /usr/home\n"
"mypool/usr/home@my_recursive_snapshot      0      -   184K  -\n"
msgstr ""
"# zfs list -rt all mypool/usr/home\n"
"NAME                                    USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool/usr/home                         184K  93.2G   184K  /usr/home\n"
"mypool/usr/home@my_recursive_snapshot      0      -   184K  -\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1824
#, fuzzy
#| msgid ""
#| "Displaying both the dataset and the snapshot together reveals how "
#| "snapshots work in <<zfs-term-cow,COW>> fashion.  They save the changes "
#| "(_delta_) made and not the complete file system contents all over again.  "
#| "This means that snapshots take little space when making changes.  Observe "
#| "space usage even more by copying a file to the dataset, then creating a "
#| "second snapshot:"
msgid ""
"Displaying both the dataset and the snapshot together reveals how snapshots "
"work in crossref:zfs[zfs-term-cow,COW] fashion.  They save the changes "
"(_delta_) made and not the complete file system contents all over again.  "
"This means that snapshots take little space when making changes.  Observe "
"space usage even more by copying a file to the dataset, then creating a "
"second snapshot:"
msgstr ""
"데이터 세트와 스냅샷을 함께 표시하면 스냅샷이 <<zfs-term-cow,COW>> 방식으로 "
"어떻게 작동하는지 알 수 있습니다.  스냅샷은 전체 파일 시스템 내용을 다시 저장"
"하는 것이 아니라 변경된 부분( _델타_ )만 저장합니다.  즉, 스냅샷은 변경을 수"
"행할 때 공간을 거의 차지하지 않습니다.  파일을 데이터 세트에 복사한 다음 두 "
"번째 스냅샷을 생성하여 공간 사용량을 더 자세히 관찰하세요:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1834
#, no-wrap
msgid ""
"# cp /etc/passwd /var/tmp\n"
"# zfs snapshot mypool/var/tmp@after_cp\n"
"# zfs list -rt all mypool/var/tmp\n"
"NAME                                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool/var/tmp                         206K  93.2G   118K  /var/tmp\n"
"mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -\n"
"mypool/var/tmp@after_cp                   0      -   118K  -\n"
msgstr ""
"# cp /etc/passwd /var/tmp\n"
"# zfs snapshot mypool/var/tmp@after_cp\n"
"# zfs list -rt all mypool/var/tmp\n"
"NAME                                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool/var/tmp                         206K  93.2G   118K  /var/tmp\n"
"mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -\n"
"mypool/var/tmp@after_cp                   0      -   118K  -\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1839
msgid ""
"The second snapshot contains the changes to the dataset after the copy "
"operation.  This yields enormous space savings.  Notice that the size of the "
"snapshot `_mypool/var/tmp@my_recursive_snapshot_` also changed in the `USED` "
"column to show the changes between itself and the snapshot taken afterwards."
msgstr ""
"두 번째 스냅샷에는 복사 작업 후 데이터 세트의 변경 사항이 포함됩니다.  이렇"
"게 하면 공간을 크게 절약할 수 있습니다.  `USED` 열에서는 `_mypool/var/"
"tmp@my_recursive_snapshot_` 스냅샷 자체의 용량과  이후에 생성된 스냅샷 사이"
"의 변경 사항을 보여줍니다."

#. type: Title ====
#: documentation/content/en/books/handbook/zfs/_index.adoc:1841
#, no-wrap
msgid "Comparing Snapshots"
msgstr "스냅샷 비교하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1847
msgid ""
"ZFS provides a built-in command to compare the differences in content "
"between two snapshots.  This is helpful with a lot of snapshots taken over "
"time when the user wants to see how the file system has changed over time.  "
"For example, `zfs diff` lets a user find the latest snapshot that still "
"contains a file deleted by accident.  Doing this for the two snapshots "
"created in the previous section yields this output:"
msgstr ""
"ZFS는 두 스냅샷 간의 콘텐츠 차이를 비교할 수 있는 기본 명령을 제공합니다.  "
"이 명령은 사용자가 시간이 지남에 따라 파일 시스템이 어떻게 변했는지 확인하고"
"자 할 때, 또는 시간이 지남에 따라 많은 스냅샷을 생성해야할 때 유용합니다.  예"
"를 들어, `zfs diff` 를 사용하면 실수로 삭제한 파일이 여전히 포함된 최신 스냅"
"샷을 찾을 수 있습니다.  이전 섹션에서 만든 두 개의 스냅샷에 대해 이 작업을 수"
"행하면 다음과 같은 출력이 생성됩니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1858
#, no-wrap
msgid ""
"# zfs list -rt all mypool/var/tmp\n"
"NAME                                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool/var/tmp                         206K  93.2G   118K  /var/tmp\n"
"mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -\n"
"mypool/var/tmp@after_cp                   0      -   118K  -\n"
"# zfs diff mypool/var/tmp@my_recursive_snapshot\n"
"M       /var/tmp/\n"
"+       /var/tmp/passwd\n"
msgstr ""
"# zfs list -rt all mypool/var/tmp\n"
"NAME                                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool/var/tmp                         206K  93.2G   118K  /var/tmp\n"
"mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -\n"
"mypool/var/tmp@after_cp                   0      -   118K  -\n"
"# zfs diff mypool/var/tmp@my_recursive_snapshot\n"
"M       /var/tmp/\n"
"+       /var/tmp/passwd\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1862
msgid ""
"The command lists the changes between the specified snapshot (in this case "
"`_mypool/var/tmp@my_recursive_snapshot_`) and the live file system.  The "
"first column shows the change type:"
msgstr ""
"이 명령은 지정된 스냅샷(이 경우 `_mypool/var/tmp@my_recursive_snapshot_` )과 "
"라이브 파일 시스템 간의 변경 내용을 나열합니다.  첫 번째 열에는 변경 유형이 "
"표시됩니다:"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:1868
#, no-wrap
msgid "+"
msgstr "+"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:1870
#, no-wrap
msgid "Adding the path or file."
msgstr "파일이나 패스를 추가."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:1871
#, no-wrap
msgid "-"
msgstr "-"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:1873
#, no-wrap
msgid "Deleting the path or file."
msgstr "파일이나 패스를 삭제."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:1874
#, no-wrap
msgid "M"
msgstr "M"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:1876
#, no-wrap
msgid "Modifying the path or file."
msgstr "파일이나 패스를 수정."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:1877
#, no-wrap
msgid "R"
msgstr "R"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:1878
#, no-wrap
msgid "Renaming the path or file."
msgstr "파일이나 패스의 이름을 변경."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1883
msgid ""
"Comparing the output with the table, it becomes clear that ZFS added [."
"filename]#passwd# after creating the snapshot `_mypool/var/"
"tmp@my_recursive_snapshot_`.  This also resulted in a modification to the "
"parent directory mounted at `_/var/tmp_`."
msgstr ""
"출력을 표와 비교하면, ZFS가 스냅샷 `_mypool/var/tmp@my_recursive_snapshot_` "
"을 생성한 후 [.filename]#passwd# 를 추가했음을 알 수 있습니다.  이로 인해 `_/"
"var/tmp_` 에 마운트된 상위 디렉터리도 수정되었습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1885
msgid ""
"Comparing two snapshots is helpful when using the ZFS replication feature to "
"transfer a dataset to a different host for backup purposes."
msgstr ""
"두 스냅샷을 비교하는 것은 ZFS 복제 기능을 사용하여 백업 목적으로 데이터 세트"
"를 다른 호스트로 전송할 때 유용합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1887
msgid ""
"Compare two snapshots by providing the full dataset name and snapshot name "
"of both datasets:"
msgstr ""
"두 데이터 세트의 전체 데이터 세트 이름과 스냅샷 이름을 제공하여 두 스냅샷을 "
"비교합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1899
#, no-wrap
msgid ""
"# cp /var/tmp/passwd /var/tmp/passwd.copy\n"
"# zfs snapshot mypool/var/tmp@diff_snapshot\n"
"# zfs diff mypool/var/tmp@my_recursive_snapshot mypool/var/tmp@diff_snapshot\n"
"M       /var/tmp/\n"
"+       /var/tmp/passwd\n"
"+       /var/tmp/passwd.copy\n"
"# zfs diff mypool/var/tmp@my_recursive_snapshot mypool/var/tmp@after_cp\n"
"M       /var/tmp/\n"
"+       /var/tmp/passwd\n"
msgstr ""
"# cp /var/tmp/passwd /var/tmp/passwd.copy\n"
"# zfs snapshot mypool/var/tmp@diff_snapshot\n"
"# zfs diff mypool/var/tmp@my_recursive_snapshot mypool/var/tmp@diff_snapshot\n"
"M       /var/tmp/\n"
"+       /var/tmp/passwd\n"
"+       /var/tmp/passwd.copy\n"
"# zfs diff mypool/var/tmp@my_recursive_snapshot mypool/var/tmp@after_cp\n"
"M       /var/tmp/\n"
"+       /var/tmp/passwd\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1903
#, fuzzy
#| msgid ""
#| "A backup administrator can compare two snapshots received from the "
#| "sending host and determine the actual changes in the dataset.  See the "
#| "<<zfs-zfs-send,Replication>> section for more information."
msgid ""
"A backup administrator can compare two snapshots received from the sending "
"host and determine the actual changes in the dataset.  See the crossref:"
"zfs[zfs-zfs-send,Replication] section for more information."
msgstr ""
"백업 관리자는 전송 호스트에서 받은 두 개의 스냅샷을 비교하여 데이터 세트의 실"
"제 변경 사항을 확인할 수 있습니다.  자세한 내용은 <<zfs-zfs-send,"
"Replication>> 섹션을 참조하세요."

#. type: Title ====
#: documentation/content/en/books/handbook/zfs/_index.adoc:1905
#, no-wrap
msgid "Snapshot Rollback"
msgstr "스냅샷 롤백"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1918
#, fuzzy
#| msgid ""
#| "When at least one snapshot is available, roll back to it at any time.  "
#| "Most often this is the case when the current state of the dataset is no "
#| "longer and if preferring an older version.  Scenarios such as local "
#| "development tests gone wrong, botched system updates hampering the system "
#| "functionality, or the need to restore deleted files or directories are "
#| "all too common occurrences.  To roll back a snapshot, use `zfs rollback "
#| "_snapshotname_`.  If a lot of changes are present, the operation will "
#| "take a long time.  During that time, the dataset always remains in a "
#| "consistent state, much like a database that conforms to ACID principles "
#| "is performing a rollback.  This is happening while the dataset is live "
#| "and accessible without requiring a downtime.  Once the snapshot rolled "
#| "back, the dataset has the same state as it had when the snapshot was "
#| "originally taken.  Rolling back to a snapshot discards all other data in "
#| "that dataset not part of the snapshot.  Taking a snapshot of the current "
#| "state of the dataset before rolling back to a previous one is a good idea "
#| "when requiring some data later.  This way, the user can roll back and "
#| "forth between snapshots without losing data that is still valuable."
msgid ""
"When at least one snapshot is available, roll back to it at any time.  Most "
"often this is the case when the current state of the dataset is no longer "
"valid or an older version is preferred.  Scenarios such as local development "
"tests gone wrong, botched system updates hampering the system functionality, "
"or the need to restore deleted files or directories are all too common "
"occurrences.  To roll back a snapshot, use `zfs rollback _snapshotname_`.  "
"If a lot of changes are present, the operation will take a long time.  "
"During that time, the dataset always remains in a consistent state, much "
"like a database that conforms to ACID principles is performing a rollback.  "
"This is happening while the dataset is live and accessible without requiring "
"a downtime.  Once the snapshot rolled back, the dataset has the same state "
"as it had when the snapshot was originally taken.  Rolling back to a "
"snapshot discards all other data in that dataset not part of the snapshot.  "
"Taking a snapshot of the current state of the dataset before rolling back to "
"a previous one is a good idea when requiring some data later.  This way, the "
"user can roll back and forth between snapshots without losing data that is "
"still valuable."
msgstr ""
"하나 이상의 스냅샷을 사용할 수 있는 경우 언제든지 해당 스냅샷으로 롤백할 수 "
"있습니다.  데이터 세트의 현재 상태가 더 이상 유지되지 않거나 이전 버전을 선호"
"하는 경우가 대부분입니다.  로컬 개발 테스트가 잘못되었거나, 시스템 업데이트"
"가 잘못되어 시스템 기능을 방해하거나, 삭제된 파일이나 디렉터리를 복원해야 하"
"는 등의 시나리오는 너무나 흔하게 발생합니다.  스냅샷을 롤백하려면 `zfs "
"rollback _snapshotname_` 을 사용합니다.  변경 사항이 많으면 작업 시간이 오래 "
"걸립니다.  이 시간 동안 데이터 세트는 항상 일관된 상태로 유지되는데, 이는 "
"ACID 원칙을 준수하는 데이터베이스가 롤백을 수행하는 것과 매우 유사합니다.  "
"이 작업은 다운타임 없이 데이터 세트가 라이브 상태이고 액세스할 수 있는 동안"
"에 이루어집니다.  스냅샷이 롤백되면 데이터 세트는 원래 스냅샷을 만들었을 때"
"와 동일한 상태가 됩니다.  스냅샷으로 롤백하면 해당 데이터 세트의 스냅샷에 포"
"함되지 않은 다른 모든 데이터가 삭제됩니다.  나중에 일부 데이터가 필요할 때 이"
"전 데이터로 롤백하기 전에 데이터 집합의 현재 상태를 스냅샷으로 찍어두는 것이 "
"좋습니다.  이렇게 하면 사용자는 여전히 중요한 데이터를 잃지 않고 스냅샷 간에 "
"롤백할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1920
#, fuzzy
#| msgid ""
#| "In the first example, roll back a snapshot because of a careless `rm` "
#| "operation that removes too much data than intended."
msgid ""
"In the first example, roll back a snapshot because a careless `rm` operation "
"removed more data than intended."
msgstr ""
"첫 번째 예에서는 부주의한 `rm` 작업으로 인해 의도한 것보다 너무 많은 데이터"
"가 제거되어 스냅샷을 롤백하는 경우입니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1934
#, no-wrap
msgid ""
"# zfs list -rt all mypool/var/tmp\n"
"NAME                                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool/var/tmp                         262K  93.2G   120K  /var/tmp\n"
"mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -\n"
"mypool/var/tmp@after_cp               53.5K      -   118K  -\n"
"mypool/var/tmp@diff_snapshot              0      -   120K  -\n"
"# ls /var/tmp\n"
"passwd          passwd.copy     vi.recover\n"
"# rm /var/tmp/passwd*\n"
"# ls /var/tmp\n"
"vi.recover\n"
msgstr ""
"# zfs list -rt all mypool/var/tmp\n"
"NAME                                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool/var/tmp                         262K  93.2G   120K  /var/tmp\n"
"mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -\n"
"mypool/var/tmp@after_cp               53.5K      -   118K  -\n"
"mypool/var/tmp@diff_snapshot              0      -   120K  -\n"
"# ls /var/tmp\n"
"passwd          passwd.copy     vi.recover\n"
"# rm /var/tmp/passwd*\n"
"# ls /var/tmp\n"
"vi.recover\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1939
msgid ""
"At this point, the user notices the removal of extra files and wants them "
"back.  ZFS provides an easy way to get them back using rollbacks, when "
"performing snapshots of important data on a regular basis.  To get the files "
"back and start over from the last snapshot, issue the command:"
msgstr ""
"이 시점에서 사용자는 여분의 파일이 제거된 것을 알아차리고 이를 되돌리고 싶어"
"합니다.  ZFS는 정기적으로 중요한 데이터의 스냅샷을 수행할 때 롤백을 사용하여 "
"파일을 쉽게 되돌릴 수 있는 방법을 제공합니다.  파일을 다시 가져오고 마지막 스"
"냅샷부터 다시 시작하려면 다음 명령을 실행하세요:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1945
#, no-wrap
msgid ""
"# zfs rollback mypool/var/tmp@diff_snapshot\n"
"# ls /var/tmp\n"
"passwd          passwd.copy     vi.recover\n"
msgstr ""
"# zfs rollback mypool/var/tmp@diff_snapshot\n"
"# ls /var/tmp\n"
"passwd          passwd.copy     vi.recover\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1950
msgid ""
"The rollback operation restored the dataset to the state of the last "
"snapshot.  Rolling back to a snapshot taken much earlier with other "
"snapshots taken afterwards is also possible.  When trying to do this, ZFS "
"will issue this warning:"
msgstr ""
"롤백 작업으로 데이터 세트가 마지막 스냅샷의 상태로 복원되었습니다.  이후에 생"
"성된 다른 스냅샷을 사용하여 훨씬 이전에 생성된 스냅샷으로 롤백할 수도 있습니"
"다.  이 작업을 시도할 때 ZFS는 이 경고를 표시합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1963
#, no-wrap
msgid ""
"# zfs list -rt snapshot mypool/var/tmp\n"
"AME                                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -\n"
"mypool/var/tmp@after_cp               53.5K      -   118K  -\n"
"mypool/var/tmp@diff_snapshot              0      -   120K  -\n"
"# zfs rollback mypool/var/tmp@my_recursive_snapshot\n"
"cannot rollback to 'mypool/var/tmp@my_recursive_snapshot': more recent snapshots exist\n"
"use '-r' to force deletion of the following snapshots:\n"
"mypool/var/tmp@after_cp\n"
"mypool/var/tmp@diff_snapshot\n"
msgstr ""
"# zfs list -rt snapshot mypool/var/tmp\n"
"AME                                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -\n"
"mypool/var/tmp@after_cp               53.5K      -   118K  -\n"
"mypool/var/tmp@diff_snapshot              0      -   120K  -\n"
"# zfs rollback mypool/var/tmp@my_recursive_snapshot\n"
"cannot rollback to 'mypool/var/tmp@my_recursive_snapshot': more recent snapshots exist\n"
"use '-r' to force deletion of the following snapshots:\n"
"mypool/var/tmp@after_cp\n"
"mypool/var/tmp@diff_snapshot\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1970
msgid ""
"This warning means that snapshots exist between the current state of the "
"dataset and the snapshot to which the user wants to roll back.  To complete "
"the rollback delete these snapshots.  ZFS cannot track all the changes "
"between different states of the dataset, because snapshots are read-only.  "
"ZFS will not delete the affected snapshots unless the user specifies `-r` to "
"confirm that this is the desired action.  If that is the intention, and "
"understanding the consequences of losing all intermediate snapshots, issue "
"the command:"
msgstr ""
"이 경고는 데이터 집합의 현재 상태와 사용자가 롤백하려는 스냅샷 사이에 스냅샷"
"이 존재한다는 의미입니다.  롤백을 완료하려면 이러한 스냅샷을 삭제합니다.  스"
"냅샷은 읽기 전용이므로 ZFS는 데이터 세트의 여러 상태 간의 모든 변경 사항을 추"
"적할 수 없습니다.  사용자가 `-r` 을 지정하여 원하는 작업임을 확인하지 않는 "
"한 ZFS는 영향을 받는 스냅샷을 삭제하지 않습니다.  이러한 의도가 있고 모든 중"
"간 스냅샷이 손실될 경우의 결과를 이해했다면 명령을 실행하세요:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:1979
#, no-wrap
msgid ""
"# zfs rollback -r mypool/var/tmp@my_recursive_snapshot\n"
"# zfs list -rt snapshot mypool/var/tmp\n"
"NAME                                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool/var/tmp@my_recursive_snapshot     8K      -   152K  -\n"
"# ls /var/tmp\n"
"vi.recover\n"
msgstr ""
"# zfs rollback -r mypool/var/tmp@my_recursive_snapshot\n"
"# zfs list -rt snapshot mypool/var/tmp\n"
"NAME                                   USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool/var/tmp@my_recursive_snapshot     8K      -   152K  -\n"
"# ls /var/tmp\n"
"vi.recover\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1982
msgid ""
"The output from `zfs list -t snapshot` confirms the removal of the "
"intermediate snapshots as a result of `zfs rollback -r`."
msgstr ""
"`zfs list -t snapshot` 의 출력은 `zfs rollback -r` 의 결과로 중간 스냅샷이 제"
"거된 것을 보여줍니다."

#. type: Title ====
#: documentation/content/en/books/handbook/zfs/_index.adoc:1984
#, no-wrap
msgid "Restoring Individual Files from Snapshots"
msgstr "스냅샷에서 개별 파일 복원하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:1991
msgid ""
"Snapshots live in a hidden directory under the parent dataset: [.filename]#."
"zfs/snapshots/snapshotname#.  By default, these directories will not show "
"even when executing a standard `ls -a` .  Although the directory doesn't "
"show, access it like any normal directory.  The property named `snapdir` "
"controls whether these hidden directories show up in a directory listing.  "
"Setting the property to `visible` allows them to appear in the output of "
"`ls` and other commands that deal with directory contents."
msgstr ""
"스냅샷은 상위 데이터 세트 아래의 숨겨진 디렉터리에 있습니다: [.filename]#."
"zfs/snapshots/snapshotname#.  기본적으로 이 디렉터리는 표준 `ls -a` 를 실행해"
"도 표시되지 않습니다.  디렉터리가 표시되지 않더라도 일반 디렉터리처럼 액세스"
"할 수 있습니다.  `snapdir` 라는 속성은 이러한 숨겨진 디렉터리를 디렉토리 목록"
"에 표시할지 여부를 제어합니다.  이 속성을 `visible` 로 설정하면 `ls` 및 디렉"
"토리 내용을 다루는 다른 명령의 출력에 표시할 수 있습니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2002
#, no-wrap
msgid ""
"# zfs get snapdir mypool/var/tmp\n"
"NAME            PROPERTY  VALUE    SOURCE\n"
"mypool/var/tmp  snapdir   hidden   default\n"
"# ls -a /var/tmp\n"
".               ..              passwd          vi.recover\n"
"# zfs set snapdir=visible mypool/var/tmp\n"
"# ls -a /var/tmp\n"
".               ..              .zfs            passwd          vi.recover\n"
msgstr ""
"# zfs get snapdir mypool/var/tmp\n"
"NAME            PROPERTY  VALUE    SOURCE\n"
"mypool/var/tmp  snapdir   hidden   default\n"
"# ls -a /var/tmp\n"
".               ..              passwd          vi.recover\n"
"# zfs set snapdir=visible mypool/var/tmp\n"
"# ls -a /var/tmp\n"
".               ..              .zfs            passwd          vi.recover\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2007
msgid ""
"Restore individual files to a previous state by copying them from the "
"snapshot back to the parent dataset.  The directory structure below [."
"filename]#.zfs/snapshot# has a directory named like the snapshots taken "
"earlier to make it easier to identify them.  The next example shows how to "
"restore a file from the hidden [.filename]#.zfs# directory by copying it "
"from the snapshot containing the latest version of the file:"
msgstr ""
"개별 파일을 스냅샷에서 상위 데이터 세트로 다시 복사하여 이전 상태로 복원합니"
"다.  [.filename]#.zfs/snapshot# 아래의 디렉토리 구조에는 앞서 만든 스냅샷과 "
"같은 이름의 디렉터리가 있어 쉽게 식별할 수 있습니다.  다음 예제에서는 파일의 "
"최신 버전이 포함된 스냅샷에서 파일을 복사하여 숨겨진 [.filename]#.zfs# 디렉터"
"리에서 파일을 복원하는 방법을 보여 줍니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2018
#, no-wrap
msgid ""
"# rm /var/tmp/passwd\n"
"# ls -a /var/tmp\n"
".               ..              .zfs            vi.recover\n"
"# ls /var/tmp/.zfs/snapshot\n"
"after_cp                my_recursive_snapshot\n"
"# ls /var/tmp/.zfs/snapshot/after_cp\n"
"passwd          vi.recover\n"
"# cp /var/tmp/.zfs/snapshot/after_cp/passwd /var/tmp\n"
msgstr ""
"# rm /var/tmp/passwd\n"
"# ls -a /var/tmp\n"
".               ..              .zfs            vi.recover\n"
"# ls /var/tmp/.zfs/snapshot\n"
"after_cp                my_recursive_snapshot\n"
"# ls /var/tmp/.zfs/snapshot/after_cp\n"
"passwd          vi.recover\n"
"# cp /var/tmp/.zfs/snapshot/after_cp/passwd /var/tmp\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2025
msgid ""
"Even if the `snapdir` property is set to hidden, running `ls .zfs/snapshot` "
"will still list the contents of that directory.  The administrator decides "
"whether to display these directories.  This is a per-dataset setting.  "
"Copying files or directories from this hidden [.filename]#.zfs/snapshot# is "
"simple enough.  Trying it the other way around results in this error:"
msgstr ""
"`snapdir` 속성이 숨김으로 설정되어 있어도 `ls .zfs/snapshot` 을 실행하면 해"
"당 디렉터리의 내용이 나열됩니다.  관리자는 이러한 디렉터리를 표시할지 여부를 "
"결정합니다.  이것은 데이터 세트별 설정입니다.  이 숨겨진 [.filename]#.zfs/"
"snapshot# 에서 파일 또는 디렉터리를 복사하는 것은 매우 간단합니다.  다른 방법"
"으로 시도하면 이 오류가 발생합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2030
#, no-wrap
msgid ""
"# cp /etc/rc.conf /var/tmp/.zfs/snapshot/after_cp/\n"
"cp: /var/tmp/.zfs/snapshot/after_cp/rc.conf: Read-only file system\n"
msgstr ""
"# cp /etc/rc.conf /var/tmp/.zfs/snapshot/after_cp/\n"
"cp: /var/tmp/.zfs/snapshot/after_cp/rc.conf: Read-only file system\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2034
msgid ""
"The error reminds the user that snapshots are read-only and cannot change "
"after creation.  Copying files into and removing them from snapshot "
"directories are both disallowed because that would change the state of the "
"dataset they represent."
msgstr ""
"이 오류는 사용자에게 스냅샷이 읽기 전용이며 생성 후에는 변경할 수 없음을 알려"
"줍니다.  스냅샷 디렉터리로 파일을 복사하거나 스냅샷 디렉터리에서 파일을 제거"
"하는 것은 모두 허용되지 않으며, 이는 파일이 나타내는 데이터 세트의 상태를 변"
"경할 수 있기 때문입니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2037
msgid ""
"Snapshots consume space based on how much the parent file system has changed "
"since the time of the snapshot.  The `written` property of a snapshot tracks "
"the space the snapshot uses."
msgstr ""
"스냅샷은 스냅샷 시점 이후 상위 파일 시스템이 얼마나 많이 변경되었는지에 따라 "
"공간을 소비합니다.  스냅샷의 `written` 속성은 스냅샷이 사용하는 공간을 추적합"
"니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2041
msgid ""
"To destroy snapshots and reclaim the space, use `zfs destroy "
"_dataset_@_snapshot_`.  Adding `-r` recursively removes all snapshots with "
"the same name under the parent dataset.  Adding `-n -v` to the command "
"displays a list of the snapshots to be deleted and an estimate of the space "
"it would reclaim without performing the actual destroy operation."
msgstr ""
"스냅샷을 삭제하고 공간을 확보하려면 `zfs destroy _dataset_@_snapshot_` 을 사"
"용합니다.  `-r` 을 추가하면 상위 데이터세트에서 이름이 같은 모든 스냅샷이 재"
"귀적으로 제거됩니다.  명령에 `-n -v` 를 추가하면 삭제할 스냅샷 목록과 실제 삭"
"제 작업을 수행하지 않고도 회수할 수 있는 공간의 예상치를 표시합니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:2043
#, no-wrap
msgid "Managing Clones"
msgstr "클론 관리하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2052
msgid ""
"A clone is a copy of a snapshot treated more like a regular dataset.  Unlike "
"a snapshot, a clone is writeable and mountable, and has its own properties.  "
"After creating a clone using `zfs clone`, destroying the originating "
"snapshot is impossible.  To reverse the child/parent relationship between "
"the clone and the snapshot use `zfs promote`.  Promoting a clone makes the "
"snapshot become a child of the clone, rather than of the original parent "
"dataset.  This will change how ZFS accounts for the space, but not actually "
"change the amount of space consumed.  Mounting the clone anywhere within the "
"ZFS file system hierarchy is possible, not only below the original location "
"of the snapshot."
msgstr ""
"클론은 일반 데이터 세트처럼 취급되는 스냅샷의 복사본입니다.  스냅샷과 달리 복"
"제본은 쓰기 및 마운트가 가능하며 고유한 속성을 가집니다.  `zfs clone` 을 사용"
"하여 클론을 생성한 후에는 원본 스냅샷을 삭제할 수 없습니다.  복제본과 스냅샷 "
"간의 자식/부모 관계를 되돌리려면 `zfs promote` 를 사용합니다.  복제본을 승격"
"하면 스냅샷이 원래 부모 데이터 세트가 아닌 복제본의 자식이 됩니다.  이렇게 하"
"면 ZFS가 공간을 차지하는 방식이 변경되지만 실제로 소비되는 공간의 양은 변경되"
"지 않습니다.  스냅샷의 원래 위치 아래뿐만 아니라 ZFS 파일 시스템 계층 구조 "
"내 어디든 클론을 마운트할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2054
msgid "To show the clone feature use this example dataset:"
msgstr "복제 기능을 표시하려면 이 예제 데이터 세트를 사용합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2062
#, no-wrap
msgid ""
"# zfs list -rt all camino/home/joe\n"
"NAME                    USED  AVAIL  REFER  MOUNTPOINT\n"
"camino/home/joe         108K   1.3G    87K  /usr/home/joe\n"
"camino/home/joe@plans    21K      -  85.5K  -\n"
"camino/home/joe@backup    0K      -    87K  -\n"
msgstr ""
"# zfs list -rt all camino/home/joe\n"
"NAME                    USED  AVAIL  REFER  MOUNTPOINT\n"
"camino/home/joe         108K   1.3G    87K  /usr/home/joe\n"
"camino/home/joe@plans    21K      -  85.5K  -\n"
"camino/home/joe@backup    0K      -    87K  -\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2068
msgid ""
"A typical use for clones is to experiment with a specific dataset while "
"keeping the snapshot around to fall back to in case something goes wrong.  "
"Since snapshots cannot change, create a read/write clone of a snapshot.  "
"After achieving the desired result in the clone, promote the clone to a "
"dataset and remove the old file system.  Removing the parent dataset is not "
"strictly necessary, as the clone and dataset can coexist without problems."
msgstr ""
"클론의 일반적인 용도는 특정 데이터 세트를 실험하면서 문제가 발생할 경우 백업"
"할 수 있도록 스냅샷을 보관하는 것입니다.  스냅샷은 변경할 수 없으므로 스냅샷"
"의 읽기/쓰기 복제본을 만듭니다.  복제본에서 원하는 결과를 얻은 후 복제본을 데"
"이터 세트로 승격하고 이전 파일 시스템을 제거합니다.  복제본과 데이터 세트가 "
"문제 없이 공존할 수 있으므로 상위 데이터 세트를 반드시 제거할 필요는 없습니"
"다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2075
#, no-wrap
msgid ""
"# zfs clone camino/home/joe@backup camino/home/joenew\n"
"# ls /usr/home/joe*\n"
"/usr/home/joe:\n"
"backup.txz     plans.txt\n"
msgstr ""
"# zfs clone camino/home/joe@backup camino/home/joenew\n"
"# ls /usr/home/joe*\n"
"/usr/home/joe:\n"
"backup.txz     plans.txt\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2082
#, no-wrap
msgid ""
"/usr/home/joenew:\n"
"backup.txz     plans.txt\n"
"# df -h /usr/home\n"
"Filesystem          Size    Used   Avail Capacity  Mounted on\n"
"usr/home/joe        1.3G     31k    1.3G     0%    /usr/home/joe\n"
"usr/home/joenew     1.3G     31k    1.3G     0%    /usr/home/joenew\n"
msgstr ""
"/usr/home/joenew:\n"
"backup.txz     plans.txt\n"
"# df -h /usr/home\n"
"Filesystem          Size    Used   Avail Capacity  Mounted on\n"
"usr/home/joe        1.3G     31k    1.3G     0%    /usr/home/joe\n"
"usr/home/joenew     1.3G     31k    1.3G     0%    /usr/home/joenew\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2091
msgid ""
"Creating a clone makes it an exact copy of the state the dataset was in when "
"taking the snapshot.  Changing the clone independently from its originating "
"dataset is possible now.  The connection between the two is the snapshot.  "
"ZFS records this connection in the property `origin`.  Promoting the clone "
"with `zfs promote` makes the clone an independent dataset.  This removes the "
"value of the `origin` property and disconnects the newly independent dataset "
"from the snapshot.  This example shows it:"
msgstr ""
"복제본을 만들면 스냅샷을 찍을 때 데이터 세트의 상태와 똑같은 복사본이 만들어"
"집니다.   이제 복제본을 원본 데이터세트와 독립적으로 변경할 수 있습니다.   "
"둘 사이의 연결이 바로 스냅샷입니다.  ZFS는 이 연결을 `origin` 속성에 기록합니"
"다.   `zfs promote` 로 복제본을 승격하면 복제본이 독립적인 데이터 세트가 됩니"
"다.  이렇게 하면 `origin` 속성의 값이 제거되고 스냅샷에서 새로 독립된 데이터 "
"세트의 연결이 끊어집니다.  이 예제는 이를 보여줍니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2101
#, no-wrap
msgid ""
"# zfs get origin camino/home/joenew\n"
"NAME                  PROPERTY  VALUE                     SOURCE\n"
"camino/home/joenew    origin    camino/home/joe@backup    -\n"
"# zfs promote camino/home/joenew\n"
"# zfs get origin camino/home/joenew\n"
"NAME                  PROPERTY  VALUE   SOURCE\n"
"camino/home/joenew    origin    -       -\n"
msgstr ""
"# zfs get origin camino/home/joenew\n"
"NAME                  PROPERTY  VALUE                     SOURCE\n"
"camino/home/joenew    origin    camino/home/joe@backup    -\n"
"# zfs promote camino/home/joenew\n"
"# zfs get origin camino/home/joenew\n"
"NAME                  PROPERTY  VALUE   SOURCE\n"
"camino/home/joenew    origin    -       -\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2106
msgid ""
"After making some changes like copying [.filename]#loader.conf# to the "
"promoted clone, for example, the old directory becomes obsolete in this "
"case.  Instead, the promoted clone can replace it.  To do this, `zfs "
"destroy` the old dataset first and then `zfs rename` the clone to the old "
"dataset name (or to an entirely different name)."
msgstr ""
"예를 들어 승격된 복제본에 [.filename]#loader.conf# 를 복사하는 등의 일부 변경"
"을 수행하면 이 경우 이전 디렉터리는 더 이상 사용되지 않게 됩니다.  대신 승격"
"된 클론이 이를 대체할 수 있습니다.  이렇게 하려면 먼저 이전 데이터세트를 "
"`zfs destroy` 한 다음, 복제본을 이전 데이터세트 이름(또는 완전히 다른 이름으"
"로)으로 `zfs rename` 를 수행합니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2117
#, no-wrap
msgid ""
"# cp /boot/defaults/loader.conf /usr/home/joenew\n"
"# zfs destroy -f camino/home/joe\n"
"# zfs rename camino/home/joenew camino/home/joe\n"
"# ls /usr/home/joe\n"
"backup.txz     loader.conf     plans.txt\n"
"# df -h /usr/home\n"
"Filesystem          Size    Used   Avail Capacity  Mounted on\n"
"usr/home/joe        1.3G    128k    1.3G     0%    /usr/home/joe\n"
msgstr ""
"# cp /boot/defaults/loader.conf /usr/home/joenew\n"
"# zfs destroy -f camino/home/joe\n"
"# zfs rename camino/home/joenew camino/home/joe\n"
"# ls /usr/home/joe\n"
"backup.txz     loader.conf     plans.txt\n"
"# df -h /usr/home\n"
"Filesystem          Size    Used   Avail Capacity  Mounted on\n"
"usr/home/joe        1.3G    128k    1.3G     0%    /usr/home/joe\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2126
msgid ""
"The cloned snapshot is now an ordinary dataset.  It contains all the data "
"from the original snapshot plus the files added to it like [."
"filename]#loader.conf#.  Clones provide useful features to ZFS users in "
"different scenarios.  For example, provide jails as snapshots containing "
"different sets of installed applications.  Users can clone these snapshots "
"and add their own applications as they see fit.  Once satisfied with the "
"changes, promote the clones to full datasets and provide them to end users "
"to work with like they would with a real dataset.  This saves time and "
"administrative overhead when providing these jails."
msgstr ""
"복제된 스냅샷은 이제 일반 데이터 세트입니다.  여기에는 원본 스냅샷의 모든 데"
"이터와 [.filename]#loader.conf# 와 같이 추가한 파일이 포함됩니다.  클론은 다"
"양한 시나리오에서 ZFS 사용자에게 유용한 기능을 제공합니다.  예를 들어, 설치"
"된 애플리케이션의 다른 세트를 포함하는 스냅샷으로 jail을 제공할 수 있습니"
"다.  사용자는 이러한 스냅샷을 복제하고 원하는 대로 애플리케이션을 추가할 수 "
"있습니다.  변경 사항이 만족스러우면 복제본을 전체 데이터 세트로 승격하여 최"
"종 사용자가 실제 데이터 세트에서 작업하는 것처럼 사용할 수 있도록 제공하세"
"요.  이렇게 하면 저장소를 제공할 때 시간과 관리 오버헤드를 절약할 수 있습니"
"다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:2128
#, no-wrap
msgid "Replication"
msgstr "복제 (Replication)"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2137
#, fuzzy
#| msgid ""
#| "Keeping data on a single pool in one location exposes it to risks like "
#| "theft and natural or human disasters.  Making regular backups of the "
#| "entire pool is vital.  ZFS provides a built-in serialization feature that "
#| "can send a stream representation of the data to standard output.  Using "
#| "this feature, storing this data on another pool connected to the local "
#| "system is possible, as is sending it over a network to another system.  "
#| "Snapshots are the basis for this replication (see the section on <<zfs-"
#| "zfs-snapshot,ZFS snapshots>>).  The commands used for replicating data "
#| "are `zfs send` and `zfs receive`."
msgid ""
"Keeping data on a single pool in one location exposes it to risks like theft "
"and natural or human disasters.  Making regular backups of the entire pool "
"is vital.  ZFS provides a built-in serialization feature that can send a "
"stream representation of the data to standard output.  Using this feature, "
"storing this data on another pool connected to the local system is possible, "
"as is sending it over a network to another system.  Snapshots are the basis "
"for this replication (see the section on crossref:zfs[zfs-zfs-snapshot,ZFS "
"snapshots]).  The commands used for replicating data are `zfs send` and `zfs "
"receive`."
msgstr ""
"한 장소에서 하나의 풀에 데이터를 한 곳에 보관하면 도난, 자연재해 또는 인적 재"
"해와 같은 위험에 노출될 수 있습니다.  따라서 전체 풀을 정기적으로 백업하는 것"
"이 중요합니다.  ZFS는 데이터의 스트림 표현을 표준 출력으로 전송할 수 있는 내"
"장 직렬화 기능을 제공합니다.  이 기능을 사용하면 로컬 시스템에 연결된 다른 풀"
"에 데이터를 저장할 수 있으며, 네트워크를 통해 다른 시스템으로 데이터를 전송"
"할 수도 있습니다.  스냅샷은 이 복제의 기본입니다( <<zfs-zfs-snapshot,ZFS "
"snapshots>> 섹션 참조 ).  데이터 복제에 사용되는 명령은 `zfs send` 와 `zfs "
"receive` 입니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2139
msgid "These examples show ZFS replication with these two pools:"
msgstr "이 예는 이 두 풀을 사용한 ZFS 복제를 보여줍니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2146
#, no-wrap
msgid ""
"# zpool list\n"
"NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG   CAP  DEDUP  HEALTH  ALTROOT\n"
"backup  960M    77K   896M         -         -     0%    0%  1.00x  ONLINE  -\n"
"mypool  984M  43.7M   940M         -         -     0%    4%  1.00x  ONLINE  -\n"
msgstr ""
"# zpool list\n"
"NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG   CAP  DEDUP  HEALTH  ALTROOT\n"
"backup  960M    77K   896M         -         -     0%    0%  1.00x  ONLINE  -\n"
"mypool  984M  43.7M   940M         -         -     0%    4%  1.00x  ONLINE  -\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2154
msgid ""
"The pool named _mypool_ is the primary pool where writing and reading data "
"happens on a regular basis.  Using a second standby pool _backup_ in case "
"the primary pool becomes unavailable.  Note that this fail-over is not done "
"automatically by ZFS, but must be manually done by a system administrator "
"when needed.  Use a snapshot to provide a consistent file system version to "
"replicate.  After creating a snapshot of _mypool_, copy it to the _backup_ "
"pool by replicating snapshots.  This does not include changes made since the "
"most recent snapshot."
msgstr ""
"_mypool_ 이라는 이름의 풀은 정기적으로 데이터 쓰기와 읽기가 이루어지는 기본 "
"풀입니다.  기본 풀을 사용할 수 없게 될 경우를 대비해 두 번째 대기 풀 "
"_backup_ 을 사용합니다.  이 페일오버는 ZFS에 의해 자동으로 수행되지 않으며, "
"필요한 경우 시스템 관리자가 수동으로 수행해야 한다는 점에 유의하세요.  스냅샷"
"을 사용하여 복제할 일관된 파일 시스템 버전을 제공하세요.  _mypool_ 의 스냅샷"
"을 생성한 후 스냅샷 복제를 통해 _backup_ 풀에 복사합니다.  여기에는 가장 최"
"근 스냅샷 이후 변경된 내용은 포함되지 않습니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2161
#, no-wrap
msgid ""
"# zfs snapshot mypool@backup1\n"
"# zfs list -t snapshot\n"
"NAME                    USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool@backup1             0      -  43.6M  -\n"
msgstr ""
"# zfs snapshot mypool@backup1\n"
"# zfs list -t snapshot\n"
"NAME                    USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool@backup1             0      -  43.6M  -\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2166
msgid ""
"Now that a snapshot exists, use `zfs send` to create a stream representing "
"the contents of the snapshot.  Store this stream as a file or receive it on "
"another pool.  Write the stream to standard output, but redirect to a file "
"or pipe or an error appears:"
msgstr ""
"이제 스냅샷이 존재하므로 `zfs send` 를 사용하여 스냅샷의 내용을 출력하는 스트"
"림을 생성합니다.  이 스트림을 파일로 저장하거나 다른 풀에서 수신합니다.  스트"
"림을 표준 출력에 쓰되, 파일이나 파이프로 리디렉션하지 않으면 오류가 발생합니"
"다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2172
#, no-wrap
msgid ""
"# zfs send mypool@backup1\n"
"Error: Stream can not be written to a terminal.\n"
"You must redirect standard output.\n"
msgstr ""
"# zfs send mypool@backup1\n"
"Error: Stream can not be written to a terminal.\n"
"You must redirect standard output.\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2176
msgid ""
"To back up a dataset with `zfs send`, redirect to a file located on the "
"mounted backup pool.  Ensure that the pool has enough free space to "
"accommodate the size of the sent snapshot, which means the data contained in "
"the snapshot, not the changes from the previous snapshot."
msgstr ""
"`zfs send` 로 데이터 세트를 백업하려면 마운트된 백업 풀에 있는 파일로 리디렉"
"션합니다.  풀에 전송된 스냅샷의 크기, 즉 이전 스냅샷의 변경 사항이 아닌 스냅"
"샷에 포함된 데이터를 수용할 수 있는 충분한 여유 공간이 있는지 확인합니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2184
#, no-wrap
msgid ""
"# zfs send mypool@backup1 > /backup/backup1\n"
"# zpool list\n"
"NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT\n"
"backup  960M  63.7M   896M         -         -     0%     6%  1.00x  ONLINE  -\n"
"mypool  984M  43.7M   940M         -         -     0%     4%  1.00x  ONLINE  -\n"
msgstr ""
"# zfs send mypool@backup1 > /backup/backup1\n"
"# zpool list\n"
"NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT\n"
"backup  960M  63.7M   896M         -         -     0%     6%  1.00x  ONLINE  -\n"
"mypool  984M  43.7M   940M         -         -     0%     4%  1.00x  ONLINE  -\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2188
msgid ""
"The `zfs send` transferred all the data in the snapshot called _backup1_ to "
"the pool named _backup_.  To create and send these snapshots automatically, "
"use a man:cron[8] job."
msgstr ""
"`zfs send` 는 _backup1_ 이라는 스냅샷의 모든 데이터를 _backup_ 이라는 풀로 전"
"송했습니다.  이러한 스냅샷을 자동으로 생성하고 전송하려면 man:cron[8] 작업을 "
"사용합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2194
msgid ""
"Instead of storing the backups as archive files, ZFS can receive them as a "
"live file system, allowing direct access to the backed up data.  To get to "
"the actual data contained in those streams, use `zfs receive` to transform "
"the streams back into files and directories.  The example below combines "
"`zfs send` and `zfs receive` using a pipe to copy the data from one pool to "
"another.  Use the data directly on the receiving pool after the transfer is "
"complete.  It is only possible to replicate a dataset to an empty dataset."
msgstr ""
"ZFS는 백업을 아카이브 파일로 저장하는 대신 라이브 파일 시스템으로 수신하여 백"
"업된 데이터에 직접 액세스할 수 있습니다.  이러한 스트림에 포함된 실제 데이터"
"에 액세스하려면 `zfs receive` 를 사용하여 스트림을 파일 및 디렉터리로 다시 변"
"환합니다.  아래 예는 파이프를 사용하여 `zfs send` 와 `zfs receive` 를 결합하"
"여 한 풀에서 다른 풀로 데이터를 복사하는 예제입니다.  전송이 완료된 후 수신 "
"풀에서 직접 데이터를 사용합니다.  데이터 세트는 빈 데이터 세트로만 복제할 수 "
"있습니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2202
#, no-wrap
msgid ""
"# zfs snapshot mypool@replica1\n"
"# zfs send -v mypool@replica1 | zfs receive backup/mypool\n"
"send from @ to mypool@replica1 estimated size is 50.1M\n"
"total estimated size is 50.1M\n"
"TIME        SENT   SNAPSHOT\n"
msgstr ""
"# zfs snapshot mypool@replica1\n"
"# zfs send -v mypool@replica1 | zfs receive backup/mypool\n"
"send from @ to mypool@replica1 estimated size is 50.1M\n"
"total estimated size is 50.1M\n"
"TIME        SENT   SNAPSHOT\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2207
#, no-wrap
msgid ""
"# zpool list\n"
"NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT\n"
"backup  960M  63.7M   896M         -         -     0%     6%  1.00x  ONLINE  -\n"
"mypool  984M  43.7M   940M         -         -     0%     4%  1.00x  ONLINE  -\n"
msgstr ""
"# zpool list\n"
"NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT\n"
"backup  960M  63.7M   896M         -         -     0%     6%  1.00x  ONLINE  -\n"
"mypool  984M  43.7M   940M         -         -     0%     4%  1.00x  ONLINE  -\n"

#. type: Title ====
#: documentation/content/en/books/handbook/zfs/_index.adoc:2210
#, no-wrap
msgid "Incremental Backups"
msgstr "증분 백업"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2215
msgid ""
"`zfs send` can also determine the difference between two snapshots and send "
"individual differences between the two.  This saves disk space and transfer "
"time.  For example:"
msgstr ""
"`zfs send` 는 두 스냅샷 간의 차이를 확인하고 두 스냅샷 간의 개별 차이를 전송"
"할 수도 있습니다.  이렇게 하면 디스크 공간과 전송 시간을 절약할 수 있습니"
"다.  예를 들어:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2227
#, no-wrap
msgid ""
"# zfs snapshot mypool@replica2\n"
"# zfs list -t snapshot\n"
"NAME                    USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool@replica1         5.72M      -  43.6M  -\n"
"mypool@replica2             0      -  44.1M  -\n"
"# zpool list\n"
"NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG   CAP  DEDUP  HEALTH  ALTROOT\n"
"backup  960M  61.7M   898M         -         -     0%    6%  1.00x  ONLINE  -\n"
"mypool  960M  50.2M   910M         -         -     0%    5%  1.00x  ONLINE  -\n"
msgstr ""
"# zfs snapshot mypool@replica2\n"
"# zfs list -t snapshot\n"
"NAME                    USED  AVAIL  REFER  MOUNTPOINT\n"
"mypool@replica1         5.72M      -  43.6M  -\n"
"mypool@replica2             0      -  44.1M  -\n"
"# zpool list\n"
"NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG   CAP  DEDUP  HEALTH  ALTROOT\n"
"backup  960M  61.7M   898M         -         -     0%    6%  1.00x  ONLINE  -\n"
"mypool  960M  50.2M   910M         -         -     0%    5%  1.00x  ONLINE  -\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2233
msgid ""
"Create a second snapshot called _replica2_.  This second snapshot contains "
"changes made to the file system between now and the previous snapshot, "
"_replica1_.  Using `zfs send -i` and indicating the pair of snapshots "
"generates an incremental replica stream containing the changed data.  This "
"succeeds if the initial snapshot already exists on the receiving side."
msgstr ""
"_replica2_ 라는 두 번째 스냅샷을 만듭니다.  이 두 번째 스냅샷에는 현재와 이"
"전 스냅샷인 _replica1_ 사이에 파일 시스템에 대한 변경 사항이 포함됩니다.  "
"`zfs send -i` 를 사용하여 스냅샷 쌍을 지정하면 변경된 데이터가 포함된 증분 복"
"제 스트림이 생성됩니다.  초기 스냅샷이 수신 측에 이미 존재하는 경우 이 방법"
"은 성공합니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2240
#, no-wrap
msgid ""
"# zfs send -v -i mypool@replica1 mypool@replica2 | zfs receive /backup/mypool\n"
"send from @replica1 to mypool@replica2 estimated size is 5.02M\n"
"total estimated size is 5.02M\n"
"TIME        SENT   SNAPSHOT\n"
msgstr ""
"# zfs send -v -i mypool@replica1 mypool@replica2 | zfs receive /backup/mypool\n"
"send from @replica1 to mypool@replica2 estimated size is 5.02M\n"
"total estimated size is 5.02M\n"
"TIME        SENT   SNAPSHOT\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2245
#, no-wrap
msgid ""
"# zpool list\n"
"NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG  CAP  DEDUP  HEALTH  ALTROOT\n"
"backup  960M  80.8M   879M         -         -     0%   8%  1.00x  ONLINE  -\n"
"mypool  960M  50.2M   910M         -         -     0%   5%  1.00x  ONLINE  -\n"
msgstr ""
"# zpool list\n"
"NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG  CAP  DEDUP  HEALTH  ALTROOT\n"
"backup  960M  80.8M   879M         -         -     0%   8%  1.00x  ONLINE  -\n"
"mypool  960M  50.2M   910M         -         -     0%   5%  1.00x  ONLINE  -\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2251
#, no-wrap
msgid ""
"# zfs list\n"
"NAME                         USED  AVAIL  REFER  MOUNTPOINT\n"
"backup                      55.4M   240G   152K  /backup\n"
"backup/mypool               55.3M   240G  55.2M  /backup/mypool\n"
"mypool                      55.6M  11.6G  55.0M  /mypool\n"
msgstr ""
"# zfs list\n"
"NAME                         USED  AVAIL  REFER  MOUNTPOINT\n"
"backup                      55.4M   240G   152K  /backup\n"
"backup/mypool               55.3M   240G  55.2M  /backup/mypool\n"
"mypool                      55.6M  11.6G  55.0M  /mypool\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2258
#, no-wrap
msgid ""
"# zfs list -t snapshot\n"
"NAME                                         USED  AVAIL  REFER  MOUNTPOINT\n"
"backup/mypool@replica1                       104K      -  50.2M  -\n"
"backup/mypool@replica2                          0      -  55.2M  -\n"
"mypool@replica1                             29.9K      -  50.0M  -\n"
"mypool@replica2                                 0      -  55.0M  -\n"
msgstr ""
"# zfs list -t snapshot\n"
"NAME                                         USED  AVAIL  REFER  MOUNTPOINT\n"
"backup/mypool@replica1                       104K      -  50.2M  -\n"
"backup/mypool@replica2                          0      -  55.2M  -\n"
"mypool@replica1                             29.9K      -  50.0M  -\n"
"mypool@replica2                                 0      -  55.0M  -\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2263
msgid ""
"The incremental stream replicated the changed data rather than the entirety "
"of _replica1_.  Sending the differences alone took much less time to "
"transfer and saved disk space by not copying the whole pool each time.  This "
"is useful when replicating over a slow network or one charging per "
"transferred byte."
msgstr ""
"증분 스트림은 _replica1_ 의 전체가 아닌 변경된 데이터만 복제했습니다.  차이점"
"만 전송하면 매번 전체 풀을 복사하지 않아도 되므로 전송 시간이 훨씬 단축되고 "
"디스크 공간도 절약됩니다.  이 방법은 느린 네트워크를 통해 복제하거나 전송된 "
"바이트당 한 번만 충전할 때 유용합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2268
msgid ""
"A new file system, _backup/mypool_, is available with the files and data "
"from the pool _mypool_.  Specifying `-p` copies the dataset properties "
"including compression settings, quotas, and mount points.  Specifying `-R` "
"copies all child datasets of the dataset along with their properties.  "
"Automate sending and receiving to create regular backups on the second pool."
msgstr ""
"풀 _mypool_ 의 파일 및 데이터와 함께 새 파일 시스템인 _backup/mypool_ 을 사용"
"할 수 있습니다.  `-p` 를 지정하면 압축 설정, 할당량, 마운트 지점을 포함한 데"
"이터 세트 속성을 복사합니다.  `-R` 을 지정하면 데이터 세트의 모든 하위 데이"
"터 세트와 해당 속성을 함께 복사합니다.  송수신을 자동화하여 두 번째 풀에 정기"
"적인 백업을 생성합니다."

#. type: Title ====
#: documentation/content/en/books/handbook/zfs/_index.adoc:2270
#, no-wrap
msgid "Sending Encrypted Backups over SSH"
msgstr "SSH를 통한 백업의 암호화 전송"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2278
msgid ""
"Sending streams over the network is a good way to keep a remote backup, but "
"it does come with a drawback.  Data sent over the network link is not "
"encrypted, allowing anyone to intercept and transform the streams back into "
"data without the knowledge of the sending user.  This is undesirable when "
"sending the streams over the internet to a remote host.  Use SSH to securely "
"encrypt data sent over a network connection.  Since ZFS requires redirecting "
"the stream from standard output, piping it through SSH is easy.  To keep the "
"contents of the file system encrypted in transit and on the remote system, "
"consider using https://wiki.freebsd.org/PEFS[PEFS]."
msgstr ""
"네트워크를 통해 스트림을 전송하는 것은 원격 백업을 유지하는 좋은 방법이지만 "
"단점이 있습니다.  네트워크 링크를 통해 전송되는 데이터는 암호화되지 않으므로 "
"전송하는 사용자 모르게 누구나 스트림을 가로채서 다시 데이터로 변환할 수 있습"
"니다.  이는 인터넷을 통해 원격 호스트로 스트림을 전송할 때 바람직하지 않습니"
"다.  네트워크 연결을 통해 전송되는 데이터를 안전하게 암호화하려면 SSH를 사용"
"하세요.  ZFS는 표준 출력에서 스트림을 리디렉션해야 하므로 SSH를 통해 스트림"
"을 파이프하는 것이 쉽습니다.  전송 중 및 원격 시스템에서 파일 시스템의 콘텐츠"
"를 암호화하여 유지하려면 https://wiki.freebsd.org/PEFS[PEFS] 를 사용하는 것"
"이 좋습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2281
msgid ""
"Change some settings and take security precautions first.  This describes "
"the necessary steps required for the `zfs send` operation; for more "
"information on SSH, see crossref:security[openssh,\"OpenSSH\"]."
msgstr ""
"일부 설정을 변경하고 보안 예방 조치를 먼저 취하세요.  여기에서는 `zfs send` "
"작업에 필요한 단계를 설명합니다. SSH에 대한 자세한 내용은 crossref:"
"security[openssh,”OpenSSH”] 를 참조하세요."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2283
msgid "Change the configuration as follows:"
msgstr "다음과 같이 구성을 변경합니다:"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2285
msgid ""
"Passwordless SSH access between sending and receiving host using SSH keys"
msgstr ""
"SSH 키를 사용하여 송신 호스트와 수신 호스트 간 비밀번호 없는 SSH 액세스"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2286
msgid ""
"ZFS requires the privileges of the `root` user to send and receive streams. "
"This requires logging in to the receiving system as `root`."
msgstr ""
"ZFS는 스트림을 보내고 받으려면 `root` 사용자의 권한이 필요합니다. 이를 위해서"
"는 수신 시스템에 `root` 로 로그인해야 합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2287
msgid "Security reasons prevent `root` from logging in by default."
msgstr "보안상의 이유로 인해 `root` 는 기본적으로 로그인할 수 없습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2289
#, fuzzy
#| msgid ""
#| "Use the <<zfs-zfs-allow,ZFS Delegation>> system to allow a non-`root` "
#| "user on each system to perform the respective send and receive "
#| "operations.  On the sending system:"
msgid ""
"Use the crossref:zfs[zfs-zfs-allow,ZFS Delegation] system to allow a non-"
"`root` user on each system to perform the respective send and receive "
"operations.  On the sending system:"
msgstr ""
"각 시스템에서 `root` 가 아닌 사용자가 각각의 송수신 작업을 수행할 수 있도록 "
"허용하려면 <<zfs-zfs-allow,ZFS Delegation>> 시스템을 사용합니다.  보내는 시스"
"템에서:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2293
#, no-wrap
msgid "# zfs allow -u someuser send,snapshot mypool\n"
msgstr "# zfs allow -u someuser send,snapshot mypool\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2296
msgid ""
"To mount the pool, the unprivileged user must own the directory, and regular "
"users need permission to mount file systems."
msgstr ""
"풀을 마운트하려면 권한이 없는 사용자가 디렉터리를 소유해야 하며 일반 사용자"
"는 파일 시스템을 마운트할 수 있는 권한이 있어야 합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2298
msgid "On the receiving system:"
msgstr "수신 시스템에서:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2307
#, no-wrap
msgid ""
"# sysctl vfs.usermount=1\n"
"vfs.usermount: 0 -> 1\n"
"# echo vfs.usermount=1 >> /etc/sysctl.conf\n"
"# zfs create recvpool/backup\n"
"# zfs allow -u someuser create,mount,receive recvpool/backup\n"
"# chown someuser /recvpool/backup\n"
msgstr ""
"# sysctl vfs.usermount=1\n"
"vfs.usermount: 0 -> 1\n"
"# echo vfs.usermount=1 >> /etc/sysctl.conf\n"
"# zfs create recvpool/backup\n"
"# zfs allow -u someuser create,mount,receive recvpool/backup\n"
"# chown someuser /recvpool/backup\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2310
msgid ""
"The unprivileged user can receive and mount datasets now, and replicates the "
"_home_ dataset to the remote system:"
msgstr ""
"권한이 없는 사용자는 이제 데이터 세트를 수신하고 마운트할 수 있으며, _home_ "
"데이터 세트를 원격 시스템에 복제합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2315
#, no-wrap
msgid ""
"% zfs snapshot -r mypool/home@monday\n"
"% zfs send -R mypool/home@monday | ssh someuser@backuphost zfs recv -dvu recvpool/backup\n"
msgstr ""
"% zfs snapshot -r mypool/home@monday\n"
"% zfs send -R mypool/home@monday | ssh someuser@backuphost zfs recv -dvu recvpool/backup\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2325
msgid ""
"Create a recursive snapshot called _monday_ of the file system dataset "
"_home_ on the pool _mypool_.  Then `zfs send -R` includes the dataset, all "
"child datasets, snapshots, clones, and settings in the stream.  Pipe the "
"output through SSH to the waiting `zfs receive` on the remote host "
"_backuphost_.  Using an IP address or fully qualified domain name is good "
"practice.  The receiving machine writes the data to the _backup_ dataset on "
"the _recvpool_ pool.  Adding `-d` to `zfs recv` overwrites the name of the "
"pool on the receiving side with the name of the snapshot.  `-u` causes the "
"file systems to not mount on the receiving side.  Using `-v` shows more "
"details about the transfer, including the elapsed time and the amount of "
"data transferred."
msgstr ""
"_mypool_ 풀에 파일 시스템 데이터 세트 _home_ 의 _monday_ 라는 재귀 스냅샷을 "
"만듭니다.  그런 다음 `zfs send -R` 에 데이터 세트, 모든 하위 데이터 세트, 스"
"냅샷, 클론 및 설정을 스트림에 포함합니다.  SSH를 통해 출력을 원격 호스트 "
"_backuphost_ 의 대기 중인 `zfs receive` 로 파이프합니다.  IP 주소나 정규화된 "
"도메인 이름을 사용하는 것이 좋습니다.  수신 머신은 _recvpool_ 풀의 _backup_ "
"데이터 세트에 데이터를 씁니다.  `zfs recv` 에 `-d` 를 추가하면 수신 측의 풀 "
"이름을 스냅샷의 이름으로 덮어씁니다.  `-u` 를 사용하면 수신 측에 파일 시스템"
"이 마운트되지 않습니다.  `-v` 를 사용하면 경과된 시간 및 전송된 데이터 양을 "
"포함하여 전송에 대한 자세한 정보가 표시됩니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:2327
#, no-wrap
msgid "Dataset, User, and Group Quotas"
msgstr "데이터 세트, 사용자, 그리고 그룹 할당량"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2333
#, fuzzy
#| msgid ""
#| "Use <<zfs-term-quota,Dataset quotas>> to restrict the amount of space "
#| "consumed by a particular dataset.  <<zfs-term-refquota,Reference Quotas>> "
#| "work in much the same way, but count the space used by the dataset "
#| "itself, excluding snapshots and child datasets.  Similarly, use <<zfs-"
#| "term-userquota,user>> and <<zfs-term-groupquota,group>> quotas to prevent "
#| "users or groups from using up all the space in the pool or dataset."
msgid ""
"Use crossref:zfs[zfs-term-quota,Dataset quotas] to restrict the amount of "
"space consumed by a particular dataset.  crossref:zfs[zfs-term-refquota,"
"Reference Quotas] work in much the same way, but count the space used by the "
"dataset itself, excluding snapshots and child datasets.  Similarly, use "
"crossref:zfs[zfs-term-userquota,user] and crossref:zfs[zfs-term-groupquota,"
"group] quotas to prevent users or groups from using up all the space in the "
"pool or dataset."
msgstr ""
"특정 데이터 세트가 사용하는 공간의 양을 제한하려면 <<zfs-term-quota,Dataset "
"quotas>> 을 사용합니다.  <<zfs-term-refquota,Reference Quotas>> 도 거의 같은 "
"방식으로 작동하지만 스냅샷 및 하위 데이터 세트를 제외하고 데이터 세트 자체에"
"서 사용하는 공간을 계산합니다.  마찬가지로, 사용자 또는 그룹이 풀 또는 데이"
"터 집합의 모든 공간을 사용하지 못하도록 하려면 <<zfs-term-userquota,user>> "
"및 <<zfs-term-groupquota,group>> 할당량을 사용합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2338
msgid ""
"The following examples assume that the users already exist in the system.  "
"Before adding a user to the system, make sure to create their home dataset "
"first and set the `mountpoint` to `/home/_bob_`.  Then, create the user and "
"make the home directory point to the dataset's `mountpoint` location.  This "
"will properly set owner and group permissions without shadowing any pre-"
"existing home directory paths that might exist."
msgstr ""
"다음 예제에서는 사용자가 이미 시스템에 존재한다고 가정합니다.  시스템에 사용"
"자를 추가하기 전에 먼저 홈 데이터 세트를 생성하고 `mountpoint` 를 `/home/"
"_bob_` 로 설정해야 합니다.  그런 다음 사용자를 만들고 홈 디렉터리가 데이터 세"
"트의 `mountpoint` 위치를 가리키도록 합니다.  이렇게 하면 기존에 존재할 수 있"
"는 홈 디렉터리 경로를 섀도잉하지 않고 소유자 및 그룹 권한을 올바르게 설정할 "
"수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2340
msgid "To enforce a dataset quota of 10 GB for [.filename]#storage/home/bob#:"
msgstr ""
"[.filename]#storage/home/bob# 에 대해 10GB의 데이터 세트 할당량을 적용하려면:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2344
#, no-wrap
msgid "# zfs set quota=10G storage/home/bob\n"
msgstr "# zfs set quota=10G storage/home/bob\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2347
msgid ""
"To enforce a reference quota of 10 GB for [.filename]#storage/home/bob#:"
msgstr "[.filename]#storage/home/bob# 에 대해 10GB의 참조 할당량을 적용하려면:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2351
#, no-wrap
msgid "# zfs set refquota=10G storage/home/bob\n"
msgstr "# zfs set refquota=10G storage/home/bob\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2354
msgid "To remove a quota of 10 GB for [.filename]#storage/home/bob#:"
msgstr "[.filename]#storage/home/bob#에 대한 할당량 10GB를 제거하려면:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2358
#, no-wrap
msgid "# zfs set quota=none storage/home/bob\n"
msgstr "# zfs set quota=none storage/home/bob\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2361
msgid ""
"The general format is `userquota@_user_=_size_`, and the user's name must be "
"in one of these formats:"
msgstr ""
"일반적인 형식은 `userquota@_user_=_size_` 이며, 사용자 이름은 이 형식 중 하나"
"이어야 합니다:"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2363
msgid "POSIX compatible name such as _joe_."
msgstr "_joe_ 와 같은 POSIX 호환 이름."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2364
msgid "POSIX numeric ID such as _789_."
msgstr "_789_ 와 같은 POSIX 숫자 ID."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2365
msgid "SID name such as _joe.bloggs@example.com_."
msgstr "_joe.bloggs@example.com_ 같은 SID 이름."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2366
msgid "SID numeric ID such as _S-1-123-456-789_."
msgstr "_S-123-456-789_ 같은 SID 숫자 ID."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2368
msgid "For example, to enforce a user quota of 50 GB for the user named _joe_:"
msgstr "예를 들어 _joe_ 라는 사용자에 대해 50GB의 사용자 할당량을 적용하려면:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2372
#, no-wrap
msgid "# zfs set userquota@joe=50G\n"
msgstr "# zfs set userquota@joe=50G\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2375
msgid "To remove any quota:"
msgstr "모든 할당량을 제거하려면:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2379
#, no-wrap
msgid "# zfs set userquota@joe=none\n"
msgstr "# zfs set userquota@joe=none\n"

#. type: delimited block = 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2386
msgid ""
"User quota properties are not displayed by `zfs get all`.  Non-`root` users "
"can't see other's quotas unless granted the `userquota` privilege.  Users "
"with this privilege are able to view and set everyone's quota."
msgstr ""
"사용자 할당량 속성은 `zfs get all` 로 표시되지 않습니다.  `root` 가 아닌 사용"
"자는 `userquota` 권한이 부여되지 않는 한 다른 사용자의 할당량을 볼 수 없습니"
"다.  이 권한이 있는 사용자는 모든 사람의 할당량을 보고 설정할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2389
msgid ""
"The general format for setting a group quota is: `groupquota@_group_=_size_`."
msgstr ""
"그룹 할당량을 설정하는 일반적인 형식은 다음과 같습니다: "
"`groupquota@_group_=_size_`."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2391
msgid "To set the quota for the group _firstgroup_ to 50 GB, use:"
msgstr "_firstgroup_ 에 대한 할당량을 50GB로 설정하려면:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2395
#, no-wrap
msgid "# zfs set groupquota@firstgroup=50G\n"
msgstr "# zfs set groupquota@firstgroup=50G\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2398
msgid ""
"To remove the quota for the group _firstgroup_, or to make sure that one is "
"not set, instead use:"
msgstr ""
"_firstgroup_ 에 대한 할당량을 제거하거나 할당량 설정이 없는지 확인하려면:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2402
#, no-wrap
msgid "# zfs set groupquota@firstgroup=none\n"
msgstr "# zfs set groupquota@firstgroup=none\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2406
msgid ""
"As with the user quota property, non-`root` users can see the quotas "
"associated with the groups to which they belong.  A user with the "
"`groupquota` privilege or `root` can view and set all quotas for all groups."
msgstr ""
"사용자 할당량 속성과 마찬가지로 `root` 가 아닌 사용자는 자신이 속한 그룹과 관"
"련된 할당량을 볼 수 있습니다.  `groupquota` 권한 또는 `root` 권한이 있는 사용"
"자는 모든 그룹의 모든 할당량을 보고 설정할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2410
msgid ""
"To display the amount of space used by each user on a file system or "
"snapshot along with any quotas, use `zfs userspace`.  For group information, "
"use `zfs groupspace`.  For more information about supported options or how "
"to display specific options alone, refer to man:zfs[1]."
msgstr ""
"할당량과 함께 파일 시스템 또는 스냅샷에서 각 사용자가 사용하는 공간을 표시하"
"려면 `zfs userspace` 를 사용합니다.  그룹 정보를 보려면 `zfs groupspace` 를 "
"사용합니다.  지원되는 옵션 또는 특정 옵션만 표시하는 방법에 대한 자세한 내용"
"은 man:zfs[1]을 참조하십시오."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2412
msgid ""
"Privileged users and `root` can list the quota for [.filename]#storage/home/"
"bob# using:"
msgstr ""
"권한 있는 사용자와 `root` 는 [.filename]#storage/home/bob# 에 대한 할당량을 "
"다음으로 나열할 수 있습니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2416
#, no-wrap
msgid "# zfs get quota storage/home/bob\n"
msgstr "# zfs get quota storage/home/bob\n"

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:2419
#, no-wrap
msgid "Reservations"
msgstr "예약"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2424
#, fuzzy
#| msgid ""
#| "<<zfs-term-reservation,Reservations>> guarantee an always-available "
#| "amount of space on a dataset.  The reserved space will not be available "
#| "to any other dataset.  This useful feature ensures that free space is "
#| "available for an important dataset or log files."
msgid ""
"crossref:zfs[zfs-term-reservation,Reservations] guarantee an always-"
"available amount of space on a dataset.  The reserved space will not be "
"available to any other dataset.  This useful feature ensures that free space "
"is available for an important dataset or log files."
msgstr ""
"<<zfs-term-reservation,Reservations>> 은 데이터 집합에서 항상 사용 가능한 공"
"간을 보장합니다.  예약된 공간은 다른 데이터 세트에서 사용할 수 없습니다.  이 "
"유용한 기능을 사용하면 중요한 데이터 세트나 로그 파일에 여유 공간을 확보할 "
"수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2426
msgid ""
"The general format of the `reservation` property is `reservation=_size_`, so "
"to set a reservation of 10 GB on [.filename]#storage/home/bob#, use:"
msgstr ""
"`reservation` 속성의 일반적인 형식은 `reservation=_size_` 이므로 [."
"filename]#storage/home/bob# 에 10GB의 예약을 설정하려면 다음과 같이 사용합니"
"다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2430
#, no-wrap
msgid "# zfs set reservation=10G storage/home/bob\n"
msgstr "# zfs set reservation=10G storage/home/bob\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2433
msgid "To clear any reservation:"
msgstr "예약을 삭제하려면:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2437
#, no-wrap
msgid "# zfs set reservation=none storage/home/bob\n"
msgstr "# zfs set reservation=none storage/home/bob\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2441
#, fuzzy
#| msgid ""
#| "The same principle applies to the `refreservation` property for setting a "
#| "<<zfs-term-refreservation,Reference Reservation>>, with the general "
#| "format `refreservation=_size_`."
msgid ""
"The same principle applies to the `refreservation` property for setting a "
"crossref:zfs[zfs-term-refreservation,Reference Reservation], with the "
"general format `refreservation=_size_`."
msgstr ""
"동일한 원칙이 <<zfs-term-refreservation,Reference Reservation>> 을 설정하기 "
"위한 `refreservation` 속성에도 적용되며, 일반적인 형식은 "
"`refreservation=_size_` 입니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2443
msgid ""
"This command shows any reservations or refreservations that exist on [."
"filename]#storage/home/bob#:"
msgstr ""
"다음 명령은 [.filename]#storage/home/bob# 에 있는 모든 예약 또는 예약을 표시"
"합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2448
#, no-wrap
msgid ""
"# zfs get reservation storage/home/bob\n"
"# zfs get refreservation storage/home/bob\n"
msgstr ""
"# zfs get reservation storage/home/bob\n"
"# zfs get refreservation storage/home/bob\n"

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:2451
#, no-wrap
msgid "Compression"
msgstr "압축"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2458
#, fuzzy
#| msgid ""
#| "ZFS provides transparent compression.  Compressing data written at the "
#| "block level saves space and also increases disk throughput.  If data "
#| "compresses by 25% the compressed data writes to the disk at the same rate "
#| "as the uncompressed version, resulting in an effective write speed of "
#| "125%.  Compression can also be a great alternative to <<zfs-zfs-"
#| "deduplication,Deduplication>> because it does not require extra memory."
msgid ""
"ZFS provides transparent compression.  Compressing data written at the block "
"level saves space and also increases disk throughput.  If data compresses by "
"25% the compressed data writes to the disk at the same rate as the "
"uncompressed version, resulting in an effective write speed of 125%.  "
"Compression can also be a great alternative to crossref:zfs[zfs-zfs-"
"deduplication,Deduplication] because it does not require extra memory."
msgstr ""
"ZFS는 투명한 압축을 제공합니다.  블록 수준에서 데이터를 압축하면 공간이 절약"
"되고 디스크 처리량도 증가합니다.  데이터가 25% 압축되면 압축된 데이터는 압축"
"되지 않은 버전과 동일한 속도로 디스크에 기록되므로 효과적인 쓰기 속도는 125%"
"에 이릅니다.  압축은 추가 메모리를 필요로 하지 않기 때문에 <<zfs-zfs-"
"deduplication,Deduplication>> 에 대한 훌륭한 대안이 될 수도 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2465
#, fuzzy
#| msgid ""
#| "ZFS offers different compression algorithms, each with different trade-"
#| "offs.  The introduction of LZ4 compression in ZFS v5000 enables "
#| "compressing the entire pool without the large performance trade-off of "
#| "other algorithms.  The biggest advantage to LZ4 is the _early abort_ "
#| "feature.  If LZ4 does not achieve at least 12.5% compression in the "
#| "header part of the data, ZFS writes the block uncompressed to avoid "
#| "wasting CPU cycles trying to compress data that is either already "
#| "compressed or uncompressible.  For details about the different "
#| "compression algorithms available in ZFS, see the <<zfs-term-compression,"
#| "Compression>> entry in the terminology section."
msgid ""
"ZFS offers different compression algorithms, each with different trade-"
"offs.  The introduction of LZ4 compression in ZFS v5000 enables compressing "
"the entire pool without the large performance trade-off of other "
"algorithms.  The biggest advantage to LZ4 is the _early abort_ feature.  If "
"LZ4 does not achieve at least 12.5% compression in the header part of the "
"data, ZFS writes the block uncompressed to avoid wasting CPU cycles trying "
"to compress data that is either already compressed or uncompressible.  For "
"details about the different compression algorithms available in ZFS, see the "
"crossref:zfs[zfs-term-compression,Compression] entry in the terminology "
"section."
msgstr ""
"ZFS는 각각 다른 장단점을 가진 다양한 압축 알고리즘을 제공합니다.  ZFS v5000"
"에 LZ4 압축을 도입하면 다른 알고리즘의 큰 성능 저하 없이 전체 풀을 압축할 수 "
"있습니다.  LZ4의 가장 큰 장점은 _조기 중단_ 기능입니다.  LZ4가 데이터의 헤더 "
"부분에서 최소 12.5%의 압축률을 달성하지 못하면 ZFS는 이미 압축되었거나 압축"
"할 수 없는 데이터를 압축하기 위해 CPU 사이클을 낭비하지 않도록 해당 블록을 압"
"축되지 않은 상태로 씁니다.  ZFS에서 사용할 수 있는 다양한 압축 알고리즘에 대"
"한 자세한 내용은 용어 섹션의 <<zfs-term-compression,Compression>> 항목을 참조"
"하세요."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2467
msgid ""
"The administrator can see the effectiveness of compression using dataset "
"properties."
msgstr "관리자는 데이터 세트 속성을 사용하여 압축의 효과를 확인할 수 있습니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2476
#, no-wrap
msgid ""
"# zfs get used,compressratio,compression,logicalused mypool/compressed_dataset\n"
"NAME        PROPERTY          VALUE     SOURCE\n"
"mypool/compressed_dataset  used              449G      -\n"
"mypool/compressed_dataset  compressratio     1.11x     -\n"
"mypool/compressed_dataset  compression       lz4       local\n"
"mypool/compressed_dataset  logicalused       496G      -\n"
msgstr ""
"# zfs get used,compressratio,compression,logicalused mypool/compressed_dataset\n"
"NAME        PROPERTY          VALUE     SOURCE\n"
"mypool/compressed_dataset  used              449G      -\n"
"mypool/compressed_dataset  compressratio     1.11x     -\n"
"mypool/compressed_dataset  compression       lz4       local\n"
"mypool/compressed_dataset  logicalused       496G      -\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2481
msgid ""
"The dataset is using 449 GB of space (the used property).  Without "
"compression, it would have taken 496 GB of space (the `logicalused` "
"property).  This results in a 1.11:1 compression ratio."
msgstr ""
"데이터 세트는 449GB의 공간(사용된 속성)을 사용하고 있습니다.  압축을 하지 않"
"았다면 496GB의 공간( `logicalused`` 속성 )이 필요했을 것입니다.  그 결과 압"
"축 비율은 1.11:1이 됩니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2488
#, fuzzy
#| msgid ""
#| "Compression can have an unexpected side effect when combined with <<zfs-"
#| "term-userquota,User Quotas>>.  User quotas restrict how much actual space "
#| "a user consumes on a dataset _after compression_.  If a user has a quota "
#| "of 10 GB, and writes 10 GB of compressible data, they will still be able "
#| "to store more data.  If they later update a file, say a database, with "
#| "more or less compressible data, the amount of space available to them "
#| "will change.  This can result in the odd situation where a user did not "
#| "increase the actual amount of data (the `logicalused` property), but the "
#| "change in compression caused them to reach their quota limit."
msgid ""
"Compression can have an unexpected side effect when combined with crossref:"
"zfs[zfs-term-userquota,User Quotas].  User quotas restrict how much actual "
"space a user consumes on a dataset _after compression_.  If a user has a "
"quota of 10 GB, and writes 10 GB of compressible data, they will still be "
"able to store more data.  If they later update a file, say a database, with "
"more or less compressible data, the amount of space available to them will "
"change.  This can result in the odd situation where a user did not increase "
"the actual amount of data (the `logicalused` property), but the change in "
"compression caused them to reach their quota limit."
msgstr ""
"압축은 <<zfs-term-userquota,User Quotas>> 과 함께 사용하면 예기치 않은 부작용"
"이 발생할 수 있습니다.  사용자 할당량은 사용자가 데이터 세트에서 _압축 후_ 실"
"제로 소비하는 공간을 제한합니다.  할당량이 10GB인 사용자가 10GB의 압축 가능"
"한 데이터를 쓰는 경우, 여전히 더 많은 데이터를 저장할 수 있습니다.  나중에 압"
"축 가능한 데이터가 더 많거나 적은 파일(예: 데이터베이스)을 업데이트하면 사용 "
"가능한 공간의 양이 변경됩니다.  이로 인해 사용자가 실제 데이터의 양"
"( `logicalused` 속성 )을 늘리지 않았지만 압축 변경으로 인해 할당량 제한에 도"
"달하는 이상한 상황이 발생할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2492
msgid ""
"Compression can have a similar unexpected interaction with backups.  Quotas "
"are often used to limit data storage to ensure there is enough backup space "
"available.  Since quotas do not consider compression ZFS may write more data "
"than would fit with uncompressed backups."
msgstr ""
"압축은 백업과 비슷한 예기치 않은 상호 작용을 일으킬 수 있습니다.  할당량은 사"
"용 가능한 백업 공간을 충분히 확보하기 위해 데이터 스토리지를 제한하는 데 자"
"주 사용됩니다.  할당량은 압축을 고려하지 않기 때문에 ZFS는 압축되지 않은 백업"
"에 적합한 것보다 더 많은 데이터를 쓸 수 있습니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:2494
#, no-wrap
msgid "Zstandard Compression"
msgstr "Zstandard 압축"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2498
msgid ""
"OpenZFS 2.0 added a new compression algorithm.  Zstandard (Zstd) offers "
"higher compression ratios than the default LZ4 while offering much greater "
"speeds than the alternative, gzip. OpenZFS 2.0 is available starting with "
"FreeBSD 12.1-RELEASE via package:sysutils/openzfs[] and has been the default "
"in since FreeBSD 13.0-RELEASE."
msgstr ""
"OpenZFS 2.0에는 새로운 압축 알고리즘이 추가되었습니다.  Zstandard(Zstd)는 기"
"본 LZ4보다 높은 압축률을 제공하는 동시에 대체 압축 방식인 gzip보다 훨씬 빠른 "
"속도를 제공합니다. OpenZFS 2.0은 FreeBSD 12.1-RELEASE부터 package:sysutils/"
"openzfs[] 를 통해 사용할 수 있으며, FreeBSD 13.0-RELEASE부터는 기본값이 되었"
"습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2502
msgid ""
"Zstd provides a large selection of compression levels, providing fine-"
"grained control over performance versus compression ratio.  One of the main "
"advantages of Zstd is that the decompression speed is independent of the "
"compression level.  For data written once but read often, Zstd allows the "
"use of the highest compression levels without a read performance penalty."
msgstr ""
"Zstd는 다양한 압축 레벨을 제공하여 성능 대비 압축률을 세밀하게 제어할 수 있습"
"니다.  압축 해제 속도가 압축 레벨과 무관하다는 것이 Zstd의 주요 장점 중 하나"
"입니다.  한 번 쓰지만 자주 읽는 데이터의 경우, Zstd를 사용하면 읽기 성능 저"
"하 없이 가장 높은 압축 수준을 사용할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2507
msgid ""
"Even with frequent data updates, enabling compression often provides higher "
"performance.  One of the biggest advantages comes from the compressed ARC "
"feature.  ZFS's Adaptive Replacement Cache (ARC) caches the compressed "
"version of the data in RAM, decompressing it each time.  This allows the "
"same amount of RAM to store more data and metadata, increasing the cache hit "
"ratio."
msgstr ""
"데이터를 자주 업데이트하는 경우에도 압축을 활성화하면 성능이 향상되는 경우가 "
"많습니다.  가장 큰 장점 중 하나는 압축된 ARC 기능에서 비롯됩니다.  ZFS의 "
"ARC(Adaptive Replacement Cache, 적응형 대체 캐시)는 압축된 버전의 데이터를 "
"RAM에 캐시하여 매번 압축을 해제합니다.  따라서 동일한 양의 RAM에 더 많은 데이"
"터와 메타데이터를 저장할 수 있으므로 캐시 적중률이 높아집니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2513
msgid ""
"ZFS offers 19 levels of Zstd compression, each offering incrementally more "
"space savings in exchange for slower compression.  The default level is "
"`zstd-3` and offers greater compression than LZ4 without being much slower.  "
"Levels above 10 require large amounts of memory to compress each block and "
"systems with less than 16 GB of RAM should not use them.  ZFS uses a "
"selection of the Zstd_fast_ levels also, which get correspondingly faster "
"but supports lower compression ratios.  ZFS supports `zstd-fast-1` through "
"`zstd-fast-10`, `zstd-fast-20` through `zstd-fast-100` in increments of 10, "
"and `zstd-fast-500` and `zstd-fast-1000` which provide minimal compression, "
"but offer high performance."
msgstr ""
"ZFS는 19단계의 Zstd 압축 레벨을 제공하며, 각 레벨은 압축 속도가 느려지는 대"
"신 점진적으로 더 많은 공간을 절약할 수 있습니다.  기본 레벨은 `zstd-3` 이며, "
"LZ4보다 많이 느리지 않으면서도 더 큰 압축률을 제공합니다.  10 이상의 레벨은 "
"각 블록을 압축하는 데 많은 양의 메모리가 필요하므로 16GB 미만의 RAM이 있는 시"
"스템에서는 사용하지 않는 것이 좋습니다.  ZFS는 그에 상응하여 더 빠르지만 더 "
"낮은 압축률을 지원하는 Zstd_fast_ 레벨도 사용합니다.  ZFS는 `zstd-fast-1` 부"
"터 `zstd-fast-10` 까지, `zstd-fast-20` 부터 `zstd-fast-100` 까지 10 단위로, "
"최소한의 압축도 제공하지만, 높은 성능을 제공하는 `zstd-fast-500` 및 `zstd-"
"fast-1000` 역시 지원합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2517
msgid ""
"If ZFS is not able to get the required memory to compress a block with Zstd, "
"it will fall back to storing the block uncompressed.  This is unlikely to "
"happen except at the highest levels of Zstd on memory constrained systems.  "
"ZFS counts how often this has occurred since loading the ZFS module with "
"`kstat.zfs.misc.zstd.compress_alloc_fail`."
msgstr ""
"ZFS가 Zstd로 블록을 압축하는 데 필요한 메모리를 확보하지 못하면 블록을 압축하"
"지 않은 상태로 저장하는 상태로 되돌아갑니다.  메모리 제약이 있는 시스템에서 "
"가장 높은 수준의 Zstd를 사용하는 경우를 제외하고는 이런 일이 발생할 가능성은 "
"거의 없습니다.  ZFS는 `kstat.zfs.misc.zstd.compress_alloc_fail` 로 ZFS 모듈"
"을 로드한 이후 이 문제가 얼마나 자주 발생했는지를 계산합니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:2519
#, no-wrap
msgid "Deduplication"
msgstr "중복 제거"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2525
#, fuzzy
#| msgid ""
#| "When enabled, <<zfs-term-deduplication,deduplication>> uses the checksum "
#| "of each block to detect duplicate blocks.  When a new block is a "
#| "duplicate of an existing block, ZFS writes a new reference to the "
#| "existing data instead of the whole duplicate block.  Tremendous space "
#| "savings are possible if the data contains a lot of duplicated files or "
#| "repeated information.  Warning: deduplication requires a large amount of "
#| "memory, and enabling compression instead provides most of the space "
#| "savings without the extra cost."
msgid ""
"When enabled, crossref:zfs[zfs-term-deduplication,deduplication] uses the "
"checksum of each block to detect duplicate blocks.  When a new block is a "
"duplicate of an existing block, ZFS writes a new reference to the existing "
"data instead of the whole duplicate block.  Tremendous space savings are "
"possible if the data contains a lot of duplicated files or repeated "
"information.  Warning: deduplication requires a large amount of memory, and "
"enabling compression instead provides most of the space savings without the "
"extra cost."
msgstr ""
"이 옵션을 활성화하면 <<zfs-term-deduplication,deduplication>> 은 각 블록의 체"
"크섬을 사용하여 중복 블록을 감지합니다.  새 블록이 기존 블록의 중복인 경우, "
"ZFS는 전체 중복 블록 대신 기존 데이터에 대한 새 참조를 씁니다.  데이터에 중복"
"된 파일이나 반복되는 정보가 많이 포함되어 있는 경우 공간을 크게 절약할 수 있"
"습니다.  경고: 중복 제거에는 많은 양의 메모리가 필요하며, 대신 압축을 활성화"
"하면 추가 비용 없이 대부분의 공간을 절약할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2527
msgid "To activate deduplication, set the `dedup` property on the target pool:"
msgstr "중복 제거를 활성화하려면 대상 풀에서 `dedup` 속성을 설정합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2531
#, no-wrap
msgid "# zfs set dedup=on pool\n"
msgstr "# zfs set dedup=on pool\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2536
msgid ""
"Deduplicating only affects new data written to the pool.  Merely activating "
"this option will not deduplicate data already written to the pool.  A pool "
"with a freshly activated deduplication property will look like this example:"
msgstr ""
"중복 제거는 풀에 새로 기록되는 데이터에만 영향을 줍니다.  이 옵션을 활성화하"
"는 것만으로는 이미 풀에 기록된 데이터는 중복 제거되지 않습니다.  새로 활성화"
"된 중복 제거 속성이 있는 풀은 이 예시와 같습니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2542
#, no-wrap
msgid ""
"# zpool list\n"
"NAME  SIZE ALLOC  FREE   CKPOINT  EXPANDSZ   FRAG   CAP   DEDUP   HEALTH   ALTROOT\n"
"pool 2.84G 2.19M 2.83G         -         -     0%    0%   1.00x   ONLINE   -\n"
msgstr ""
"# zpool list\n"
"NAME  SIZE ALLOC  FREE   CKPOINT  EXPANDSZ   FRAG   CAP   DEDUP   HEALTH   ALTROOT\n"
"pool 2.84G 2.19M 2.83G         -         -     0%    0%   1.00x   ONLINE   -\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2547
msgid ""
"The `DEDUP` column shows the actual rate of deduplication for the pool.  A "
"value of `1.00x` shows that data has not deduplicated yet.  The next example "
"copies some system binaries three times into different directories on the "
"deduplicated pool created above."
msgstr ""
"`DEDUP` 열에는 풀의 실제 중복 제거율이 표시됩니다.  값이 `1.00x` 이면 데이터"
"가 아직 중복 제거되지 않았음을 나타냅니다.  다음 예제는 위에서 생성한 중복 제"
"거 풀의 다른 디렉터리에 일부 시스템 바이너리를 세 번 복사합니다."

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2553
#, no-wrap
msgid ""
"# for d in dir1 dir2 dir3; do\n"
"> mkdir $d && cp -R /usr/bin $d &\n"
"> done\n"
msgstr ""
"# for d in dir1 dir2 dir3; do\n"
"> mkdir $d && cp -R /usr/bin $d &\n"
"> done\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2556
msgid "To observe deduplicating of redundant data, use:"
msgstr "중복 데이터의 중복 제거를 관찰하려면 다음을 사용하세요:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2562
#, no-wrap
msgid ""
"# zpool list\n"
"NAME SIZE  ALLOC  FREE   CKPOINT  EXPANDSZ   FRAG  CAP   DEDUP   HEALTH   ALTROOT\n"
"pool 2.84G 20.9M 2.82G         -         -     0%   0%   3.00x   ONLINE   -\n"
msgstr ""
"# zpool list\n"
"NAME SIZE  ALLOC  FREE   CKPOINT  EXPANDSZ   FRAG  CAP   DEDUP   HEALTH   ALTROOT\n"
"pool 2.84G 20.9M 2.82G         -         -     0%   0%   3.00x   ONLINE   -\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2567
msgid ""
"The `DEDUP` column shows a factor of `3.00x`.  Detecting and deduplicating "
"copies of the data uses a third of the space.  The potential for space "
"savings can be enormous, but comes at the cost of having enough memory to "
"keep track of the deduplicated blocks."
msgstr ""
"`DEDUP` 열에는 `3.00x` 의 계수가 표시됩니다.  데이터의 복사본을 감지하고 중"
"복 제거하면 1/3의 공간을 사용하게 됩니다.  공간 절약의 잠재력은 엄청날 수 있"
"지만, 중복 제거된 블록을 추적할 수 있는 충분한 메모리를 확보해야 한다는 대가"
"가 따릅니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2570
msgid ""
"Deduplication is not always beneficial when the data in a pool is not "
"redundant.  ZFS can show potential space savings by simulating deduplication "
"on an existing pool:"
msgstr ""
"풀의 데이터가 중복되지 않는 경우 중복 제거가 항상 이로운 것은 아닙니다.  ZFS"
"는 기존 풀에서 중복 제거를 시뮬레이션하여 잠재적인 공간 절약 효과를 보여줄 "
"수 있습니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2575
#, no-wrap
msgid ""
"# zdb -S pool\n"
"Simulated DDT histogram:\n"
msgstr ""
"# zdb -S pool\n"
"Simulated DDT histogram:\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2591
#, no-wrap
msgid ""
"bucket              allocated                       referenced\n"
"______   ______________________________   ______________________________\n"
"refcnt   blocks   LSIZE   PSIZE   DSIZE   blocks   LSIZE   PSIZE   DSIZE\n"
"------   ------   -----   -----   -----   ------   -----   -----   -----\n"
"     1    2.58M    289G    264G    264G    2.58M    289G    264G    264G\n"
"     2     206K   12.6G   10.4G   10.4G     430K   26.4G   21.6G   21.6G\n"
"     4    37.6K    692M    276M    276M     170K   3.04G   1.26G   1.26G\n"
"     8    2.18K   45.2M   19.4M   19.4M    20.0K    425M    176M    176M\n"
"    16      174   2.83M   1.20M   1.20M    3.33K   48.4M   20.4M   20.4M\n"
"    32       40   2.17M    222K    222K    1.70K   97.2M   9.91M   9.91M\n"
"    64        9     56K   10.5K   10.5K      865   4.96M    948K    948K\n"
"   128        2   9.50K      2K      2K      419   2.11M    438K    438K\n"
"   256        5   61.5K     12K     12K    1.90K   23.0M   4.47M   4.47M\n"
"    1K        2      1K      1K      1K    2.98K   1.49M   1.49M   1.49M\n"
" Total    2.82M    303G    275G    275G    3.20M    319G    287G    287G\n"
msgstr ""
"bucket              allocated                       referenced\n"
"______   ______________________________   ______________________________\n"
"refcnt   blocks   LSIZE   PSIZE   DSIZE   blocks   LSIZE   PSIZE   DSIZE\n"
"------   ------   -----   -----   -----   ------   -----   -----   -----\n"
"     1    2.58M    289G    264G    264G    2.58M    289G    264G    264G\n"
"     2     206K   12.6G   10.4G   10.4G     430K   26.4G   21.6G   21.6G\n"
"     4    37.6K    692M    276M    276M     170K   3.04G   1.26G   1.26G\n"
"     8    2.18K   45.2M   19.4M   19.4M    20.0K    425M    176M    176M\n"
"    16      174   2.83M   1.20M   1.20M    3.33K   48.4M   20.4M   20.4M\n"
"    32       40   2.17M    222K    222K    1.70K   97.2M   9.91M   9.91M\n"
"    64        9     56K   10.5K   10.5K      865   4.96M    948K    948K\n"
"   128        2   9.50K      2K      2K      419   2.11M    438K    438K\n"
"   256        5   61.5K     12K     12K    1.90K   23.0M   4.47M   4.47M\n"
"    1K        2      1K      1K      1K    2.98K   1.49M   1.49M   1.49M\n"
" Total    2.82M    303G    275G    275G    3.20M    319G    287G    287G\n"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2593
#, no-wrap
msgid "dedup = 1.05, compress = 1.11, copies = 1.00, dedup * compress / copies = 1.16\n"
msgstr "dedup = 1.05, compress = 1.11, copies = 1.00, dedup * compress / copies = 1.16\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2603
#, fuzzy
#| msgid ""
#| "After `zdb -S` finishes analyzing the pool, it shows the space reduction "
#| "ratio that activating deduplication would achieve.  In this case, `1.16` "
#| "is a poor space saving ratio mainly provided by compression.  Activating "
#| "deduplication on this pool would not save any amount of space, and is not "
#| "worth the amount of memory required to enable deduplication.  Using the "
#| "formula _ratio = dedup * compress / copies_, system administrators can "
#| "plan the storage allocation, deciding whether the workload will contain "
#| "enough duplicate blocks to justify the memory requirements.  If the data "
#| "is reasonably compressible, the space savings may be good.  Good practice "
#| "is to enable compression first as compression also provides greatly "
#| "increased performance.  Enable deduplication in cases where savings are "
#| "considerable and with enough available memory for the <<zfs-term-"
#| "deduplication,DDT>>."
msgid ""
"After `zdb -S` finishes analyzing the pool, it shows the space reduction "
"ratio that activating deduplication would achieve.  In this case, `1.16` is "
"a poor space saving ratio mainly provided by compression.  Activating "
"deduplication on this pool would not save any amount of space, and is not "
"worth the amount of memory required to enable deduplication.  Using the "
"formula _ratio = dedup * compress / copies_, system administrators can plan "
"the storage allocation, deciding whether the workload will contain enough "
"duplicate blocks to justify the memory requirements.  If the data is "
"reasonably compressible, the space savings may be good.  Good practice is to "
"enable compression first as compression also provides greatly increased "
"performance.  Enable deduplication in cases where savings are considerable "
"and with enough available memory for the crossref:zfs[zfs-term-deduplication,"
"DDT]."
msgstr ""
"`zdb -S` 가 풀 분석을 완료한 후 중복 제거를 활성화하면 얻을 수 있는 공간 절"
"감 비율을 보여줍니다.  이 경우 `1.16` 은 주로 압축에 의해 제공되는 낮은 공간 "
"절약 비율입니다.  이 풀에서 중복 제거를 활성화해도 공간을 절약할 수 없으며 중"
"복 제거를 활성화하는 데 필요한 메모리 양에 비해 가치가 없습니다.  시스템 관리"
"자는 _비율 = 중복 제거 * 압축 / 복사본_ 공식을 사용하여 스토리지 할당을 계획"
"하고 워크로드에 메모리 요구 사항을 정당화할 수 있는 충분한 중복 블록이 포함될"
"지 여부를 결정할 수 있습니다.  데이터를 합리적으로 압축할 수 있는 경우 공간"
"을 절약할 수 있습니다.  압축을 사용하면 성능이 크게 향상되므로 압축을 먼저 활"
"성화하는 것이 좋습니다.  <<zfs-term-deduplication,DDT>> 에 사용할 수 있는 메"
"모리가 충분하고 절약 효과가 상당한 경우 중복 제거를 활성화합니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:2605
#, no-wrap
msgid "ZFS and Jails"
msgstr "ZFS와 Jails"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2611
msgid ""
"Use `zfs jail` and the corresponding `jailed` property to delegate a ZFS "
"dataset to a crossref:jails[jails,Jail].  `zfs jail _jailid_` attaches a "
"dataset to the specified jail, and `zfs unjail` detaches it.  To control the "
"dataset from within a jail, set the `jailed` property.  ZFS forbids mounting "
"a jailed dataset on the host because it may have mount points that would "
"compromise the security of the host."
msgstr ""
"`zfs jail` 및 해당 `jailed` 속성을 사용하여 ZFS 데이터 집합을 crossref:"
"jails[jails,Jail] 에 위임합니다.  `zfs jail _jailid_` 는 데이터 집합을 지정"
"된 jail에 연결하고, `zfs unjail` 은 데이터 집합을 분리합니다.  jail 내에서 데"
"이터 세트를 제어하려면 `jailed` 속성을 설정합니다.  ZFS는 호스트의 보안을 손"
"상시킬 수 있는 마운트 지점이 있을 수 있으므로 호스트에 jailed 데이터 세트를 "
"마운트하는 것을 금지합니다."

#. type: Title ==
#: documentation/content/en/books/handbook/zfs/_index.adoc:2613
#, no-wrap
msgid "Delegated Administration"
msgstr "위임 관리"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2621
msgid ""
"A comprehensive permission delegation system allows unprivileged users to "
"perform ZFS administration functions.  For example, if each user's home "
"directory is a dataset, users need permission to create and destroy "
"snapshots of their home directories.  A user performing backups can get "
"permission to use replication features.  ZFS allows a usage statistics "
"script to run with access to only the space usage data for all users.  "
"Delegating the ability to delegate permissions is also possible.  Permission "
"delegation is possible for each subcommand and most properties."
msgstr ""
"포괄적인 권한 위임 시스템을 통해 권한이 없는 사용자도 ZFS 관리 기능을 수행할 "
"수 있습니다.  예를 들어, 각 사용자의 홈 디렉터리가 데이터 세트인 경우, 사용자"
"는 홈 디렉터리의 스냅샷을 생성하고 삭제할 수 있는 권한이 필요합니다.  백업을 "
"수행하는 사용자는 복제 기능을 사용할 수 있는 권한을 얻을 수 있습니다.  ZFS를 "
"사용하면 모든 사용자의 공간 사용량 데이터에만 액세스하여 사용량 통계 스크립트"
"를 실행할 수 있습니다.  권한을 위임하는 기능도 가능합니다.  각 하위 명령과 대"
"부분의 속성에 대해 권한 위임이 가능합니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:2623
#, no-wrap
msgid "Delegating Dataset Creation"
msgstr "데이터 세트 생성 위임하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2629
msgid ""
"`zfs allow _someuser_ create _mydataset_` gives the specified user "
"permission to create child datasets under the selected parent dataset.  A "
"caveat: creating a new dataset involves mounting it.  That requires setting "
"the FreeBSD `vfs.usermount` man:sysctl[8] to `1` to allow non-root users to "
"mount a file system.  Another restriction aimed at preventing abuse: non-"
"`root` users must own the mountpoint where mounting the file system."
msgstr ""
"`zfs allow _someuser_ create _mydataset_` 은 지정된 사용자에게 선택한 상위 데"
"이터 집합 아래에 하위 데이터 집합을 만들 수 있는 권한을 부여합니다.  주의: "
"새 데이터세트를 생성하려면 마운팅이 필요합니다.  이를 위해서는 루트 사용자가 "
"아닌 사용자가 파일 시스템을 마운트할 수 있도록 FreeBSD `vfs.usermount` man:"
"sysctl[8]을 `1` 로 설정해야 합니다.  악용을 방지하기 위한 또 다른 제한: "
"`root` 가 아닌 사용자는 파일 시스템을 마운트하는 마운트 지점을 소유하고 있어"
"야 합니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:2631
#, no-wrap
msgid "Delegating Permission Delegation"
msgstr "권한 위임 위임하기"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2635
msgid ""
"`zfs allow _someuser_ allow _mydataset_` gives the specified user the "
"ability to assign any permission they have on the target dataset, or its "
"children, to other users.  If a user has the `snapshot` permission and the "
"`allow` permission, that user can then grant the `snapshot` permission to "
"other users."
msgstr ""
"`zfs allow _someuser_ allow _mydataset_` 은 지정된 사용자에게 대상 데이터셋 "
"또는 그 하위 데이터셋에 대한 모든 권한을 다른 사용자에게 할당할 수 있는 기능"
"을 제공합니다.  사용자에게 `snapshot` 권한과 `allow` 권한이 있는 경우, 해당 "
"사용자는 다른 사용자에게 `snapshot` 권한을 부여할 수 있습니다."

#. type: Title ==
#: documentation/content/en/books/handbook/zfs/_index.adoc:2637
#, no-wrap
msgid "Advanced Topics"
msgstr "고급 주제"

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:2640
#, no-wrap
msgid "Tuning"
msgstr "튜닝"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2643
msgid "Adjust tunables to make ZFS perform best for different workloads."
msgstr ""
"튜너블을 조정하여 ZFS가 다양한 워크로드에 가장 적합한 성능을 발휘하도록 하세"
"요."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2647
#, fuzzy
#| msgid ""
#| "[[zfs-advanced-tuning-arc_max]] `_vfs.zfs.arc.max_` starting with 13.x "
#| "(`vfs.zfs.arc_max` for 12.x) - Upper size of the <<zfs-term-arc,ARC>>. "
#| "The default is all RAM but 1 GB, or 5/8 of all RAM, whichever is more. "
#| "Use a lower value if the system runs any other daemons or processes that "
#| "may require memory. Adjust this value at runtime with man:sysctl[8] and "
#| "set it in [.filename]#/boot/loader.conf# or [.filename]#/etc/sysctl.conf#."
msgid ""
"[[zfs-advanced-tuning-arc_max]] `_vfs.zfs.arc.max_` starting with 13.x (`vfs."
"zfs.arc_max` for 12.x) - Upper size of the crossref:zfs[zfs-term-arc,ARC]. "
"The default is all RAM but 1 GB, or 5/8 of all RAM, whichever is more. Use a "
"lower value if the system runs any other daemons or processes that may "
"require memory. Adjust this value at runtime with man:sysctl[8] and set it "
"in [.filename]#/boot/loader.conf# or [.filename]#/etc/sysctl.conf#."
msgstr ""
"13.x에서 [[zfs-advanced-tuning-arc_max]] `_vfs.zfs.arc.max_` 로 시작하기 (12."
"x의 경우 `vfs.zfs.arc_max`) - <<zfs-term-arc,ARC>>의 상위 크기입니다. 기본값"
"은 1GB를 제외한 모든 RAM 또는 전체 RAM의 5/8 중 더 큰 값입니다. 시스템에서 메"
"모리가 필요할 수 있는 다른 데몬이나 프로세스를 실행하는 경우 더 낮은 값을 사"
"용하세요. 런타임에 man:sysctl[8]을 사용하여 이 값을 조정하고 [.filename]#/"
"boot/loader.conf# 또는 [.filename]#/etc/sysctl.conf# 에서 설정합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2654
#, fuzzy
#| msgid ""
#| "[[zfs-advanced-tuning-arc_meta_limit]] `_vfs.zfs.arc.meta_limit_` "
#| "starting with 13.x (`vfs.zfs.arc_meta_limit` for 12.x)` - Limit the "
#| "amount of the <<zfs-term-arc,ARC>> used to store metadata. The default is "
#| "one fourth of `vfs.zfs.arc.max`. Increasing this value will improve "
#| "performance if the workload involves operations on a large number of "
#| "files and directories, or frequent metadata operations, at the cost of "
#| "less file data fitting in the <<zfs-term-arc,ARC>>. Adjust this value at "
#| "runtime with man:sysctl[8] in [.filename]#/boot/loader.conf# or [."
#| "filename]#/etc/sysctl.conf#."
msgid ""
"[[zfs-advanced-tuning-arc_meta_limit]] `_vfs.zfs.arc.meta_limit_` starting "
"with 13.x (`vfs.zfs.arc_meta_limit` for 12.x) - Limit the amount of the "
"crossref:zfs[zfs-term-arc,ARC] used to store metadata. The default is one "
"fourth of `vfs.zfs.arc.max`. Increasing this value will improve performance "
"if the workload involves operations on a large number of files and "
"directories, or frequent metadata operations, at the cost of less file data "
"fitting in the crossref:zfs[zfs-term-arc,ARC]. Adjust this value at runtime "
"with man:sysctl[8] in [.filename]#/boot/loader.conf# or [.filename]#/etc/"
"sysctl.conf#."
msgstr ""
"13.x에서 [[zfs-advanced-tuning-arc_meta_limit]] `_vfs.zfs.arc.meta_limit_` "
"로 시작하기 ( 12.x의 경우 `vfs.zfs.arc_meta_limit` ) - 메타데이터 저장에 사용"
"되는 <<zfs-term-arc,ARC>>의 양을 제한합니다. 기본값은 `vfs.zfs.arc.max` 의 4"
"분의 1입니다. 워크로드에 많은 수의 파일 및 디렉터리에 대한 작업 또는 빈번한 "
"메타데이터 작업이 포함된 경우 이 값을 늘리면 <<zfs-term-arc,ARC>> 에 맞는 파"
"일 데이터가 줄어드는 대신 성능이 향상됩니다. 런타임에 [.filename]#/boot/"
"loader.conf# 또는 [.filename]#/etc/sysctl.conf# 에서 man:sysctl[8]을 사용하"
"여 이 값을 조정합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2659
#, fuzzy
#| msgid ""
#| "[[zfs-advanced-tuning-arc_min]] `_vfs.zfs.arc.min_` starting with 13.x "
#| "(`vfs.zfs.arc_min` for 12.x) - Lower size of the <<zfs-term-arc,ARC>>. "
#| "The default is one half of `vfs.zfs.arc.meta_limit`. Adjust this value to "
#| "prevent other applications from pressuring out the entire <<zfs-term-arc,"
#| "ARC>>. Adjust this value at runtime with man:sysctl[8] and in [."
#| "filename]#/boot/loader.conf# or [.filename]#/etc/sysctl.conf#."
msgid ""
"[[zfs-advanced-tuning-arc_min]] `_vfs.zfs.arc.min_` starting with 13.x (`vfs."
"zfs.arc_min` for 12.x) - Lower size of the crossref:zfs[zfs-term-arc,ARC]. "
"The default is one half of `vfs.zfs.arc.meta_limit`. Adjust this value to "
"prevent other applications from pressuring out the entire crossref:zfs[zfs-"
"term-arc,ARC]. Adjust this value at runtime with man:sysctl[8] and in [."
"filename]#/boot/loader.conf# or [.filename]#/etc/sysctl.conf#."
msgstr ""
"13.x에서 [[zfs-advanced-tuning-arc_min]] `_vfs.zfs.arc.min_` 로 시작하기 "
"( 12.x의 경우 `vfs.zfs.arc_min` ) - <<zfs-term-arc,ARC>> 의 크기를 낮춥니다. "
"기본값은 `vfs.zfs.arc.meta_limit` 의 절반입니다. 이 값을 조정하면 다른 응용 "
"프로그램이 전체 <<zfs-term-arc,ARC>> 에 압력을 가하는 것을 방지할 수 있습니"
"다. 런타임에 man:sysctl[8] 및 [.filename]#/boot/loader.conf# 또는 [."
"filename]#/etc/sysctl.conf# 에서 이 값을 조정합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2660
msgid ""
"[[zfs-advanced-tuning-vdev-cache-size]] `_vfs.zfs.vdev.cache.size_` - A "
"preallocated amount of memory reserved as a cache for each device in the "
"pool. The total amount of memory used will be this value multiplied by the "
"number of devices. Set this value at boot time and in [.filename]#/boot/"
"loader.conf#."
msgstr ""
"[[zfs-advanced-tuning-vdev-cache-size]] `_vfs.zfs.vdev.cache.size_` - 풀의 "
"각 장치에 대해 캐시로 예약된 사전 할당된 메모리 양입니다. 사용되는 총 메모리 "
"양은 이 값에 장치 수를 곱한 값입니다. 이 값은 부팅 시와 [.filename]#/boot/"
"loader.conf# 에서 설정합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2661
msgid ""
"[[zfs-advanced-tuning-min-auto-ashift]] `_vfs.zfs.min_auto_ashift_` - Lower "
"`ashift` (sector size) used automatically at pool creation time. The value "
"is a power of two. The default value of `9` represents `2^9 = 512`, a sector "
"size of 512 bytes. To avoid _write amplification_ and get the best "
"performance, set this value to the largest sector size used by a device in "
"the pool."
msgstr ""
"[[zfs-advanced-tuning-min-auto-ashift]] `_vfs.zfs.min_auto_ashift_` - 풀 생"
"성 시 자동으로 사용되는 `ashift` (섹터 크기)를 낮춥니다. 이 값은 2의 거듭제곱"
"입니다. 기본값인 `9` 는 512바이트의 섹터 크기인 `2^9 = 512` 를 나타냅니다. 쓰"
"기 증폭을 방지하고 최상의 성능을 얻으려면 이 값을 풀의 장치에서 사용하는 가"
"장 큰 섹터 크기로 설정하십시오."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2667
msgid ""
"Common drives have 4 KB sectors.  Using the default `ashift` of `9` with "
"these drives results in write amplification on these devices.  Data "
"contained in a single 4 KB write is instead written in eight 512-byte "
"writes.  ZFS tries to read the native sector size from all devices when "
"creating a pool, but drives with 4 KB sectors report that their sectors are "
"512 bytes for compatibility.  Setting `vfs.zfs.min_auto_ashift` to `12` "
"(`2^12 = 4096`) before creating a pool forces ZFS to use 4 KB blocks for "
"best performance on these drives."
msgstr ""
"일반적인 드라이브에는 4KB 섹터가 있습니다.  이러한 드라이브에 기본값인 "
"`ashift` 를 `9` 로 사용하면 이러한 장치에서 쓰기 증폭이 발생합니다.  단일 "
"4KB 쓰기에 포함된 데이터는 대신 8개의 512바이트 쓰기로 기록됩니다.  ZFS는 풀"
"을 생성할 때 모든 장치에서 기본 섹터 크기를 읽으려고 시도하지만, 4KB 섹터를 "
"가진 드라이브는 호환성을 위해 섹터가 512바이트라고 보고합니다.  풀을 생성하"
"기 전에 `vfs.zfs.min_auto_ashift` 를 `12` ( `2^12 = 4096` )로 설정하면 이러"
"한 드라이브에서 최상의 성능을 위해 ZFS가 4KB 블록을 사용하도록 강제합니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2670
msgid ""
"Forcing 4 KB blocks is also useful on pools with planned disk upgrades.  "
"Future disks use 4 KB sectors, and `ashift` values cannot change after "
"creating a pool."
msgstr ""
"4KB 블록 강제화는 디스크 업그레이드가 계획된 풀에서도 유용합니다.  향후 디스"
"크는 4KB 섹터를 사용하며, 풀을 생성한 후에는 `ashift` 값을 변경할 수 없습니"
"다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2674
msgid ""
"In some specific cases, the smaller 512-byte block size might be "
"preferable.  When used with 512-byte disks for databases or as storage for "
"virtual machines, less data transfers during small random reads.  This can "
"provide better performance when using a smaller ZFS record size."
msgstr ""
"일부 특정 경우에는 더 작은 512바이트 블록 크기가 더 바람직할 수 있습니다.  데"
"이터베이스나 가상 머신의 스토리지로 512바이트 디스크를 사용하면 소규모 랜덤 "
"읽기 시 데이터 전송이 줄어듭니다.  따라서 더 작은 ZFS 레코드 크기를 사용할 "
"때 더 나은 성능을 제공할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2678
#, fuzzy
#| msgid ""
#| "[[zfs-advanced-tuning-prefetch_disable]] `_vfs.zfs.prefetch_disable_` - "
#| "Disable prefetch. A value of `0` enables and `1` disables it. The default "
#| "is `0`, unless the system has less than 4 GB of RAM. Prefetch works by "
#| "reading larger blocks than requested into the <<zfs-term-arc,ARC>> in "
#| "hopes to soon need the data. If the workload has a large number of random "
#| "reads, disabling prefetch may actually improve performance by reducing "
#| "unnecessary reads. Adjust this value at any time with man:sysctl[8]."
msgid ""
"[[zfs-advanced-tuning-prefetch_disable]] `_vfs.zfs.prefetch_disable_` - "
"Disable prefetch. A value of `0` enables and `1` disables it. The default is "
"`0`, unless the system has less than 4 GB of RAM. Prefetch works by reading "
"larger blocks than requested into the crossref:zfs[zfs-term-arc,ARC] in "
"hopes to soon need the data. If the workload has a large number of random "
"reads, disabling prefetch may actually improve performance by reducing "
"unnecessary reads. Adjust this value at any time with man:sysctl[8]."
msgstr ""
"[[zfs-advanced-tuning-prefetch_disable]] `_vfs.zfs.prefetch_disable_` - 프리"
"페치를 비활성화합니다. 값이 `0` 이면 활성화하고 `1` 이면 비활성화합니다. 시스"
"템의 RAM이 4GB 미만인 경우를 제외하고 기본값은 `0` 입니다. 프리페치는 데이터"
"가 곧 필요할 것으로 예상하여 요청된 것보다 더 큰 블록을 <<zfs-term-arc,ARC>> "
"로 읽어오는 방식으로 작동합니다. 워크로드에 무작위 읽기가 많은 경우, 프리페치"
"를 비활성화하면 불필요한 읽기가 줄어들어 성능이 실제로 향상될 수 있습니다. "
"이 값은 언제든지 man:sysctl[8]로 조정할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2679
msgid ""
"[[zfs-advanced-tuning-vdev-trim_on_init]] `_vfs.zfs.vdev.trim_on_init_` - "
"Control whether new devices added to the pool have the `TRIM` command run on "
"them. This ensures the best performance and longevity for SSDs, but takes "
"extra time. If the device has already been secure erased, disabling this "
"setting will make the addition of the new device faster. Adjust this value "
"at any time with man:sysctl[8]."
msgstr ""
"[[zfs-advanced-tuning-vdev-trim_on_init]] `_vfs.zfs.vdev.trim_on_init_` - 풀"
"에 추가된 새 장치에서 `TRIM` 명령을 실행할지 여부를 제어합니다. 이렇게 하면 "
"SSD의 성능과 수명을 최상으로 유지할 수 있지만 시간이 더 걸립니다. 장치가 이"
"미 보안 삭제된 경우 이 설정을 비활성화하면 새 장치를 더 빠르게 추가할 수 있습"
"니다. 이 값은 언제든지 man:sysctl[8]로 조정할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2680
msgid ""
"[[zfs-advanced-tuning-vdev-max_pending]] `_vfs.zfs.vdev.max_pending_` - "
"Limit the number of pending I/O requests per device. A higher value will "
"keep the device command queue full and may give higher throughput. A lower "
"value will reduce latency. Adjust this value at any time with man:sysctl[8]."
msgstr ""
"[[zfs-advanced-tuning-vdev-max_pending]] `_vfs.zfs.vdev.max_pending_` - 장치"
"당 보류 중인 I/O 요청 수를 제한합니다. 값이 클수록 장치 명령 대기열이 가득 차"
"며 처리량이 높아질 수 있습니다. 값이 낮을수록 지연 시간이 줄어듭니다. 이 값"
"은 언제든지 man:sysctl[8]로 조정할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2686
#, fuzzy
#| msgid ""
#| "[[zfs-advanced-tuning-top_maxinflight]] `_vfs.zfs.top_maxinflight_` - "
#| "Upper number of outstanding I/Os per top-level <<zfs-term-vdev,vdev>>. "
#| "Limits the depth of the command queue to prevent high latency. The limit "
#| "is per top-level vdev, meaning the limit applies to each <<zfs-term-vdev-"
#| "mirror,mirror>>, <<zfs-term-vdev-raidz,RAID-Z>>, or other vdev "
#| "independently. Adjust this value at any time with man:sysctl[8]."
msgid ""
"[[zfs-advanced-tuning-top_maxinflight]] `_vfs.zfs.top_maxinflight_` - Upper "
"number of outstanding I/Os per top-level crossref:zfs[zfs-term-vdev,vdev].  "
"Limits the depth of the command queue to prevent high latency. The limit is "
"per top-level vdev, meaning the limit applies to each crossref:zfs[zfs-term-"
"vdev-mirror,mirror], crossref:zfs[zfs-term-vdev-raidz,RAID-Z], or other vdev "
"independently. Adjust this value at any time with man:sysctl[8]."
msgstr ""
"[[zfs-advanced-tuning-top_maxinflight]] `_vfs.zfs.top_maxinflight_` - 최상위 "
"레벨 <<zfs-term-vdev,vdev>> 당 미해결 I/O의 상한 수입니다. 높은 대기 시간을 "
"방지하기 위해 명령 대기열의 깊이를 제한합니다. 이 제한은 최상위 vdev별로 적용"
"되므로, 각 <<zfs-term-vdev-mirror,mirror>>, <<zfs-term-vdev-raidz,RAID-Z>> 또"
"는 기타 vdev에 독립적으로 적용됩니다. 이 값은 언제든지 man:sysctl[8]로 조정"
"할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2688
#, fuzzy
#| msgid ""
#| "[[zfs-advanced-tuning-l2arc_write_max]] `_vfs.zfs.l2arc_write_max_` - "
#| "Limit the amount of data written to the <<zfs-term-l2arc,L2ARC>> per "
#| "second. This tunable extends the longevity of SSDs by limiting the amount "
#| "of data written to the device. Adjust this value at any time with man:"
#| "sysctl[8]."
msgid ""
"[[zfs-advanced-tuning-l2arc_write_max]] `_vfs.zfs.l2arc_write_max_` - Limit "
"the amount of data written to the crossref:zfs[zfs-term-l2arc,L2ARC] per "
"second. This tunable extends the longevity of SSDs by limiting the amount of "
"data written to the device. Adjust this value at any time with man:sysctl[8]."
msgstr ""
"[[zfs-advanced-tuning-l2arc_write_max]] `_vfs.zfs.l2arc_write_max_` - 초당 "
"<<zfs-term-l2arc,L2ARC>> 에 기록되는 데이터의 양을 제한합니다. 이 튜너블은 장"
"치에 기록되는 데이터의 양을 제한하여 SSD의 수명을 연장합니다. 이 값은 언제든"
"지 man:sysctl[8]로 조정할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2694
#, fuzzy
#| msgid ""
#| "[[zfs-advanced-tuning-l2arc_write_boost]] `_vfs.zfs.l2arc_write_boost_` - "
#| "Adds the value of this tunable to <<zfs-advanced-tuning-l2arc_write_max,"
#| "`vfs.zfs.l2arc_write_max`>> and increases the write speed to the SSD "
#| "until evicting the first block from the <<zfs-term-l2arc,L2ARC>>. This "
#| "\"Turbo Warmup Phase\" reduces the performance loss from an empty <<zfs-"
#| "term-l2arc,L2ARC>> after a reboot. Adjust this value at any time with man:"
#| "sysctl[8]."
msgid ""
"[[zfs-advanced-tuning-l2arc_write_boost]] `_vfs.zfs.l2arc_write_boost_` - "
"Adds the value of this tunable to crossref:zfs[zfs-advanced-tuning-"
"l2arc_write_max,`vfs.zfs.l2arc_write_max`] and increases the write speed to "
"the SSD until evicting the first block from the crossref:zfs[zfs-term-l2arc,"
"L2ARC]. This \"Turbo Warmup Phase\" reduces the performance loss from an "
"empty crossref:zfs[zfs-term-l2arc,L2ARC] after a reboot. Adjust this value "
"at any time with man:sysctl[8]."
msgstr ""
"[[zfs-advanced-tuning-l2arc_write_boost]] `_vfs.zfs.l2arc_write_boost_` - 이 "
"튜너블의 값을 <<zfs-advanced-tuning-l2arc_write_max,`vfs.zfs."
"l2arc_write_max`>> 에 추가하고 <<zfs-term-l2arc,L2ARC>> 에서 첫 블록을 제거"
"할 때까지 SSD에 쓰기 속도를 높입니다. 이 “터보 워밍업 단계”는 재부팅 후 비어 "
"있는 <<zfs-term-l2arc,L2ARC>> 로 인한 성능 손실을 줄여줍니다. 이 값은 man:"
"sysctl[8]로 언제든지 조정할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2699
msgid ""
"[[zfs-advanced-tuning-scrub_delay]]`_vfs.zfs.scrub_delay_` - Number of ticks "
"to delay between each I/O during a crossref:zfs[zfs-term-scrub,`scrub`]. To "
"ensure that a `scrub` does not interfere with the normal operation of the "
"pool, if any other I/O is happening the `scrub` will delay between each "
"command. This value controls the limit on the total IOPS (I/Os Per Second)"
msgstr ""

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2707
#, fuzzy
#| msgid ""
#| "[[zfs-advanced-tuning-scrub_delay]]`_vfs.zfs.scrub_delay_` - Number of "
#| "ticks to delay between each I/O during a <<zfs-term-scrub,`scrub`>>. To "
#| "ensure that a `scrub` does not interfere with the normal operation of the "
#| "pool, if any other I/O is happening the `scrub` will delay between each "
#| "command. This value controls the limit on the total IOPS (I/Os Per "
#| "Second) generated by the `scrub`. The granularity of the setting is "
#| "determined by the value of `kern.hz` which defaults to 1000 ticks per "
#| "second. Changing this setting results in a different effective IOPS "
#| "limit.  The default value is `4`, resulting in a limit of: 1000 ticks/"
#| "sec / 4 = 250 IOPS. Using a value of _20_ would give a limit of: 1000 "
#| "ticks/sec / 20 = 50 IOPS. Recent activity on the pool limits the speed of "
#| "`scrub`, as determined by <<zfs-advanced-tuning-scan_idle,`vfs.zfs."
#| "scan_idle`>>. Adjust this value at any time with man:sysctl[8]."
msgid ""
"generated by the `scrub`. The granularity of the setting is determined by "
"the value of `kern.hz` which defaults to 1000 ticks per second.  Changing "
"this setting results in a different effective IOPS limit.  The default value "
"is `4`, resulting in a limit of: 1000 ticks/sec / 4 = 250 IOPS. Using a "
"value of _20_ would give a limit of: 1000 ticks/sec / 20 = 50 IOPS. Recent "
"activity on the pool limits the speed of `scrub`, as determined by crossref:"
"zfs[zfs-advanced-tuning-scan_idle,`vfs.zfs.scan_idle`]. Adjust this value at "
"any time with man:sysctl[8]."
msgstr ""
"[[zfs-advanced-tuning-scrub_delay]]`_vfs.zfs.scrub_delay_` - <<zfs-term-"
"scrub,`scrub`>> 동안 각 I/O 사이에 지연시킬 틱 수입니다. `scrub` 이 풀의 정상"
"적인 작동을 방해하지 않도록 하기 위해, 다른 I/O가 발생하면 `scrub` 은 각 명"
"령 사이에 지연됩니다. 이 값은 `scrub` 에 의해 생성되는 총 IOPS(초당 입출력 횟"
"수)의 제한을 관리합니다. 설정의 세부 단위는 기본값이 초당 1000 틱인 `kern."
"hz` 값에 의해 결정됩니다. 이 설정을 변경하면 유효 IOPS 제한이 달라집니다.  기"
"본값은 `4` 이며, 그 결과 제한은 다음과 같습니다: 1000 ticks/sec / 4 = 250 "
"IOPS. _20_ 을 사용하면 다음과 같은 제한이 적용됩니다: 1000 ticks/sec / 20 = "
"50 IOPS. 풀의 최근 활동은 <<zfs-advanced-tuning-scan_idle,`vfs.zfs."
"scan_idle`>> 에 의해 결정되는 `scrub` 의 속도를 제한합니다. man:sysctl[8]로 "
"언제든지 이 값을 조정할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2723
#, fuzzy
#| msgid ""
#| "[[zfs-advanced-tuning-resilver_delay]] `_vfs.zfs.resilver_delay_` - "
#| "Number of milliseconds of delay inserted between each I/O during a <<zfs-"
#| "term-resilver,resilver>>. To ensure that a resilver does not interfere "
#| "with the normal operation of the pool, if any other I/O is happening the "
#| "resilver will delay between each command. This value controls the limit "
#| "of total IOPS (I/Os Per Second) generated by the resilver. ZFS determins "
#| "the granularity of the setting by the value of `kern.hz` which defaults "
#| "to 1000 ticks per second. Changing this setting results in a different "
#| "effective IOPS limit. The default value is 2, resulting in a limit of: "
#| "1000 ticks/sec / 2 = 500 IOPS. Returning the pool to an <<zfs-term-online,"
#| "Online>> state may be more important if another device failing could "
#| "<<zfs-term-faulted,Fault>> the pool, causing data loss. A value of 0 will "
#| "give the resilver operation the same priority as other operations, "
#| "speeding the healing process. Other recent activity on the pool limits "
#| "the speed of resilver, as determined by <<zfs-advanced-tuning-scan_idle,"
#| "`vfs.zfs.scan_idle`>>. Adjust this value at any time with man:sysctl[8]."
msgid ""
"[[zfs-advanced-tuning-resilver_delay]] `_vfs.zfs.resilver_delay_` - Number "
"of milliseconds of delay inserted between each I/O during a crossref:zfs[zfs-"
"term-resilver,resilver]. To ensure that a resilver does not interfere with "
"the normal operation of the pool, if any other I/O is happening the resilver "
"will delay between each command. This value controls the limit of total IOPS "
"(I/Os Per Second) generated by the resilver. ZFS determins the granularity "
"of the setting by the value of `kern.hz` which defaults to 1000 ticks per "
"second. Changing this setting results in a different effective IOPS limit. "
"The default value is 2, resulting in a limit of: 1000 ticks/sec / 2 = 500 "
"IOPS. Returning the pool to an crossref:zfs[zfs-term-online,Online] state "
"may be more important if another device failing could crossref:zfs[zfs-term-"
"faulted,Fault] the pool, causing data loss. A value of 0 will give the "
"resilver operation the same priority as other operations, speeding the "
"healing process. Other recent activity on the pool limits the speed of "
"resilver, as determined by crossref:zfs[zfs-advanced-tuning-scan_idle,`vfs."
"zfs.scan_idle`]. Adjust this value at any time with man:sysctl[8]."
msgstr ""
"[[zfs-advanced-tuning-resilver_delay]] `_vfs.zfs.resilver_delay_` - <<zfs-"
"term-resilver,resilver>> 동안 각 I/O 사이에 삽입되는 지연 시간(밀리초). "
"Resilver가 풀의 정상적인 작동을 방해하지 않도록 하기 위해 다른 I/O가 발생하"
"면 resilver가 각 명령 사이에 지연을 줍니다. 이 값은 resilver가 생성하는 총 "
"IOPS(초당 입출력 횟수)의 한도를 제어합니다. ZFS는 기본값이 초당 1000 틱인 "
"`kern.hz` 값에 따라 설정의 세분성을 결정합니다. 이 설정을 변경하면 유효 IOPS "
"제한이 달라집니다. 기본값은 2이므로 제한은 다음과 같습니다: 1000 ticks/sec / "
"2 = 500 IOPS. 다른 장치에 장애가 발생하여 풀이 <<zfs-term-online,Online>> 상"
"태가 되어 데이터가 손실될 수 있는 경우 풀을 <<zfs-term-faulted,Fault>> 상태"
"로 되돌리면 더 중요할 수 있습니다. 값이 0이면 복구 작업에 다른 작업과 동일한 "
"우선순위를 부여하여 복구 프로세스의 속도를 높일 수 있습니다. 풀의 다른 최근 "
"활동은 <<zfs-advanced-tuning-scan_idle,`vfs.zfs.scan_idle`>> 에 의해 결정되"
"는 resilver의 속도를 제한합니다. 이 값은 언제든지 man:sysctl[8]로 조정할 수 "
"있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2727
#, fuzzy
#| msgid ""
#| "[[zfs-advanced-tuning-scan_idle]] `_vfs.zfs.scan_idle_` - Number of "
#| "milliseconds since the last operation before considering the pool is "
#| "idle. ZFS disables the rate limiting for <<zfs-term-scrub,`scrub`>> and "
#| "<<zfs-term-resilver,resilver>> when the pool is idle. Adjust this value "
#| "at any time with man:sysctl[8]."
msgid ""
"[[zfs-advanced-tuning-scan_idle]] `_vfs.zfs.scan_idle_` - Number of "
"milliseconds since the last operation before considering the pool is idle. "
"ZFS disables the rate limiting for crossref:zfs[zfs-term-scrub,`scrub`] and "
"crossref:zfs[zfs-term-resilver,resilver] when the pool is idle. Adjust this "
"value at any time with man:sysctl[8]."
msgstr ""
"[[zfs-advanced-tuning-scan_idle]] `_vfs.zfs.scan_idle_` - 풀이 유휴 상태라고 "
"간주하기 전 마지막 작업 이후의 시간(밀리초). ZFS는 풀이 유휴 상태일 때 <<zfs-"
"term-scrub,`scrub`>> 및 <<zfs-term-resilver,resilver>>에 대한 속도 제한을 비"
"활성화합니다. 이 값은 언제든지 man:sysctl[8]로 조정할 수 있습니다."

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2729
#, fuzzy
#| msgid ""
#| "[[zfs-advanced-tuning-txg-timeout]] `_vfs.zfs.txg.timeout_` - Upper "
#| "number of seconds between <<zfs-term-txg,transaction group>>s. The "
#| "current transaction group writes to the pool and a fresh transaction "
#| "group starts if this amount of time elapsed since the previous "
#| "transaction group. A transaction group may trigger earlier if writing "
#| "enough data. The default value is 5 seconds. A larger value may improve "
#| "read performance by delaying asynchronous writes, but this may cause "
#| "uneven performance when writing the transaction group. Adjust this value "
#| "at any time with man:sysctl[8]."
msgid ""
"[[zfs-advanced-tuning-txg-timeout]] `_vfs.zfs.txg.timeout_` - Upper number "
"of seconds between crossref:zfs[zfs-term-txg,transaction group]s. The "
"current transaction group writes to the pool and a fresh transaction group "
"starts if this amount of time elapsed since the previous transaction group. "
"A transaction group may trigger earlier if writing enough data. The default "
"value is 5 seconds. A larger value may improve read performance by delaying "
"asynchronous writes, but this may cause uneven performance when writing the "
"transaction group. Adjust this value at any time with man:sysctl[8]."
msgstr ""
"[[zfs-advanced-tuning-txg-timeout]] `_vfs.zfs.txg.timeout_` - <<zfs-term-txg,"
"transaction group>> 사이의 초 단위 상한 시간. 이전 트랜잭션 그룹 이후 이 시간"
"이 경과하면 현재 트랜잭션 그룹이 풀에 쓰고 새로운 트랜잭션 그룹이 시작됩니"
"다. 데이터를 충분히 쓰면 트랜잭션 그룹이 더 일찍 트리거될 수 있습니다. 기본값"
"은 5초입니다. 값이 클수록 비동기 쓰기가 지연되어 읽기 성능이 향상될 수 있지"
"만 트랜잭션 그룹을 쓸 때 성능이 고르지 않을 수 있습니다. 이 값은 언제든지 "
"man:sysctl[8]로 조정할 수 있습니다."

#. type: Title ===
#: documentation/content/en/books/handbook/zfs/_index.adoc:2731
#, no-wrap
msgid "ZFS on i386"
msgstr "i386에서 ZFS"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2734
msgid ""
"Some of the features provided by ZFS are memory intensive, and may require "
"tuning for upper efficiency on systems with limited RAM."
msgstr ""
"ZFS에서 제공하는 일부 기능은 메모리를 많이 사용하므로 RAM이 제한된 시스템에"
"서 효율성을 높이기 위해 튜닝이 필요할 수 있습니다."

#. type: Title ====
#: documentation/content/en/books/handbook/zfs/_index.adoc:2735
#, no-wrap
msgid "Memory"
msgstr "메모리"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2743
msgid ""
"As a lower value, the total system memory should be at least one gigabyte.  "
"The amount of recommended RAM depends upon the size of the pool and which "
"features ZFS uses.  A general rule of thumb is 1 GB of RAM for every 1 TB of "
"storage.  If using the deduplication feature, a general rule of thumb is 5 "
"GB of RAM per TB of storage to deduplicate.  While some users use ZFS with "
"less RAM, systems under heavy load may panic due to memory exhaustion.  ZFS "
"may require further tuning for systems with less than the recommended RAM "
"requirements."
msgstr ""
"더 낮은 값으로, 총 시스템 메모리는 1기가바이트 이상이어야 합니다.  권장 RAM "
"용량은 풀의 크기와 ZFS에서 사용하는 기능에 따라 다릅니다.  일반적으로 1TB의 "
"스토리지당 1GB의 RAM이 권장됩니다.  중복 제거 기능을 사용하는 경우에는 일반적"
"으로 중복 제거할 스토리지 1TB당 5GB의 RAM이 필요합니다.  일부 사용자는 더 적"
"은 RAM으로 ZFS를 사용하지만, 과부하 상태의 시스템은 메모리 고갈로 인해 당황"
"할 수 있습니다.  권장 RAM 요구 사항보다 적은 시스템에서는 ZFS를 추가로 조정해"
"야 할 수도 있습니다."

#. type: Title ====
#: documentation/content/en/books/handbook/zfs/_index.adoc:2744
#, no-wrap
msgid "Kernel Configuration"
msgstr "커널 구성"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2747
msgid ""
"Due to the address space limitations of the i386(TM) platform, ZFS users on "
"the i386(TM) architecture must add this option to a custom kernel "
"configuration file, rebuild the kernel, and reboot:"
msgstr ""
"i386(TM) 플랫폼의 주소 공간 제한으로 인해 i386(TM) 아키텍처의 ZFS 사용자는 사"
"용자 지정 커널 구성 파일에 이 옵션을 추가하고 커널을 다시 빌드한 후 재부팅해"
"야 합니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2751
#, no-wrap
msgid "options        KVA_PAGES=512\n"
msgstr "options        KVA_PAGES=512\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2756
msgid ""
"This expands the kernel address space, allowing the `vm.kvm_size` tunable to "
"push beyond the imposed limit of 1 GB, or the limit of 2 GB for PAE.  To "
"find the most suitable value for this option, divide the desired address "
"space in megabytes by four.  In this example `512` for 2 GB."
msgstr ""
"이렇게 하면 커널 주소 공간이 확장되어 `vm.kvm_size` 튜너블에 부과된 제한인 "
"1GB 또는 PAE의 경우 제한인 2GB를 초과할 수 있습니다.  이 옵션에 가장 적합한 "
"값을 찾으려면 원하는 주소 공간을 메가바이트 단위에 4로 나누면 됩니다.  이 예"
"에서는 2GB의 경우 `512` 입니다."

#. type: Title ====
#: documentation/content/en/books/handbook/zfs/_index.adoc:2757
#, no-wrap
msgid "Loader Tunables"
msgstr "로더 튜너블"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2761
msgid ""
"Increases the [.filename]#kmem# address space on all FreeBSD architectures.  "
"A test system with 1 GB of physical memory benefitted from adding these "
"options to [.filename]#/boot/loader.conf# and then restarting:"
msgstr ""
"모든 FreeBSD 아키텍처에서 [.filename]#kmem# 주소 공간을 늘립니다.  1GB의 물리"
"적 메모리가 있는 테스트 시스템에서 [.filename]#/boot/loader.conf# 에 아래의 "
"옵션을 추가한 후 다시 시작하면 이점을 얻을 수 있었습니다:"

#. type: delimited block . 4
#: documentation/content/en/books/handbook/zfs/_index.adoc:2768
#, no-wrap
msgid ""
"vm.kmem_size=\"330M\"\n"
"vm.kmem_size_max=\"330M\"\n"
"vfs.zfs.arc.max=\"40M\"\n"
"vfs.zfs.vdev.cache.size=\"5M\"\n"
msgstr ""
"vm.kmem_size=\"330M\"\n"
"vm.kmem_size_max=\"330M\"\n"
"vfs.zfs.arc.max=\"40M\"\n"
"vfs.zfs.vdev.cache.size=\"5M\"\n"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2771
msgid ""
"For a more detailed list of recommendations for ZFS-related tuning, see "
"https://wiki.freebsd.org/ZFSTuningGuide[]."
msgstr ""
"ZFS 관련 튜닝에 대한 자세한 권장 사항 목록은 https://wiki.freebsd.org/"
"ZFSTuningGuide[]를 참조하세요."

#. type: Title ==
#: documentation/content/en/books/handbook/zfs/_index.adoc:2773
#, no-wrap
msgid "Further Resources"
msgstr "추가 자료"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2776
msgid "https://openzfs.org/[OpenZFS]"
msgstr "https://openzfs.org/[OpenZFS]"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2777
msgid "https://wiki.freebsd.org/ZFSTuningGuide[FreeBSD Wiki - ZFS Tuning]"
msgstr "https://wiki.freebsd.org/ZFSTuningGuide[FreeBSD Wiki - ZFS Tuning]"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2778
msgid ""
"https://calomel.org/zfs_raid_speed_capacity.html[Calomel Blog - ZFS Raidz "
"Performance, Capacity and Integrity]"
msgstr ""
"https://calomel.org/zfs_raid_speed_capacity.html[Calomel Blog - ZFS Raidz "
"Performance, Capacity and Integrity]"

#. type: Title ==
#: documentation/content/en/books/handbook/zfs/_index.adoc:2780
#, no-wrap
msgid "ZFS Features and Terminology"
msgstr "ZFS 기능 및 용어"

#. type: Plain text
#: documentation/content/en/books/handbook/zfs/_index.adoc:2789
msgid ""
"More than a file system, ZFS is fundamentally different.  ZFS combines the "
"roles of file system and volume manager, enabling new storage devices to add "
"to a live system and having the new space available on the existing file "
"systems in that pool at once.  By combining the traditionally separate "
"roles, ZFS is able to overcome previous limitations that prevented RAID "
"groups being able to grow.  A _vdev_ is a top level device in a pool and can "
"be a simple disk or a RAID transformation such as a mirror or RAID-Z array.  "
"ZFS file systems (called _datasets_) each have access to the combined free "
"space of the entire pool.  Used blocks from the pool decrease the space "
"available to each file system.  This approach avoids the common pitfall with "
"extensive partitioning where free space becomes fragmented across the "
"partitions."
msgstr ""
"ZFS는 파일 시스템 이상의 근본적인 차이가 있습니다.  ZFS는 파일 시스템과 볼륨 "
"관리자의 역할을 결합하여 새로운 스토리지 장치를 라이브 시스템에 추가하고 해"
"당 풀의 기존 파일 시스템에서 새 공간을 한 번에 사용할 수 있도록 합니다.  전통"
"적으로 분리되어 있던 역할을 결합함으로써 ZFS는 RAID 그룹을 확장할 수 없었던 "
"이전의 한계를 극복할 수 있습니다.  _vdev_ 는 풀의 최상위 장치로, 단순한 디스"
"크일 수도 있고 미러 또는 RAID-Z 어레이와 같은 RAID 변환일 수도 있습니다.  _데"
"이터 세트_라고 하는 ZFS 파일 시스템은 각각 전체 풀의 여유 공간을 합친 공간에 "
"액세스할 수 있습니다.  풀에서 사용된 블록은 각 파일 시스템에서 사용할 수 있"
"는 공간을 감소시킵니다.  이 접근 방식은 파티션 전체에 걸쳐 여유 공간이 파편화"
"되는 광범위한 파티셔닝의 일반적인 함정을 피할 수 있습니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2795
#, no-wrap
msgid "[[zfs-term-pool]]pool"
msgstr "[[zfs-term-pool]]pool"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2798
#, no-wrap
msgid ""
"A storage _pool_ is the most basic building block of ZFS. A pool consists of one or more vdevs, the underlying devices that store the data. A pool is then used to create one or more file systems (datasets) or block devices (volumes).\n"
"These datasets and volumes share the pool of remaining free space. Each pool is uniquely identified by a name and a GUID. The ZFS version number on the pool determines the features available."
msgstr ""
"스토리지 _pool_ 은 ZFS의 가장 기본적인 구성 요소입니다. 풀은 데이터를 저장하는 기본 장치인 하나 이상의 vdev로 구성됩니다. 그런 다음 풀은 하나 이상의 파일 시스템(데이터 세트) 또는 블록 장치(볼륨)를 생성하는 데 사용됩니다.\n"
"이러한 데이터 세트와 볼륨은 남은 여유 공간 풀을 공유합니다. 각 풀은 이름과 GUID로 고유하게 식별됩니다. 풀의 ZFS 버전 번호에 따라 사용 가능한 기능이 결정됩니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2799
#, no-wrap
msgid "[[zfs-term-vdev]]vdev Types"
msgstr "[[zfs-term-vdev]]vdev Types"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2832
#, fuzzy, no-wrap
#| msgid ""
#| "A pool consists of one or more vdevs, which themselves are a single disk or a group of disks, transformed to a RAID. When using a lot of vdevs, ZFS spreads data across the vdevs to increase performance and maximize usable space. All vdevs must be at least 128 MB in size. \n"
#| "\n"
#| "* [[zfs-term-vdev-disk]] _Disk_ - The most basic vdev type is a standard block device. This can be an entire disk (such as [.filename]#/dev/ada0# or [.filename]#/dev/da0#) or a partition ([.filename]#/dev/ada0p3#). On FreeBSD, there is no performance penalty for using a partition rather than the entire disk. This differs from recommendations made by the Solaris documentation.\n"
#| "+\n"
#| "[CAUTION]\n"
#| "====\n"
#| "Using an entire disk as part of a bootable pool is strongly discouraged, as this may render the pool unbootable.\n"
#| "Likewise, you should not use an entire disk as part of a mirror or RAID-Z vdev.\n"
#| "Reliably determining the size of an unpartitioned disk at boot time is impossible and there's no place to put in boot code.\n"
#| "====\n"
#| "\n"
#| "* [[zfs-term-vdev-file]] _File_ - Regular files may make up ZFS pools, which is useful for testing and experimentation. Use the full path to the file as the device path in `zpool create`.\n"
#| "* [[zfs-term-vdev-mirror]] _Mirror_ - When creating a mirror, specify the `mirror` keyword followed by the list of member devices for the mirror. A mirror consists of two or more devices, writing all data to all member devices. A mirror vdev will hold as much data as its smallest member. A mirror vdev can withstand the failure of all but one of its members without losing any data.\n"
#| "+\n"
#| "[NOTE]\n"
#| "====\n"
#| "To upgrade a regular single disk vdev to a mirror vdev at any time, use `zpool <<zfs-zpool-attach,attach>>`.\n"
#| "====\n"
#| "\n"
#| "* [[zfs-term-vdev-raidz]] _RAID-Z_ - ZFS uses RAID-Z, a variation on standard RAID-5 that offers better distribution of parity and eliminates the \"RAID-5 write hole\" in which the data and parity information become inconsistent after an unexpected restart. ZFS supports three levels of RAID-Z which provide varying levels of redundancy in exchange for decreasing levels of usable storage. ZFS uses RAID-Z1 through RAID-Z3 based on the number of parity devices in the array and the number of disks which can fail before the pool stops  being operational.\n"
#| "+\n"
#| "In a RAID-Z1 configuration with four disks, each 1 TB, usable storage is 3 TB and the pool will still be able to operate in degraded mode with one faulted disk. If another disk goes offline before replacing and resilvering the faulted disk would result in losing all pool data.\n"
#| "+\n"
#| "In a RAID-Z3 configuration with eight disks of 1 TB, the volume will provide 5 TB of usable space and still be able to operate with three faulted disks. Sun(TM) recommends no more than nine disks in a single vdev. If more disks make up the configuration, the recommendation is to divide them into separate vdevs and stripe the pool data across them.\n"
#| "+\n"
#| "A configuration of two RAID-Z2 vdevs consisting of 8 disks each would create something like a RAID-60 array. A RAID-Z group's storage capacity is about the size of the smallest disk multiplied by the number of non-parity disks. Four 1 TB disks in RAID-Z1 has an effective size of about 3 TB, and an array of eight 1 TB disks in RAID-Z3 will yield 5 TB of usable space.\n"
#| "* [[zfs-term-vdev-spare]] _Spare_ - ZFS has a special pseudo-vdev type for keeping track of available hot spares. Note that installed hot spares are not deployed automatically; manually configure them to replace the failed device using `zfs replace`.\n"
#| "* [[zfs-term-vdev-log]] _Log_ - ZFS Log Devices, also known as ZFS Intent Log (<<zfs-term-zil,ZIL>>) move the intent log from the regular pool devices to a dedicated device, typically an SSD. Having a dedicated log device improves the performance of applications with a high volume of synchronous writes like databases. Mirroring of log devices is possible, but RAID-Z is not supported. If using a lot of log devices, writes will be load-balanced across them.\n"
#| "* [[zfs-term-vdev-cache]] _Cache_ - Adding a cache vdev to a pool will add the storage of the cache to the <<zfs-term-l2arc,L2ARC>>. Mirroring cache devices is impossible. Since a cache device stores only new copies of existing data, there is no risk of data loss."
msgid ""
"A pool consists of one or more vdevs, which themselves are a single disk or a group of disks, transformed to a RAID. When using a lot of vdevs, ZFS spreads data across the vdevs to increase performance and maximize usable space. All vdevs must be at least 128 MB in size.\n"
"\n"
"* [[zfs-term-vdev-disk]] _Disk_ - The most basic vdev type is a standard block device. This can be an entire disk (such as [.filename]#/dev/ada0# or [.filename]#/dev/da0#) or a partition ([.filename]#/dev/ada0p3#). On FreeBSD, there is no performance penalty for using a partition rather than the entire disk. This differs from recommendations made by the Solaris documentation.\n"
"+\n"
"[CAUTION]\n"
"====\n"
"Using an entire disk as part of a bootable pool is strongly discouraged, as this may render the pool unbootable.\n"
"Likewise, you should not use an entire disk as part of a mirror or RAID-Z vdev.\n"
"Reliably determining the size of an unpartitioned disk at boot time is impossible and there's no place to put in boot code.\n"
"====\n"
"\n"
"* [[zfs-term-vdev-file]] _File_ - Regular files may make up ZFS pools, which is useful for testing and experimentation. Use the full path to the file as the device path in `zpool create`.\n"
"* [[zfs-term-vdev-mirror]] _Mirror_ - When creating a mirror, specify the `mirror` keyword followed by the list of member devices for the mirror. A mirror consists of two or more devices, writing all data to all member devices. A mirror vdev will hold as much data as its smallest member. A mirror vdev can withstand the failure of all but one of its members without losing any data.\n"
"+\n"
"[NOTE]\n"
"====\n"
"To upgrade a regular single disk vdev to a mirror vdev at any time, use `zpool\n"
"crossref:zfs[zfs-zpool-attach,attach]`.\n"
"====\n"
"\n"
"* [[zfs-term-vdev-raidz]] _RAID-Z_ - ZFS uses RAID-Z, a variation on standard RAID-5 that offers better distribution of parity and eliminates the \"RAID-5 write hole\" in which the data and parity information become inconsistent after an unexpected restart. ZFS supports three levels of RAID-Z which provide varying levels of redundancy in exchange for decreasing levels of usable storage. ZFS uses RAID-Z1 through RAID-Z3 based on the number of parity devices in the array and the number of disks which can fail before the pool stops  being operational.\n"
"+\n"
"In a RAID-Z1 configuration with four disks, each 1 TB, usable storage is 3 TB and the pool will still be able to operate in degraded mode with one faulted disk. If another disk goes offline before replacing and resilvering the faulted disk would result in losing all pool data.\n"
"+\n"
"In a RAID-Z3 configuration with eight disks of 1 TB, the volume will provide 5 TB of usable space and still be able to operate with three faulted disks. Sun(TM) recommends no more than nine disks in a single vdev. If more disks make up the configuration, the recommendation is to divide them into separate vdevs and stripe the pool data across them.\n"
"+\n"
"A configuration of two RAID-Z2 vdevs consisting of 8 disks each would create something like a RAID-60 array. A RAID-Z group's storage capacity is about the size of the smallest disk multiplied by the number of non-parity disks. Four 1 TB disks in RAID-Z1 has an effective size of about 3 TB, and an array of eight 1 TB disks in RAID-Z3 will yield 5 TB of usable space.\n"
"* [[zfs-term-vdev-spare]] _Spare_ - ZFS has a special pseudo-vdev type for keeping track of available hot spares. Note that installed hot spares are not deployed automatically; manually configure them to replace the failed device using `zfs replace`.\n"
"* [[zfs-term-vdev-log]] _Log_ - ZFS Log Devices, also known as ZFS Intent Log\n"
"  (crossref:zfs[zfs-term-zil,ZIL]) move the intent log from the regular pool devices to a dedicated device, typically an SSD. Having a dedicated log device improves the performance of applications with a high volume of synchronous writes like databases. Mirroring of log devices is possible, but RAID-Z is not supported. If using a lot of log devices, writes will be load-balanced across them.\n"
"* [[zfs-term-vdev-cache]] _Cache_ - Adding a cache vdev to a pool will add the\n"
"  storage of the cache to the crossref:zfs[zfs-term-l2arc,L2ARC]. Mirroring cache devices is impossible. Since a cache device stores only new copies of existing data, there is no risk of data loss."
msgstr ""
"풀은 하나 이상의 vdev로 구성되며, 하나 이상의 vdev는 그 자체로 단일 디스크이거나 RAID로 변환된 디스크 그룹입니다. 많은 수의 vdev를 사용하는 경우, ZFS는 성능을 높이고 사용 가능한 공간을 최대화하기 위해 vdev에 데이터를 분산시킵니다. 모든 vdev의 크기는 128MB 이상이어야 합니다. \n"
"\n"
"* [[zfs-term-vdev-disk]] _Disk_ - 가장 기본적인 vdev 유형은 표준 블록 장치입니다. 이것은 전체 디스크( 예: [.filename]#/dev/ada0# 또는 [.filename]#/dev/da0# ) 또는 파티션( [.filename]#/dev/ada0p3# )이 될 수 있습니다. FreeBSD에서는 전체 디스크가 아닌 파티션을 사용해도 성능에 대한 불이익이 없습니다. 이는 Solaris 설명서의 권장 사항과 다릅니다.\n"
"+\n"
"[CAUTION]\n"
"====\n"
"전체 디스크를 부팅 가능한 풀의 일부로 사용하는 것은 풀을 부팅할 수 없게 만들 수 있으므로 절대 권장하지 않습니다.\n"
"마찬가지로 전체 디스크를 미러 또는 RAID-Z vdev의 일부로 사용해서는 안 됩니다.\n"
"부팅 시 파티션되지 않은 디스크의 크기를 안정적으로 결정하는 것은 불가능하며 부트 코드를 넣을 수 있는 위치가 없습니다.\n"
"====\n"
"\n"
"* [[zfs-term-vdev-file]] _File_ - 테스트 및 실험에 유용한 일반 파일이 ZFS 풀을 구성할 수 있습니다. 파일의 전체 경로를 `zpool create` 의 장치 경로로 사용합니다.\n"
"* [[zfs-term-vdev-mirror]] _Mirror_ - 미러를 생성할 때 `mirror` 키워드 뒤에 미러의 구성원 장치 목록을 지정합니다. 미러는 두 개 이상의 장치로 구성되며, 모든 데이터를 모든 구성원 장치에 기록합니다. 미러 vdev는 가장 작은 멤버만큼의 데이터를 보유합니다. 미러 디바이스는 멤버 중 한 대를 제외한 모든 디바이스가 장애가 발생해도 데이터 손실 없이 견딜 수 있습니다.\n"
"+\n"
"[NOTE]\n"
"====\n"
"일반 단일 디스크 vdev를 언제든지 미러 vdev로 업그레이드하려면 `zpool <<zfs-zpool-attach,attach>>` 를 사용합니다.\n"
"====\n"
"\n"
"* [[zfs-term-vdev-raidz]] _RAID-Z_ - ZFS는 더 나은 패리티 분배를 제공하고 예기치 않은 재시작 후 데이터와 패리티 정보가 일치하지 않는 “RAID-5 write hole”을 제거하는 표준 RAID-5의 변형인 RAID-Z를 사용합니다. ZFS는 사용 가능한 스토리지의 수준을 낮추는 대신 다양한 수준의 중복성을 제공하는 세 가지 레벨의 RAID-Z를 지원합니다. ZFS는 어레이의 패리티 디바이스 수와 풀이 작동을 멈추기 전에 장애가 발생할 수 있는 디스크 수에 따라 RAID-Z1부터 RAID-Z3까지 사용합니다.\n"
"+\n"
"각각 1TB씩 4개의 디스크가 있는 RAID-Z1 구성에서 사용 가능한 스토리지는 3TB이며, 하나의 디스크에 오류가 발생해도 풀은 여전히 성능 저하 모드로 작동할 수 있습니다. 장애가 발생한 디스크를 교체하고 복구하기 전에 다른 디스크가 오프라인 상태가 되면 모든 풀 데이터가 손실될 수 있습니다.\n"
"+\n"
"1TB의 디스크 8개를 사용하는 RAID-Z3 구성의 경우, 이 볼륨은 5TB의 가용 공간을 제공하면서도 3개의 디스크 장애상태에서도 작동할 수 있습니다. Sun(TM)은 단일 vdev에 9개 이하의 디스크를 사용할 것을 권장합니다. 더 많은 디스크가 구성을 구성하는 경우, 이를 별도의 vdev로 분할하여 풀 데이터를 스트라이핑하는 것이 좋습니다.\n"
"+\n"
"각각 8개의 디스크로 구성된 2개의 RAID-Z2 vdev를 구성하면 RAID-60 어레이와 같은 구성이 됩니다. RAID-Z 그룹의 스토리지 용량은 가장 작은 디스크의 크기에 패리티가 아닌 디스크의 수를 곱한 값입니다. RAID-Z1의 1TB 디스크 4개는 약 3TB의 유효 크기를 가지며, RAID-Z3의 1TB 디스크 8개로 구성된 어레이는 5TB의 사용 가능한 공간을 확보합니다.\n"
"* [[zfs-term-vdev-spare]] _Spare_ - ZFS에는 사용 가능한 핫 스페어를 추적하기 위한 특수한 pseudo-vdev 유형이 있습니다. 설치된 핫 스페어는 자동으로 배포되지 않으므로 `zfs replace` 를 사용하여 장애가 발생한 장치를 대체하도록 수동으로 구성해야 합니다.\n"
"* [[zfs-term-vdev-log]] _Log_ - ZFS Intent Log (<<zfs-term-zil,ZIL>>)라고도 하는 ZFS 로그 장치는 인텐트 로그를 일반 풀 장치에서 전용 장치(일반적으로 SSD)로 이동 시킵니다. 전용 로그 장치를 사용하면 데이터베이스와 같이 동기식 쓰기량이 많은 애플리케이션의 성능이 향상됩니다. 로그 장치의 미러링은 가능하지만 RAID-Z는 지원하지 않습니다. 많은 로그 장치를 사용하는 경우 쓰기 작업이 장치 간에 로드 밸런싱됩니다.\n"
"* [[zfs-term-vdev-cache]] _Cache_ - 풀에 cache vdev를 추가하면 캐시의 스토리지가 <<zfs-term-l2arc,L2ARC>> 에 추가됩니다. 캐시 디바이스 미러링은 불가능합니다. 캐시 장치는 기존 데이터의 새 복사본만 저장하므로 데이터 손실의 위험이 없습니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2833
#, no-wrap
msgid "[[zfs-term-txg]] Transaction Group (TXG)"
msgstr "[[zfs-term-txg]] Transaction Group (TXG)"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2854
#, fuzzy, no-wrap
#| msgid ""
#| "Transaction Groups are the way ZFS groups blocks changes together and writes them to the pool. Transaction groups are the atomic unit that ZFS uses to ensure consistency. ZFS assigns each transaction group a unique 64-bit consecutive identifier. There can be up to three active transaction groups at a time, one in each of these three states: \n"
#| "\n"
#| "* _Open_ - A new transaction group begins in the open state and accepts new writes. There is always a transaction group in the open state, but the transaction group may refuse new writes if it has reached a limit. Once the open transaction group has reached a limit, or reaching the <<zfs-advanced-tuning-txg-timeout,`vfs.zfs.txg.timeout`>>, the transaction group advances to the next state.\n"
#| "* _Quiescing_ - A short state that allows any pending operations to finish without blocking the creation of a new open transaction group. Once all the transactions in the group have completed, the transaction group advances to the final state.\n"
#| "* _Syncing_ - Write all the data in the transaction group to stable storage. This process will in turn change other data, such as metadata and space maps, that ZFS will also write to stable storage. The process of syncing involves several passes. On the first and biggest, all the changed data blocks; next come the metadata, which may take several passes to complete. Since allocating space for the data blocks generates new metadata, the syncing state cannot finish until a pass completes that does not use any new space. The syncing state is also where _synctasks_ complete. Synctasks are administrative operations such as creating or destroying snapshots and datasets that complete the uberblock change. Once the sync state completes the transaction group in the quiescing state advances to the syncing state. All administrative functions, such as <<zfs-term-snapshot,`snapshot`>> write as part of the transaction group. ZFS adds a created synctask to the open transaction group, and that group advances as fast as possible to the syncing state to reduce the latency of administrative commands."
msgid ""
"Transaction Groups are the way ZFS groups blocks changes together and writes them to the pool. Transaction groups are the atomic unit that ZFS uses to ensure consistency. ZFS assigns each transaction group a unique 64-bit consecutive identifier. There can be up to three active transaction groups at a time, one in each of these three states:\n"
"\n"
"* _Open_ - A new transaction group begins in the open state and accepts new\n"
"  writes. There is always a transaction group in the open state, but the\n"
"  transaction group may refuse new writes if it has reached a limit. Once the\n"
"  open transaction group has reached a limit, or reaching the\n"
"  crossref:zfs[zfs-advanced-tuning-txg-timeout,`vfs.zfs.txg.timeout`], the transaction group advances to the next state.\n"
"* _Quiescing_ - A short state that allows any pending operations to finish without blocking the creation of a new open transaction group. Once all the transactions in the group have completed, the transaction group advances to the final state.\n"
"* _Syncing_ - Write all the data in the transaction group to stable storage.\n"
"  This process will in turn change other data, such as metadata and space maps,\n"
"  that ZFS will also write to stable storage. The process of syncing involves\n"
"  several passes. On the first and biggest, all the changed data blocks; next\n"
"  come the metadata, which may take several passes to complete. Since allocating\n"
"  space for the data blocks generates new metadata, the syncing state cannot\n"
"  finish until a pass completes that does not use any new space. The syncing\n"
"  state is also where _synctasks_ complete. Synctasks are administrative\n"
"  operations such as creating or destroying snapshots and datasets that complete\n"
"  the uberblock change. Once the sync state completes the transaction group in\n"
"  the quiescing state advances to the syncing state. All administrative\n"
"  functions, such as crossref:zfs[zfs-term-snapshot,`snapshot`] write as part of the transaction group. ZFS adds a created synctask to the open transaction group, and that group advances as fast as possible to the syncing state to reduce the latency of administrative commands."
msgstr ""
"트랜잭션 그룹은 ZFS가 블록 변경 사항을 함께 그룹화하여 풀에 쓰는 방식입니다. 트랜잭션 그룹은 ZFS가 일관성을 보장하기 위해 사용하는 원자 단위입니다. ZFS는 각 트랜잭션 그룹에 고유한 64비트 연속 식별자를 할당합니다. 한 번에 최대 3개의 활성 트랜잭션 그룹이 있을 수 있으며, 이 세 가지 상태 각각에 하나씩 있습니다: \n"
"\n"
"* _Open_ - 새 트랜잭션 그룹이 개방 상태에서 시작하여 새 쓰기를 수락합니다. 항상 열려 있는 상태의 트랜잭션 그룹이 있지만, 트랜잭션 그룹이 한도에 도달하면 새 쓰기를 거부할 수 있습니다. 열려 있는 트랜잭션 그룹이 한도에 도달하거나 <<zfs-advanced-tuning-txg-timeout,`vfs.zfs.txg.timeout`>> 에 도달하면 트랜잭션 그룹은 다음 상태로 진행합니다.\n"
"* _Quiescing_ - 새로운 미결 트랜잭션 그룹 생성을 차단하지 않고 보류 중인 모든 작업을 완료할 수 있는 짧은 상태입니다. 그룹의 모든 트랜잭션이 완료되면 트랜잭션 그룹은 최종 상태로 진행됩니다.\n"
"* _Syncing_ - 트랜잭션 그룹의 모든 데이터를 안정적인 스토리지에 씁니다. 이 프로세스는 메타데이터 및 스페이스 맵과 같은 다른 데이터도 변경하여 ZFS가 안정적인 스토리지에 기록합니다. 동기화 프로세스에는 여러 번의 패스가 포함됩니다. 가장 먼저 변경된 모든 데이터 블록이 동기화되고, 다음으로 메타데이터가 동기화되며, 이 작업은 여러 번에 걸쳐 진행됩니다. 데이터 블록에 공간을 할당하면 새 메타데이터가 생성되므로 새 공간을 사용하지 않는 패스가 완료될 때까지 동기화 상태가 완료될 수 없습니다. 동기화 상태는 _synctasks_ 가 완료되는 지점이기도 합니다. Synctasks는 uberblock 변경을 완료하는 스냅샷 및 데이터세트 생성 또는 삭제와 같은 관리 작업입니다. 동기화 상태가 완료되면 대기 상태의 트랜잭션 그룹이 동기화 상태로 진행됩니다. <<zfs-term-snapshot,`snapshot`>> 과 같은 모든 관리 함수는 트랜잭션 그룹의 일부로 기록하게 됩니다. ZFS는 생성된 synctask를 열린 트랜잭션 그룹에 추가하고, 해당 그룹은 관리 명령의 대기 시간을 줄이기 위해 가능한 한 빨리 동기화 상태로 진행합니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2855
#, no-wrap
msgid "[[zfs-term-arc]]Adaptive Replacement Cache (ARC)"
msgstr "[[zfs-term-arc]]Adaptive Replacement Cache (ARC)"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2857
#, no-wrap
msgid "ZFS uses an Adaptive Replacement Cache (ARC), rather than a more traditional Least Recently Used (LRU) cache. An LRU cache is a simple list of items in the cache, sorted by how recently object was used, adding new items to the head of the list. When the cache is full, evicting items from the tail of the list makes room for more active objects. An ARC consists of four lists; the Most Recently Used (MRU) and Most Frequently Used (MFU) objects, plus a ghost list for each. These ghost lists track evicted objects to prevent adding them back to the cache. This increases the cache hit ratio by avoiding objects that have a history of occasional use. Another advantage of using both an MRU and MFU is that scanning an entire file system would evict all data from an MRU or LRU cache in favor of this freshly accessed content. With ZFS, there is also an MFU that tracks the most frequently used objects, and the cache of the most commonly accessed blocks remains."
msgstr "ZFS는 기존의 가장 최근에 사용된 캐시(Least Recently Used, LRU)가 아닌 적응형 대체 캐시(Adaptive Replacement, ARC)를 사용합니다. LRU 캐시는 캐시에 있는 항목의 간단한 목록으로, 개체를 가장 최근에 사용한 순서대로 정렬하여 목록의 맨 위에 새 항목을 추가합니다. 캐시가 가득 차면 목록의 꼬리에서 항목을 제거하여 더 많은 활성 개체를 위한 공간을 확보합니다. ARC는 가장 최근에 사용한(MRU) 개체와 가장 자주 사용한(Most Recently Used, MFU) 개체, 그리고 각각에 대한 고스트(Ghost) 목록의 네 가지 목록으로 구성됩니다. 이러한 고스트 목록은 퇴거된 개체를 추적하여 캐시에 다시 추가되지 않도록 합니다. 이렇게 하면 가끔씩만 사용한 기록이 있는 개체를 피함으로써 캐시 적중률이 높아집니다. MRU와 MFU를 모두 사용할 때의 또 다른 장점은 전체 파일 시스템을 스캔하면 새로 액세스한 콘텐츠를 위해 MRU 또는 LRU 캐시에서 모든 데이터를 제거할 수 있다는 것입니다. ZFS에는 가장 자주 사용되는 개체를 추적하는 MFU도 있으며, 가장 자주 액세스하는 블록의 캐시는 그대로 유지됩니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2858
#, no-wrap
msgid "[[zfs-term-l2arc]]L2ARC"
msgstr "[[zfs-term-l2arc]]L2ARC"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2876
#, fuzzy, no-wrap
#| msgid "L2ARC is the second level of the ZFS caching system. RAM stores the primary ARC. Since the amount of available RAM is often limited, ZFS can also use <<zfs-term-vdev-cache,cache vdevs>>. Solid State Disks (SSDs) are often used as these cache devices due to their higher speed and lower latency compared to traditional spinning disks. L2ARC is entirely optional, but having one will increase read speeds for cached files on the SSD instead of having to read from the regular disks. L2ARC can also speed up <<zfs-term-deduplication,deduplication>> because a deduplication table (DDT) that does not fit in RAM but does fit in the L2ARC will be much faster than a DDT that must read from disk. Limits on the data rate added to the cache devices prevents prematurely wearing out SSDs with extra writes. Until the cache is full (the first block evicted to make room), writes to the L2ARC limit to the sum of the write limit and the boost limit, and afterwards limit to the write limit. A pair of man:sysctl[8] values control these rate limits. <<zfs-advanced-tuning-l2arc_write_max,`vfs.zfs.l2arc_write_max`>> controls the number of bytes written to the cache per second, while <<zfs-advanced-tuning-l2arc_write_boost,`vfs.zfs.l2arc_write_boost`>> adds to this limit during the \"Turbo Warmup Phase\" (Write Boost)."
msgid ""
"L2ARC is the second level of the ZFS caching system. RAM stores the primary\n"
"ARC. Since the amount of available RAM is often limited, ZFS can also use\n"
"crossref:zfs[zfs-term-vdev-cache,cache vdevs]. Solid State Disks (SSDs) are\n"
"often used as these cache devices due to their higher speed and lower latency\n"
"compared to traditional spinning disks. L2ARC is entirely optional, but having\n"
"one will increase read speeds for cached files on the SSD instead of having to\n"
"read from the regular disks. L2ARC can also speed up\n"
"crossref:zfs[zfs-term-deduplication,deduplication] because a deduplication table\n"
"(DDT) that does not fit in RAM but does fit in the L2ARC will be much faster\n"
"than a DDT that must read from disk. Limits on the data rate added to the cache\n"
"devices prevents prematurely wearing out SSDs with extra writes. Until the cache\n"
"is full (the first block evicted to make room), writes to the L2ARC limit to the\n"
"sum of the write limit and the boost limit, and afterwards limit to the write\n"
"limit. A pair of man:sysctl[8] values control these rate limits.\n"
"crossref:zfs[zfs-advanced-tuning-l2arc_write_max,`vfs.zfs.l2arc_write_max`]\n"
"controls the number of bytes written to the cache per second, while\n"
"crossref:zfs[zfs-advanced-tuning-l2arc_write_boost,`vfs.zfs.l2arc_write_boost`] adds to this limit during the \"Turbo Warmup Phase\" (Write Boost)."
msgstr "L2ARC는 ZFS 캐싱 시스템의 두 번째 레벨입니다. RAM은 기본 ARC를 저장합니다. 사용 가능한 RAM의 양이 제한되어 있는 경우가 많기 때문에 ZFS는 <<zfs-term-vdev-cache,cache vdevs>> 도 사용할 수 있습니다. 솔리드 스테이트 디스크(SSD)는 기존 스피닝 디스크에 비해 속도가 빠르고 지연 시간이 짧기 때문에 이러한 캐시 장치로 자주 사용됩니다. L2ARC는 전적으로 선택 사항이지만, 이를 사용하면 일반 디스크에서 읽을 필요가 없어 SSD에 캐시된 파일의 읽기 속도가 빨라집니다. 또한 L2ARC는 <<zfs-term-deduplication,deduplication>> 속도를 높일 수 있는데, 이는 RAM에는 맞지 않지만 L2ARC에는 맞는 중복 제거 테이블(DeDuplication tableDDT)이 디스크에서 읽어야 하는 DDT보다 훨씬 빠르기 때문입니다. 캐시 장치에 추가되는 데이터 속도에 제한을 두면 추가 쓰기로 인해 SSD가 조기에 마모되는 것을 방지할 수 있습니다. 캐시가 가득 찰 때까지(공간을 확보하기 위해 첫 번째 블록이 퇴거됨) 쓰기 제한과 부스트 제한의 합으로 L2ARC에 대한 쓰기가 제한되고, 그 이후에는 쓰기 제한으로 제한됩니다. 한 쌍의 man:sysctl[8] 값이 이러한 속도 제한을 제어합니다. <<zfs-advanced-tuning-l2arc_write_max,`vfs.zfs.l2arc_write_max`>> 는 초당 캐시에 기록되는 바이트 수를 제어하고, <<zfs-advanced-tuning-l2arc_write_boost,`vfs.zfs.l2arc_write_boost`>> 는 “터보 워밍업 단계(Write Boost)” 동안 이 제한을 추가합니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2877
#, no-wrap
msgid "[[zfs-term-zil]]ZIL"
msgstr "[[zfs-term-zil]]ZIL"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2879
#, no-wrap
msgid "ZIL accelerates synchronous transactions by using storage devices like SSDs that are faster than those used in the main storage pool. When an application requests a synchronous write (a guarantee that the data is stored to disk rather than merely cached for later writes), writing the data to the faster ZIL storage then later flushing it out to the regular disks greatly reduces latency and improves performance. Synchronous workloads like databases will profit from a ZIL alone. Regular asynchronous writes such as copying files will not use the ZIL at all."
msgstr "ZIL은 기본 스토리지 풀에서 사용되는 것보다 빠른 SSD와 같은 저장 장치를 사용하여 동기 트랜잭션을 가속화합니다. 애플리케이션이 동기 쓰기(나중에 쓰기 위해 데이터를 단순히 캐시하지 않고 디스크에 저장하는 것)를 요청할 때, 더 빠른 ZIL 스토리지에 데이터를 쓴 다음 나중에 일반 디스크로 플러시하면 지연 시간을 크게 줄이고 성능을 향상시킬 수 있습니다. 데이터베이스와 같은 동기식 워크로드는 ZIL만으로도 이점을 얻을 수 있습니다. 파일 복사와 같은 일반적인 비동기 쓰기는 ZIL을 전혀 사용하지 않습니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2880
#, no-wrap
msgid "[[zfs-term-cow]]Copy-On-Write"
msgstr "[[zfs-term-cow]]Copy-On-Write"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2882
#, no-wrap
msgid "Unlike a traditional file system, ZFS writes a different block rather than overwriting the old data in place. When completing this write the metadata updates to point to the new location. When a shorn write (a system crash or power loss in the middle of writing a file) occurs, the entire original contents of the file are still available and ZFS discards the incomplete write. This also means that ZFS does not require a man:fsck[8] after an unexpected shutdown."
msgstr "기존 파일 시스템과 달리 ZFS는 기존 데이터를 덮어쓰지 않고 다른 블록을 작성합니다. 이 쓰기가 완료되면 메타데이터가 새 위치를 가리키도록 업데이트됩니다. Shorn 쓰기(파일 쓰기 도중 시스템 충돌 또는 전원 손실)가 발생해도 파일의 전체 원본 콘텐츠는 계속 사용할 수 있으며 ZFS는 불완전한 쓰기를 삭제합니다. 이는 또한 예기치 않게 종료된 후에도 ZFS에 man:fsck[8]가 필요하지 않다는 것을 의미합니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2883
#, no-wrap
msgid "[[zfs-term-dataset]]Dataset"
msgstr "[[zfs-term-dataset]]Dataset"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2893
#, fuzzy, no-wrap
#| msgid "_Dataset_ is the generic term for a ZFS file system, volume, snapshot or clone. Each dataset has a unique name in the format _poolname/path@snapshot_. The root of the pool is a dataset as well. Child datasets have hierarchical names like directories. For example, _mypool/home_, the home dataset, is a child of _mypool_ and inherits properties from it. Expand this further by creating _mypool/home/user_. This grandchild dataset will inherit properties from the parent and grandparent. Set properties on a child to override the defaults inherited from the parent and grandparent. Administration of datasets and their children can be <<zfs-zfs-allow,delegated>>."
msgid ""
"_Dataset_ is the generic term for a ZFS file system, volume, snapshot or clone.\n"
"Each dataset has a unique name in the format _poolname/path@snapshot_. The root\n"
"of the pool is a dataset as well. Child datasets have hierarchical names like\n"
"directories. For example, _mypool/home_, the home dataset, is a child of\n"
"_mypool_ and inherits properties from it. Expand this further by creating\n"
"_mypool/home/user_. This grandchild dataset will inherit properties from the\n"
"parent and grandparent. Set properties on a child to override the defaults\n"
"inherited from the parent and grandparent. Administration of datasets and their\n"
"children can be crossref:zfs[zfs-zfs-allow,delegated]."
msgstr "_데이터 세트_는 ZFS 파일 시스템, 볼륨, 스냅샷 또는 복제본에 대한 일반적인 용어입니다. 각 데이터 세트에는 _poolname/path@snapshot_ 형식의 고유한 이름이 있습니다. 풀의 루트도 데이터 세트입니다. 하위 데이터 세트는 디렉터리와 같은 계층적 이름을 갖습니다. 예를 들어, 홈 데이터 세트인 _mypool/home_ 은 _mypool_ 의 하위 데이터 세트이며 이로부터 속성을 상속받습니다. _mypool/home/user_ 를 생성하여 이를 더 확장할 수 있습니다. 이 손자 데이터 세트는 부모 및 조부모로부터 속성을 상속받습니다. 자식에 속성을 설정하여 부모 및 조부모로부터 상속된 기본값을 재정의할 수 있습니다. 데이터 세트와 그 자식 데이터 세트의 관리는 <<zfs-zfs-allow,delegated>> 가 될 수 있습니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2894
#, no-wrap
msgid "[[zfs-term-filesystem]]File system"
msgstr "[[zfs-term-filesystem]]File system"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2896
#, no-wrap
msgid "A ZFS dataset is most often used as a file system. Like most other file systems, a ZFS file system mounts somewhere in the systems directory hierarchy and contains files and directories of its own with permissions, flags, and other metadata."
msgstr "ZFS 데이터 세트는 파일 시스템으로 가장 자주 사용됩니다. 대부분의 다른 파일 시스템과 마찬가지로 ZFS 파일 시스템은 시스템 디렉토리 계층 구조의 어딘가에 마운트되며 권한, 플래그 및 기타 메타데이터가 있는 자체 파일과 디렉터리를 포함합니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2897
#, no-wrap
msgid "[[zfs-term-volume]]Volume"
msgstr "[[zfs-term-volume]]Volume"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2899
#, no-wrap
msgid "ZFS can also create volumes, which appear as disk devices. Volumes have a lot of the same features as datasets, including copy-on-write, snapshots, clones, and checksumming. Volumes can be useful for running other file system formats on top of ZFS, such as UFS virtualization, or exporting iSCSI extents."
msgstr "ZFS는 디스크 장치로 표시되는 볼륨도 생성할 수 있습니다. 볼륨에는 데이터 세트와 동일한 기능이 많이 있으며, 여기에는 copy-on-write, 스냅샷, 클론, 체크섬 등이 포함됩니다. 볼륨은 UFS 가상화 또는 iSCSI 확장 내보내기와 같은 다른 파일 시스템 형식을 ZFS 위에서 실행하는 데 유용할 수 있습니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2900
#, no-wrap
msgid "[[zfs-term-snapshot]]Snapshot"
msgstr "[[zfs-term-snapshot]]Snapshot"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2920
#, fuzzy, no-wrap
#| msgid "The <<zfs-term-cow,copy-on-write>> (COW) design of ZFS allows for nearly instantaneous, consistent snapshots with arbitrary names. After taking a snapshot of a dataset, or a recursive snapshot of a parent dataset that will include all child datasets, new data goes to new blocks, but without reclaiming the old blocks as free space. The snapshot contains the original file system version and the live file system contains any changes made since taking the snapshot using no other space. New data written to the live file system uses new blocks to store this data. The snapshot will grow as the blocks are no longer used in the live file system, but in the snapshot alone. Mount these snapshots read-only allows recovering of previous file versions. A <<zfs-zfs-snapshot,rollback>> of a live file system to a specific snapshot is possible, undoing any changes that took place after taking the snapshot. Each block in the pool has a reference counter which keeps track of the snapshots, clones, datasets, or volumes use that block. As files and snapshots get deleted, the reference count  decreases, reclaiming the free space when no longer referencing a block. Marking snapshots with a <<zfs-zfs-snapshot,hold>> results in any attempt to destroy it will  returns an `EBUSY` error. Each snapshot can have holds with a unique name each. The <<zfs-zfs-snapshot,release>> command removes the hold so the snapshot can deleted. Snapshots, cloning, and rolling back works on volumes, but independently mounting does not."
msgid ""
"The crossref:zfs[zfs-term-cow,copy-on-write] (COW) design of ZFS allows for\n"
"nearly instantaneous, consistent snapshots with arbitrary names. After taking a\n"
"snapshot of a dataset, or a recursive snapshot of a parent dataset that will\n"
"include all child datasets, new data goes to new blocks, but without reclaiming\n"
"the old blocks as free space. The snapshot contains the original file system\n"
"version and the live file system contains any changes made since taking the\n"
"snapshot using no other space. New data written to the live file system uses new\n"
"blocks to store this data. The snapshot will grow as the blocks are no longer\n"
"used in the live file system, but in the snapshot alone. Mount these snapshots\n"
"read-only allows recovering of previous file versions. A\n"
"crossref:zfs[zfs-zfs-snapshot,rollback] of a live file system to a specific\n"
"snapshot is possible, undoing any changes that took place after taking the\n"
"snapshot. Each block in the pool has a reference counter which keeps track of\n"
"the snapshots, clones, datasets, or volumes use that block. As files and\n"
"snapshots get deleted, the reference count  decreases, reclaiming the free space\n"
"when no longer referencing a block. Marking snapshots with a\n"
"crossref:zfs[zfs-zfs-snapshot,hold] results in any attempt to destroy it will\n"
"returns an `EBUSY` error. Each snapshot can have holds with a unique name each.\n"
"The crossref:zfs[zfs-zfs-snapshot,release] command removes the hold so the snapshot can deleted. Snapshots, cloning, and rolling back works on volumes, but independently mounting does not."
msgstr "ZFS의 <<zfs-term-cow,copy-on-write>>(COW) 설계는 임의의 이름을 가진 거의 즉각적이고 일관된 스냅샷을 허용합니다. 데이터 세트의 스냅샷 또는 모든 하위 데이터 세트를 포함하는 상위 데이터 세트의 재귀적 스냅샷을 생성한 후, 새 데이터는 새 블록으로 이동하지만 이전 블록을 여유 공간으로 되찾지 않습니다. 스냅샷에는 원래 파일 시스템 버전이 포함되며, 라이브 파일 시스템에는 다른 공간을 사용하지 않고 스냅샷을 만든 이후 변경된 모든 내용이 포함됩니다. 라이브 파일 시스템에 새로 쓰여진 데이터는 새 블록을 사용하여 이 데이터를 저장합니다. 블록이 더 이상 라이브 파일 시스템에서 사용되지 않고 스냅샷에서만 사용됨에 따라 스냅샷이 커집니다. 이러한 스냅샷을 읽기 전용으로 마운트하면 이전 파일 버전을 복구할 수 있습니다. 라이브 파일 시스템을 특정 스냅샷으로 <<zfs-zfs-snapshot,rollback>> 하여 스냅샷을 생성한 후 발생한 모든 변경 사항을 취소할 수 있습니다. 풀의 각 블록에는 해당 블록을 사용하는 스냅샷, 복제본, 데이터 세트 또는 볼륨을 추적하는 참조 카운터가 있습니다. 파일과 스냅샷이 삭제되면 참조 카운터가 감소하여 더 이상 블록을 참조하지 않을 때 여유 공간이 확보됩니다. 스냅샷을 <<zfs-zfs-snapshot,hold>> 로 표시하면 스냅샷을 삭제하려고 시도 했을때 `EBUSY` 오류가 반환됩니다. 각 스냅샷은 각각 고유한 이름을 가진 보류를 가질 수 있습니다. <<zfs-zfs-snapshot,release>> 명령은 보류를 제거하여 스냅샷을 삭제할 수 있도록 합니다. 스냅샷, 복제 및 롤백은 볼륨에서 작동하지만 독립적으로 마운트하는 것은 작동하지 않습니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2921
#, no-wrap
msgid "[[zfs-term-clone]]Clone"
msgstr "[[zfs-term-clone]]Clone"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2923
#, no-wrap
msgid "Cloning a snapshot is also possible. A clone is a writable version of a snapshot, allowing the file system to fork as a new dataset. As with a snapshot, a clone initially consumes no new space. As new data written to a clone uses new blocks, the size of the clone grows. When blocks are overwritten in the cloned file system or volume, the reference count on the previous block decreases. Removing the snapshot upon which a clone bases is impossible because the clone depends on it. The snapshot is the parent, and the clone is the child. Clones can be _promoted_, reversing this dependency and making the clone the parent and the previous parent the child. This operation requires no new space. Since the amount of space used by the parent and child reverses, it may affect existing quotas and reservations."
msgstr "스냅샷 복제도 가능합니다. 복제본(클론)은 쓰기 가능한 버전의 스냅샷으로, 파일 시스템을 새로운 데이터 세트로 포크할 수 있습니다. 스냅샷과 마찬가지로 복제본은 처음에 새 공간을 차지하지 않습니다. 복제본에 기록된 새 데이터는 새 블록을 사용하므로 복제본의 크기가 커집니다. 복제된 파일 시스템 또는 볼륨에서 블록을 덮어쓰면 이전 블록의 참조 수가 감소합니다. 복제본이 스냅샷에 의존하기 때문에 복제본의 기반이 되는 스냅샷을 제거할 수 없습니다. 스냅샷은 부모이고 복제본은 자식입니다. 복제본을 _승격_하여 이 종속성을 역전시키고 복제본을 부모로, 이전 부모를 자식으로 만들 수 있습니다. 이 작업에는 새 공간이 필요하지 않습니다. 부모와 자식이 사용하는 공간의 양이 바뀌기 때문에 기존 할당량 및 예약에 영향을 줄 수 있습니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2924
#, no-wrap
msgid "[[zfs-term-checksum]]Checksum"
msgstr "[[zfs-term-checksum]]Checksum"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2936
#, fuzzy, no-wrap
#| msgid ""
#| "Every block is also checksummed. The checksum algorithm used is a per-dataset property, see <<zfs-zfs-set,`set`>>. The checksum of each block is transparently validated when read, allowing ZFS to detect silent corruption. If the data read does not match the expected checksum, ZFS will attempt to recover the data from any available redundancy, like mirrors or RAID-Z. Triggering a validation of all checksums with <<zfs-term-scrub,`scrub`>>. Checksum algorithms include:\n"
#| "\n"
#| "* `fletcher2`\n"
#| "* `fletcher4`\n"
#| "* `sha256`\n"
#| " The `fletcher` algorithms are faster, but `sha256` is a strong cryptographic hash and has a much lower chance of collisions at the  cost of some performance. Deactivating checksums is possible, but  strongly discouraged."
msgid ""
"Every block is also checksummed. The checksum algorithm used is a per-dataset\n"
"property, see crossref:zfs[zfs-zfs-set,`set`]. The checksum of each block is\n"
"transparently validated when read, allowing ZFS to detect silent corruption. If\n"
"the data read does not match the expected checksum, ZFS will attempt to recover\n"
"the data from any available redundancy, like mirrors or RAID-Z. Triggering a\n"
"validation of all checksums with crossref:zfs[zfs-term-scrub,`scrub`]. Checksum algorithms include:\n"
"\n"
"* `fletcher2`\n"
"* `fletcher4`\n"
"* `sha256`\n"
" The `fletcher` algorithms are faster, but `sha256` is a strong cryptographic hash and has a much lower chance of collisions at the  cost of some performance. Deactivating checksums is possible, but  strongly discouraged."
msgstr ""
"모든 블록도 체크섬됩니다. 사용되는 체크섬 알고리즘은 per-dataset 속성입니다( <<zfs-zfs-set,`set`>> 참조). 각 블록의 체크섬은 읽을 때 투명하게 검증되므로 ZFS가 조용한 손상을 감지할 수 있습니다. 읽은 데이터가 예상 체크섬과 일치하지 않으면 ZFS는 미러 또는 RAID-Z와 같은 사용 가능한 중복성에서 데이터를 복구하려고 시도합니다. <<zfs-term-scrub,`scrub`>> 으로 모든 체크섬의 유효성 검사를 트리거합니다. 체크섬 알고리즘은 다음과 같습니다:\n"
"\n"
"* `fletcher2`\n"
"* `fletcher4`\n"
"* `sha256`\n"
"`fletcher` 알고리즘이 더 빠르지만 `sha256` 은 강력한 암호화 해시이며 일부 성능 저하를 감수하더라도 충돌 가능성이 훨씬 낮습니다. 체크섬을 비활성화하는 것도 가능하지만 권장하지 않습니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2937
#, no-wrap
msgid "[[zfs-term-compression]]Compression"
msgstr "[[zfs-term-compression]]Compression"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2951
#, fuzzy, no-wrap
#| msgid ""
#| "Each dataset has a compression property, which defaults to off. Set this property to an available compression algorithm. This causes compression of all new data written to the dataset. Beyond a reduction in space used, read and write throughput often increases because fewer blocks need reading or writing. \n"
#| "\n"
#| "[[zfs-term-compression-lz4]]\n"
#| "* _LZ4_ - Added in ZFS pool version 5000 (feature flags), LZ4 is now the recommended compression algorithm. LZ4 works about 50% faster than LZJB when operating on compressible data, and is over three times faster when operating on uncompressible data. LZ4 also decompresses about 80% faster than LZJB. On modern CPUs, LZ4 can often compress at over 500 MB/s, and decompress at over 1.5 GB/s (per single CPU core).\n"
#| "\n"
#| "[[zfs-term-compression-lzjb]]\n"
#| "* _LZJB_ - The default compression algorithm. Created by Jeff Bonwick (one of the original creators of ZFS). LZJB offers good compression with less CPU overhead compared to GZIP. In the future, the default compression algorithm will change to LZ4.\n"
#| "\n"
#| "[[zfs-term-compression-gzip]]\n"
#| "* _GZIP_ - A popular stream compression algorithm available in ZFS. One of the main advantages of using GZIP is its configurable level of compression. When setting the `compress` property, the administrator can choose the level of compression, ranging from `gzip1`, the lowest level of compression, to `gzip9`, the highest level of compression. This gives the administrator control over how much CPU time to trade for saved disk space.\n"
#| "\n"
#| "[[zfs-term-compression-zle]]\n"
#| "* _ZLE_ - Zero Length Encoding is a special compression algorithm that compresses continuous runs of zeros alone. This compression algorithm is useful when the dataset contains large blocks of zeros."
msgid ""
"Each dataset has a compression property, which defaults to off. Set this property to an available compression algorithm. This causes compression of all new data written to the dataset. Beyond a reduction in space used, read and write throughput often increases because fewer blocks need reading or writing.\n"
"\n"
"[[zfs-term-compression-lz4]]\n"
"* _LZ4_ - Added in ZFS pool version 5000 (feature flags), LZ4 is now the recommended compression algorithm. LZ4 works about 50% faster than LZJB when operating on compressible data, and is over three times faster when operating on uncompressible data. LZ4 also decompresses about 80% faster than LZJB. On modern CPUs, LZ4 can often compress at over 500 MB/s, and decompress at over 1.5 GB/s (per single CPU core).\n"
"\n"
"[[zfs-term-compression-lzjb]]\n"
"* _LZJB_ - The default compression algorithm. Created by Jeff Bonwick (one of the original creators of ZFS). LZJB offers good compression with less CPU overhead compared to GZIP. In the future, the default compression algorithm will change to LZ4.\n"
"\n"
"[[zfs-term-compression-gzip]]\n"
"* _GZIP_ - A popular stream compression algorithm available in ZFS. One of the main advantages of using GZIP is its configurable level of compression. When setting the `compress` property, the administrator can choose the level of compression, ranging from `gzip1`, the lowest level of compression, to `gzip9`, the highest level of compression. This gives the administrator control over how much CPU time to trade for saved disk space.\n"
"\n"
"[[zfs-term-compression-zle]]\n"
"* _ZLE_ - Zero Length Encoding is a special compression algorithm that compresses continuous runs of zeros alone. This compression algorithm is useful when the dataset contains large blocks of zeros."
msgstr ""
"각 데이터 세트에는 압축 속성이 있으며, 기본값은 꺼짐입니다. 이 속성을 사용 가능한 압축 알고리즘으로 설정합니다. 이렇게 하면 데이터 세트에 새로 쓰여지는 모든 데이터가 압축됩니다. 사용되는 공간이 줄어드는 것 외에도 읽기 또는 쓰기가 필요한 블록이 줄어들기 때문에 읽기 및 쓰기 처리량이 증가하는 경우가 많습니다. \n"
"\n"
"[[zfs-term-compression-lz4]]\n"
"* _LZ4_ - ZFS 풀 버전 5000 (기능 플래그)에 추가된 LZ4는 이제 권장 압축 알고리즘입니다. LZ4는 압축 가능한 데이터에서 작동할 때 LZJB보다 약 50% 더 빠르게 작동하며, 압축 불가능한 데이터에서 작동할 때는 3배 이상 더 빠릅니다. 또한 LZ4는 LZJB보다 약 80% 더 빠르게 압축을 해제합니다. 최신 CPU에서 LZ4는 종종 500MB/s 이상의 속도로 압축하고 1.5GB/s 이상의 속도로 압축을 해제할 수 있습니다(단일 CPU 코어당).\n"
"\n"
"[[zfs-term-compression-lzjb]]\n"
"* _LZJB_ - 기본 압축 알고리즘입니다. Jeff Bonwick (ZFS의 최초 개발자 중 한 명)이 만들었습니다. LZJB는 GZIP에 비해 CPU 오버헤드가 적으면서도 우수한 압축 성능을 제공합니다. 앞으로 기본 압축 알고리즘은 LZ4로 변경될 예정입니다.\n"
"\n"
"[[zfs-term-compression-gzip]]\n"
"* _GZIP_ - ZFS에서 널리 사용되는 스트림 압축 알고리즘입니다. GZIP 사용의 주요 장점 중 하나는 압축 수준을 구성할 수 있다는 것입니다. `compress` 속성을 설정할 때 관리자는 가장 낮은 압축 수준인 `gzip1` 부터 가장 높은 압축 수준인 `gzip9` 까지 압축 수준을 선택할 수 있습니다. 이를 통해 관리자는 절약된 디스크 공간과 교환할 CPU 시간을 제어할 수 있습니다.\n"
"\n"
"[[zfs-term-compression-zle]]\n"
"* _ZLE_ - Zero Length Encoding은 0의 연속 실행만을 압축하는 특수 압축 알고리즘입니다. 이 압축 알고리즘은 데이터 세트에 큰 0 블록이 포함되어 있을 때 유용합니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2952
#, no-wrap
msgid "[[zfs-term-copies]]Copies"
msgstr "[[zfs-term-copies]]Copies"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2955
#, fuzzy, no-wrap
#| msgid "When set to a value greater than 1, the `copies` property instructs ZFS to maintain copies of each block in the <<zfs-term-filesystem,file system>> or <<zfs-term-volume,volume>>. Setting this property on important datasets provides added redundancy from which to recover a block that does not match its checksum. In pools without redundancy, the copies feature is the single form of redundancy. The copies feature can recover from a single bad sector or other forms of minor corruption, but it does not protect the pool from the loss of an entire disk."
msgid ""
"When set to a value greater than 1, the `copies` property instructs ZFS to\n"
"maintain copies of each block in the crossref:zfs[zfs-term-filesystem,file\n"
"system] or crossref:zfs[zfs-term-volume,volume]. Setting this property on important datasets provides added redundancy from which to recover a block that does not match its checksum. In pools without redundancy, the copies feature is the single form of redundancy. The copies feature can recover from a single bad sector or other forms of minor corruption, but it does not protect the pool from the loss of an entire disk."
msgstr "1보다 큰 값으로 설정하면 `copies` 속성은 <<zfs-term-filesystem,file system>> 또는 <<zfs-term-volume,volume>> 에 각 블록의 사본을 유지하도록 ZFS에 지시합니다. 중요한 데이터 세트에 이 속성을 설정하면 체크섬이 일치하지 않는 블록을 복구할 수 있는 추가 중복성이 제공됩니다. 중복성이 없는 풀에서는 복사본 기능이 단일 형태의 중복성입니다. 복사본 기능은 단일 불량 섹터 또는 다른 형태의 사소한 손상으로부터 복구할 수 있지만, 전체 디스크의 손실로부터 풀을 보호하지는 못합니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2956
#, no-wrap
msgid "[[zfs-term-deduplication]]Deduplication"
msgstr "[[zfs-term-deduplication]]Deduplication"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2958
#, no-wrap
msgid "Checksums make it possible to detect duplicate blocks when writing data. With deduplication, the reference count of an existing, identical block increases, saving storage space. ZFS keeps a deduplication table (DDT) in memory to detect duplicate blocks. The table contains a list of unique checksums, the location of those blocks, and a reference count. When writing new data, ZFS calculates checksums and compares them to the list. When finding a match it uses the existing block. Using the SHA256 checksum algorithm with deduplication provides a secure cryptographic hash. Deduplication is tunable. If `dedup` is `on`, then a matching checksum means that the data is identical. Setting `dedup` to `verify`, ZFS performs a byte-for-byte check on the data ensuring they are actually identical. If the data is not identical, ZFS will note the hash collision and store the two blocks separately. As the DDT must store the hash of each unique block, it consumes a large amount of memory. A general rule of thumb is 5-6 GB of ram per 1 TB of deduplicated data). In situations not practical to have enough RAM to keep the entire DDT in memory, performance will suffer greatly as the DDT must read from disk before writing each new block. Deduplication can use L2ARC to store the DDT, providing a middle ground between fast system memory and slower disks. Consider using compression instead, which often provides nearly as much space savings without the increased memory."
msgstr "체크섬을 사용하면 데이터를 쓸 때 중복된 블록을 감지할 수 있습니다. 중복 제거를 사용하면 기존의 동일한 블록의 참조 수가 증가하여 저장 공간을 절약할 수 있습니다. ZFS는 중복 블록을 감지하기 위해 중복 제거 테이블(DDT)을 메모리에 보관합니다. 이 테이블에는 고유 체크섬 목록, 해당 블록의 위치 및 참조 카운트가 포함되어 있습니다. 새 데이터를 쓸 때 ZFS는 체크섬을 계산하여 목록과 비교합니다. 일치하는 항목을 찾으면 기존 블록을 사용합니다. 중복 제거와 함께 SHA256 체크섬 알고리즘을 사용하면 안전한 암호화 해시를 제공합니다. 중복 제거는 조정할 수 있습니다. `dedup` 가 `on` 이면 체크섬이 일치한다는 것은 데이터가 동일하다는 것을 의미합니다. `dedup` 를 `verify` 으로 설정하면 ZFS는 데이터에 대해 바이트 단위로 검사를 수행하여 데이터가 실제로 동일한지 확인합니다. 데이터가 동일하지 않으면 ZFS는 해시 충돌을 기록하고 두 블록을 별도로 저장합니다. DDT는 각각의 고유한 블록의 해시를 저장해야 하므로 많은 양의 메모리를 소비합니다. 일반적으로 중복 제거된 데이터 1TB당 5~6GB의 램이 필요합니다.) 전체 DDT를 메모리에 보관할 수 있는 충분한 RAM이 없는 상황에서는 새 블록을 쓰기 전에 디스크에서 읽어야 하므로 성능이 크게 저하됩니다. 중복 제거는 빠른 시스템 메모리와 느린 디스크 사이의 중간 지점을 제공하는 L2ARC를 사용하여 DDT를 저장할 수 있습니다. 대신 압축을 사용하면 메모리를 늘리지 않고도 거의 동일한 공간 절약 효과를 얻을 수 있습니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2959
#, no-wrap
msgid "[[zfs-term-scrub]]Scrub"
msgstr "[[zfs-term-scrub]]Scrub"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2970
#, fuzzy, no-wrap
#| msgid "Instead of a consistency check like man:fsck[8], ZFS has `scrub`. `scrub` reads all data blocks stored on the pool and verifies their checksums against the known good checksums stored in the metadata. A periodic check of all the data stored on the pool ensures the recovery of any corrupted blocks before needing them. A scrub is not required after an unclean shutdown, but good practice is at least once every three months. ZFS verifies the checksum of each block during normal use, but a scrub makes certain to check even infrequently used blocks for silent corruption. ZFS improves data security in archival storage situations. Adjust the relative priority of `scrub` with <<zfs-advanced-tuning-scrub_delay,`vfs.zfs.scrub_delay`>> to prevent the scrub from degrading the performance of other workloads on the pool."
msgid ""
"Instead of a consistency check like man:fsck[8], ZFS has `scrub`. `scrub` reads\n"
"all data blocks stored on the pool and verifies their checksums against the\n"
"known good checksums stored in the metadata. A periodic check of all the data\n"
"stored on the pool ensures the recovery of any corrupted blocks before needing\n"
"them. A scrub is not required after an unclean shutdown, but good practice is at\n"
"least once every three months. ZFS verifies the checksum of each block during\n"
"normal use, but a scrub makes certain to check even infrequently used blocks for\n"
"silent corruption. ZFS improves data security in archival storage situations.\n"
"Adjust the relative priority of `scrub` with\n"
"crossref:zfs[zfs-advanced-tuning-scrub_delay,`vfs.zfs.scrub_delay`] to prevent the scrub from degrading the performance of other workloads on the pool."
msgstr "man:fsck[8]와 같은 일관성 검사 대신, ZFS에는 `scrub` 이 있습니다. `scrub` 은 풀에 저장된 모든 데이터 블록을 읽고 메타데이터에 저장된 알려진 정상 체크섬과 비교하여 체크섬을 확인합니다. 풀에 저장된 모든 데이터를 주기적으로 검사하여 손상된 블록을 필요하기 전에 복구할 수 있습니다. 비정상 종료 후에는 스크럽이 필요하지 않지만, 적어도 3개월에 한 번씩은 스크럽을 수행하는 것이 좋습니다. ZFS는 정상적으로 사용하는 동안 각 블록의 체크섬을 확인하지만, 스크럽은 자주 사용하지 않는 블록도 조용한 손상이 있는지 확인합니다. ZFS는 아카이브 스토리지 상황에서 데이터 보안을 향상시킵니다. 스크럽으로 인해 풀의 다른 워크로드 성능이 저하되지 않도록 <<zfs-advanced-tuning-scrub_delay,`vfs.zfs.scrub_delay`>> 로 `scrub` 의 상대적 우선순위를 조정합니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2971
#, no-wrap
msgid "[[zfs-term-quota]]Dataset Quota"
msgstr "[[zfs-term-quota]]Dataset Quota"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2985
#, fuzzy, no-wrap
#| msgid ""
#| "ZFS provides fast and accurate dataset, user, and group space accounting as well as quotas and space reservations. This gives the administrator fine grained control over space allocation and allows reserving space for critical file systems. \n"
#| "\n"
#| "ZFS supports different types of quotas: the dataset quota, the <<zfs-term-refquota,reference quota (refquota)>>, the <<zfs-term-userquota,user quota>>, and the <<zfs-term-groupquota,group quota>>.\n"
#| "\n"
#| "Quotas limit the total size of a dataset and its descendants, including snapshots of the dataset, child datasets, and the snapshots of those datasets.\n"
#| "\n"
#| "[NOTE]\n"
#| "====\n"
#| "Volumes do not support quotas, as the `volsize` property acts as an implicit quota.\n"
#| "===="
msgid ""
"ZFS provides fast and accurate dataset, user, and group space accounting as well as quotas and space reservations. This gives the administrator fine grained control over space allocation and allows reserving space for critical file systems.\n"
"\n"
"ZFS supports different types of quotas: the dataset quota, the\n"
"crossref:zfs[zfs-term-refquota,reference quota (refquota)], the\n"
"crossref:zfs[zfs-term-userquota,user quota], and the\n"
"crossref:zfs[zfs-term-groupquota,group quota].\n"
"\n"
"Quotas limit the total size of a dataset and its descendants, including snapshots of the dataset, child datasets, and the snapshots of those datasets.\n"
"\n"
"[NOTE]\n"
"====\n"
"Volumes do not support quotas, as the `volsize` property acts as an implicit quota.\n"
"===="
msgstr ""
"ZFS는 빠르고 정확한 데이터 세트, 사용자, 그룹 공간 계정은 물론 할당량과 공간 예약 기능을 제공합니다. 이를 통해 관리자는 공간 할당을 세밀하게 제어할 수 있으며 중요한 파일 시스템을 위한 공간을 예약할 수 있습니다. \n"
"\n"
"ZFS는 데이터 세트 할당량, <<zfs-term-refquota,reference quota (refquota)>>, <<zfs-term-userquota,user quota>>, <<zfs-term-groupquota,group quota>> 과 같은 다양한 유형의 할당량을 지원합니다.\n"
"\n"
"할당량은 데이터 세트의 스냅샷, 하위 데이터 세트 및 해당 데이터 세트의 스냅샷을 포함하여 데이터 세트과 그 하위 데이터 세트의 총 크기를 제한합니다.\n"
"\n"
"[참고]\n"
"====\n"
"볼륨은 할당량을 지원하지 않습니다. `volsize` 속성이 암시적 할당량으로 작동하기 때문입니다.\n"
"===="

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2986
#, no-wrap
msgid "[[zfs-term-refquota]]Reference Quota"
msgstr "[[zfs-term-refquota]]Reference Quota"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2988
#, no-wrap
msgid "A reference quota limits the amount of space a dataset can consume by enforcing a hard limit. This hard limit includes space referenced by the dataset alone and does not include space used by descendants, such as file systems or snapshots."
msgstr "참조 할당량은 하드 제한을 적용하여 데이터 집합이 사용할 수 있는 공간의 양을 제한합니다. 이 하드 제한에는 데이터 집합이 참조하는 공간만 포함되며, 파일 시스템이나 스냅샷과 같은 하위 집합이 사용하는 공간은 포함되지 않습니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2989
#, no-wrap
msgid "[[zfs-term-userquota]]User Quota"
msgstr "[[zfs-term-userquota]]User Quota"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2991
#, no-wrap
msgid "User quotas are useful to limit the amount of space used by the specified user."
msgstr "사용자 할당량은 지정된 사용자가 사용하는 공간의 양을 제한하는 데 유용합니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2992
#, no-wrap
msgid "[[zfs-term-groupquota]]Group Quota"
msgstr "[[zfs-term-groupquota]]Group Quota"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2994
#, no-wrap
msgid "The group quota limits the amount of space that a specified group can consume."
msgstr "그룹 할당량은 지정된 그룹이 사용할 수 있는 공간의 양을 제한합니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:2995
#, no-wrap
msgid "[[zfs-term-reservation]]Dataset Reservation"
msgstr "[[zfs-term-reservation]]Dataset Reservation"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:3003
#, fuzzy, no-wrap
#| msgid ""
#| "The `reservation` property makes it possible to guarantee an amount of space for a specific dataset and its descendants. This means that setting a 10 GB reservation on [.filename]#storage/home/bob# prevents other datasets from using up all free space, reserving at least 10 GB of space for this dataset. Unlike a regular <<zfs-term-refreservation,`refreservation`>>, space used by snapshots and descendants is not counted against the reservation. For example, if taking a snapshot of [.filename]#storage/home/bob#, enough disk space other than the `refreservation` amount must exist for the operation to succeed. Descendants of the main data set are not counted in the `refreservation` amount and so do not encroach on the space set.\n"
#| "\n"
#| "Reservations of any sort are useful in situations such as planning and testing the suitability of disk space allocation in a new system, or ensuring that enough space is available on file systems for audio logs or system recovery procedures and files."
msgid ""
"The `reservation` property makes it possible to guarantee an amount of space\n"
"for a specific dataset and its descendants. This means that setting a 10 GB\n"
"reservation on [.filename]#storage/home/bob# prevents other datasets from using\n"
"up all free space, reserving at least 10 GB of space for this dataset. Unlike a\n"
"regular crossref:zfs[zfs-term-refreservation,`refreservation`], space used by snapshots and descendants is not counted against the reservation. For example, if taking a snapshot of [.filename]#storage/home/bob#, enough disk space other than the `refreservation` amount must exist for the operation to succeed. Descendants of the main data set are not counted in the `refreservation` amount and so do not encroach on the space set.\n"
"\n"
"Reservations of any sort are useful in situations such as planning and testing the suitability of disk space allocation in a new system, or ensuring that enough space is available on file systems for audio logs or system recovery procedures and files."
msgstr ""
"`reservation` 속성을 사용하면 특정 데이터 세트와 그 하위 데이터 세트에 대해 일정 공간을 보장할 수 있습니다. 즉, [.filename]#storage/home/bob# 에 10GB 예약을 설정하면 다른 데이터 세트가 모든 여유 공간을 사용하지 못하도록 하여 이 데이터 세트에 최소 10GB의 공간을 예약할 수 있습니다. 일반 <<zfs-term-refreservation,`refreservation`>> 과 달리, 스냅샷 및 하위 항목에서 사용하는 공간은 예약에 포함되지 않습니다. 예를 들어 [.filename]#storage/home/bob# 의 스냅샷을 만드는 경우, 작업이 성공하려면 `refreservation` 양 이외의 충분한 디스크 공간이 있어야 합니다. 주 데이터 세트의 하위 데이터 세트는 `refreservation` 양에 포함되지 않으므로 공간을 잠식하지 않습니다.\n"
"\n"
"모든 종류의 예약은 새 시스템에서 디스크 공간 할당의 적합성을 계획하고 테스트하거나 오디오 로그 또는 시스템 복구 절차 및 파일을 위해 파일 시스템에 충분한 공간을 확보하는 등의 상황에서 유용합니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:3004
#, no-wrap
msgid "[[zfs-term-refreservation]]Reference Reservation"
msgstr "[[zfs-term-refreservation]]Reference Reservation"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:3010
#, fuzzy, no-wrap
#| msgid "The `refreservation` property makes it possible to guarantee an amount of space for the use of a specific dataset _excluding_ its descendants. This means that setting a 10 GB reservation on [.filename]#storage/home/bob#, and another dataset tries to use the free space, reserving at least 10 GB of space  for this dataset. In contrast to a regular <<zfs-term-reservation,reservation>>, space used by snapshots and descendant datasets is not counted against the reservation. For example, if taking a snapshot of [.filename]#storage/home/bob#, enough disk space other than the `refreservation` amount must exist for the operation to succeed. Descendants of the  main data set are not counted in the `refreservation` amount and so do not encroach on the space set."
msgid ""
"The `refreservation` property makes it possible to guarantee an amount of space\n"
"for the use of a specific dataset _excluding_ its descendants. This means that\n"
"setting a 10 GB reservation on [.filename]#storage/home/bob#, and another\n"
"dataset tries to use the free space, reserving at least 10 GB of space  for this\n"
"dataset. In contrast to a regular crossref:zfs[zfs-term-reservation,reservation], space used by snapshots and descendant datasets is not counted against the reservation. For example, if taking a snapshot of [.filename]#storage/home/bob#, enough disk space other than the `refreservation` amount must exist for the operation to succeed. Descendants of the  main data set are not counted in the `refreservation` amount and so do not encroach on the space set."
msgstr "`refreservation` 속성을 사용하면 특정 데이터 세트의 하위 데이터 세트를 _제외하고_ 해당 데이터 세트의 사용을 위한 공간을 일정량 보장할 수 있습니다. 즉, [.filename]#storage/home/bob# 에 10GB 예약을 설정하면 다른 데이터 세트가 이 데이터 세트에 최소 10GB의 공간을 예약하여 여유 공간을 사용하려고 시도합니다. 일반 <<zfs-term-reservation,reservation>> 과 달리, 스냅샷 및 하위 데이터 세트에서 사용하는 공간은 예약에 포함되지 않습니다. 예를 들어, [.filename]#storage/home/bob# 의 스냅샷을 생성하는 경우, 작업이 성공하려면 `refreservation` 양 이외의 충분한 디스크 공간이 있어야 합니다. 주 데이터 세트의 하위 데이터 세트는 `refreservation` 양에 포함되지 않으므로 공간을 잠식하지 않습니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:3011
#, no-wrap
msgid "[[zfs-term-resilver]]Resilver"
msgstr "[[zfs-term-resilver]]Resilver"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:3012
#, no-wrap
msgid "When replacing a failed disk, ZFS must fill the new disk with the lost data. _Resilvering_ is the process of using the parity information distributed across the remaining drives to calculate and write the missing data to the new drive."
msgstr "고장난 디스크를 교체할 때 ZFS는 새 디스크에 손실된 데이터를 채워야 합니다. _Resilvering_ 은 나머지 드라이브에 분산된 패리티 정보를 사용하여 누락된 데이터를 계산하고 새 드라이브에 쓰는 프로세스입니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:3013
#, no-wrap
msgid "[[zfs-term-online]]Online"
msgstr "[[zfs-term-online]]Online"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:3015
#, no-wrap
msgid "A pool or vdev in the `Online` state has its member devices connected and fully operational. Individual devices in the `Online` state are functioning."
msgstr "`Online` 상태의 풀 또는 가상 디바이스는 구성원 디바이스가 연결되어 있고 완전히 작동합니다. `Online` 상태의 개별 디바이스가 작동 중입니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:3016
#, no-wrap
msgid "[[zfs-term-offline]]Offline"
msgstr "[[zfs-term-offline]]Offline"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:3020
#, fuzzy, no-wrap
#| msgid "The administrator puts individual devices in an `Offline` state if enough redundancy exists to avoid putting the pool or vdev into a <<zfs-term-faulted,Faulted>> state. An administrator may choose to offline a disk in preparation for replacing it, or to make it easier to identify."
msgid ""
"The administrator puts individual devices in an `Offline` state if enough\n"
"redundancy exists to avoid putting the pool or vdev into a\n"
"crossref:zfs[zfs-term-faulted,Faulted] state. An administrator may choose to offline a disk in preparation for replacing it, or to make it easier to identify."
msgstr "관리자는 풀 또는 vdev가 <<zfs-term-faulted,Faulted>> 상태가 되지 않도록 충분한 중복성이 존재하는 경우 개별 장치를 `Offline` 상태로 설정합니다. 관리자는 디스크 교체를 준비하거나 식별하기 쉽게 하기 위해 디스크를 오프라인 상태로 설정할 수 있습니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:3021
#, no-wrap
msgid "[[zfs-term-degraded]]Degraded"
msgstr "[[zfs-term-degraded]]Degraded"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:3028
#, fuzzy, no-wrap
#| msgid "A pool or vdev in the `Degraded` state has one or more disks that disappeared or failed. The pool is still usable, but if other devices fail, the pool may become unrecoverable. Reconnecting the missing devices or replacing the failed disks will return the pool to an <<zfs-term-online,Online>> state after the reconnected or new device has completed the <<zfs-term-resilver,Resilver>> process."
msgid ""
"A pool or vdev in the `Degraded` state has one or more disks that disappeared\n"
"or failed. The pool is still usable, but if other devices fail, the pool may\n"
"become unrecoverable. Reconnecting the missing devices or replacing the failed\n"
"disks will return the pool to an crossref:zfs[zfs-term-online,Online] state\n"
"after the reconnected or new device has completed the\n"
"crossref:zfs[zfs-term-resilver,Resilver] process."
msgstr "`Degraded` 상태의 풀 또는 vdev에는 사라지거나 실패한 디스크가 하나 이상 있습니다. 풀은 여전히 사용할 수 있지만 다른 장치에 장애가 발생하면 풀을 복구할 수 없게 될 수 있습니다. 누락된 디바이스를 다시 연결하거나 장애가 발생한 디스크를 교체하면 다시 연결된 디바이스 또는 새 디바이스가 <<zfs-term-online,Online>> 프로세스를 완료한 후 풀이 <<zfs-term-resilver,Resilver>> 상태로 돌아갑니다."

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:3029
#, no-wrap
msgid "[[zfs-term-faulted]]Faulted"
msgstr "[[zfs-term-faulted]]Faulted"

#. type: Table
#: documentation/content/en/books/handbook/zfs/_index.adoc:3034
#, fuzzy, no-wrap
#| msgid "A pool or vdev in the `Faulted` state is no longer operational. Accessing the data is no longer possible. A pool or vdev enters the `Faulted` state when the number of missing or failed devices exceeds the level of redundancy in the vdev. If reconnecting missing devices the pool will return to an <<zfs-term-online,Online>> state. Insufficient redundancy to compensate for the number of failed disks loses the pool contents and requires restoring from backups."
msgid ""
"A pool or vdev in the `Faulted` state is no longer operational. Accessing the\n"
"data is no longer possible. A pool or vdev enters the `Faulted` state when the\n"
"number of missing or failed devices exceeds the level of redundancy in the vdev.\n"
"If reconnecting missing devices the pool will return to an\n"
"crossref:zfs[zfs-term-online,Online] state. Insufficient redundancy to compensate for the number of failed disks loses the pool contents and requires restoring from backups."
msgstr "`Faulted` 상태의 풀 또는 vdev는 더 이상 작동할 수 없습니다. 데이터에 더 이상 액세스할 수 없습니다. 누락되거나 장애가 발생한 디바이스의 수가 디바이스의 중복성 수준을 초과하면 풀 또는 vdev가 `Faulted` 상태가 됩니다. 누락된 디바이스를 다시 연결하면 풀은 <<zfs-term-online,Online>> 상태로 돌아갑니다. 실패한 디스크 수를 보상할 수 있는 중복성이 충분하지 않으면 풀 콘텐츠가 손실되고 백업에서 복원해야 합니다."
